%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Define Article %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[11pt,colorlinks]{book}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Using Packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{amsthm}
\usepackage{empheq}
\usepackage{mdframed}
\usepackage{booktabs}
\usepackage{changepage}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{psfrag}
\usepackage[T1]{fontenc}
\usepackage{physics}
\usepackage{pgfplots}
\usepackage{fancyhdr}
\usepackage[french]{babel}
\usepackage{bm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Other Settings

%%%%%%%%%%%%%%%%%%%%%%%%%% Page Setting %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\geometry{margin=1.8cm,head=14.5pt}
%%%%%%%%%%%%%%%%%%%%%%%%%% Define some useful colors %%%%%%%%%%%%%%%%%%%%%%%%%%
\definecolor{ocre}{RGB}{243,102,25}
\definecolor{mygray}{RGB}{243,243,244}
\definecolor{deepGreen}{RGB}{26,111,0}
\definecolor{shallowGreen}{RGB}{235,255,255}
\definecolor{deepBlue}{RGB}{61,124,222}
\definecolor{shallowBlue}{RGB}{235,249,255}
\definecolor{deepRed}{RGB}{133,1,1}
\definecolor{shallowRed}{RGB}{255,127,127}
\definecolor{lilac}{HTML}{c8a2c8}
\definecolor{deepPurple}{HTML}{643a64}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%% Define an orangebox command %%%%%%%%%%%%%%%%%%%%%%%%
\newcommand\orangebox[1]{\fcolorbox{ocre}{mygray}{\hspace{1em}#1\hspace{1em}}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%% English Environments %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheoremstyle{mytheoremstyle}
  {3pt}{3pt}
  {\normalfont}{0cm}
  {\rmfamily\bfseries}{}
  {\newline }{{\color{black}\thmname{#1}~\thmnumber{#2}}\thmnote{\,--\,#3}}
\newtheoremstyle{myproblemstyle}{3pt}{3pt}{\normalfont}{0cm}{\rmfamily\bfseries}{}{1em}{{\color{black}\thmname{#1}~\thmnumber{#2}}\thmnote{\,--\,#3}}
\theoremstyle{mytheoremstyle}
\newmdtheoremenv[everyline=true,linewidth=1pt,backgroundcolor=shallowGreen,linecolor=deepGreen,innertopmargin=1pt,leftmargin=1pt,innerleftmargin=20pt,innerrightmargin=20pt]{theorem}{Théorème}[section]
\theoremstyle{mytheoremstyle}
\newmdtheoremenv[everyline=true,linewidth=1pt,backgroundcolor=shallowRed,linecolor=deepRed,innertopmargin=1pt,leftmargin=1pt,innerleftmargin=20pt,innerrightmargin=20pt,]{prop}{Proposition}[section]
\theoremstyle{mytheoremstyle}
\newmdtheoremenv[everyline=true,linewidth=1pt,backgroundcolor=lilac,linecolor=deepPurple,innertopmargin=1pt,leftmargin=1pt,innerleftmargin=20pt,innerrightmargin=20pt,]{definition}{Définition}[section]
\theoremstyle{mytheoremstyle}
\newmdtheoremenv[everyline=true,linewidth=1pt,backgroundcolor=shallowBlue,linecolor=deepBlue,innertopmargin=1pt,leftmargin=1pt,innerleftmargin=20pt,innerrightmargin=20pt,]{ef}{EF}[section]
\theoremstyle{mytheoremstyle}
\newmdtheoremenv[everyline=true,linewidth=1pt,backgroundcolor=shallowBlue, linecolor=deepBlue,innertopmargin=1pt,leftmargin=1pt, innerleftmargin=10pt, innerrightmargin=10pt,]{st}{S}[section]
\theoremstyle{mytheoremstyle}
\newmdtheoremenv[everyline=true,linewidth=1pt,backgroundcolor=mygray,linecolor=black,leftmargin=1pt,innertopmargin=1pt,innerleftmargin=20pt,innerrightmargin=20pt,]{ex}{}[section]

\theoremstyle{mytheoremstyle}
\newmdtheoremenv[everyline=true,linewidth=1pt,backgroundcolor=shallowBlue,linecolor=deepBlue,innertopmargin=1pt,leftmargin=1pt,innerleftmargin=20pt,innerrightmargin=20pt,]{rmq}{Remarque}[section]

\theoremstyle{mytheoremstyle}
\newmdtheoremenv[everyline=true,linewidth=1pt,backgroundcolor=mygray,linecolor=black,innertopmargin=1pt,leftmargin=1pt,innerleftmargin=20pt,innerrightmargin=20pt,]{exe}{Exercice}[section]


\theoremstyle{myproblemstyle}
\newmdtheoremenv[everyline=true,linecolor=black,leftmargin=1pt,innerleftmargin=10pt,innerrightmargin=10pt,]{problem}{Problem}[section]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Plotting Settings %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepgfplotslibrary{colorbrewer}
\pgfplotsset{width=8cm,compat=1.9}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%
% Required to support mathematical unicode
\usepackage[warnunknown, fasterrors, mathletters]{ucs}
\usepackage[utf8x]{inputenc}
\allowdisplaybreaks[1]
% Always typeset math in display style
\everymath{\displaystyle}

% Use a larger font size
\usepackage[fontsize=14pt]{scrextend}

% Standard mathematical typesetting packages
\usepackage{amsfonts, amsthm, amsmath, amssymb}
\usepackage{mathtools}  % Extension to amsmath

% Symbol and utility packages
\usepackage{cancel, textcomp}
\usepackage[mathscr]{euscript}
\usepackage[nointegrals]{wasysym}

% Extras
\usepackage{physics}  % Lots of useful shortcuts and macros
\usepackage{tikz-cd}  % For drawing commutative diagrams easily
\usepackage{color}  % Add some colour to life
\usepackage{microtype}  % Minature font tweaks

% Common shortcuts
\def\mbb#1{\mathbb{#1}}
\def\mfk#1{\mathfrak{#1}}
\def\mfc#1{\mathcal{#1}}


\def\bN{\mbb{N}}
\def\bC{\mbb{C}}
\def\bR{\mbb{R}}
\def\bQ{\mbb{Q}}
\def\bZ{\mbb{Z}}
\def\mC{\mathcal{C}}
\def\mM{\mathcal{M}}
\def\L{\mfc{L}^1(I,\bK)}
\def\Li#1{\mfc{L}^{#1}(I,\bK)}
\def\LI#1{\mfc{L}^1(#1,\bK)}
\def\ib#1{\int_{a}^{b} #1}
\def\ig#1{\int_{a}^{\infty} #1}
\def\bK{\mbb{K}}
\def\af{[a,\infty[}
\def\ab{[a,b[}
\def\abc{]a,b]}
\def\abd{]a,b[}
\def\x{$x \in \bR$}
\def\z{$z \in \bC$}
\def\n{$n \in \bN$}
\def\is#1{\sum_{n=0}^\infty #1}
\def\iss#1#2{\sum_{n=#1}^\infty #2}
\def\se{\sum a_n z^n}
\def\ser{\sum a_n t^n}
\def\seq#1{\sum a_n z_{#1}^n}
\def\seb#1{\sum #1_n z^n}
\def\fn{\forall n \in \bN,}
\def\born{l^{\infty}\left( \bC \right)}
\def\fef{\textbf{FEF}}
\def\ln{\lim_{n \to \infty}}
\def\bO{\mfc{O}}
\def\rN{\bR^{\bN}}
\def\rNp{\left(\bR^{+}\right)^{\bN}}
\def\sn{\sum_{n=0}^{\infty} u_n}
\def\satp{série à terme positifs}
\def\apcr{à partir d'un certain rang}
% Sometimes helpful macros
\newcommand{\func}[3]{#1\colon#2\to#3}
\newcommand{\cvs}[2]{converge simplement sur $#1$ vers $#2$}
\newcommand{\cvu}[2]{converge uniformément sur $#1$ vers $#2$}
\newcommand{\ppl}[1]{par passage à la limite lorsque #1}
\newcommand{\ppln}[1]{par passage à la limite lorsque $n \to \infty$}
\newcommand{\de}[4]{\begin{cases}
    #1 & \text{si } #2 \\
    #3 & \text{si } #4
\end{cases}}
\newcommand{\deq}[3]{\begin{cases}
    #1 & \text{si } #2 \\
    #3 & \text{sinon}
\end{cases}}
\newcommand{\vfunc}[5]{
  \begin{align*}
    #1 \colon #2 &\to #3\\
    #4 &\mapsto #5.
  \end{align*}
}
\newcommand{\parenth}[1]{\left(#1\right)}
\newcommand\restr[2]{{% we make the whole thing an ordinary symbol
  \left.\kern-\nulldelimiterspace % automatically resize the bar with \right
  #1 % the function
  \vphantom{\big|} % pretend it's a little taller at normal size
  \right|_{#2} % this is the delimiter
  }}
%%
\renewcommand{\equiv}{\sim}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Title & Author %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Math3 CM}
\author{Cours de L. PASQUEREAU \\ Note de C. THOMAS}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
    \maketitle
    \tableofcontents
    \pagestyle{fancy}
    \fancyfoot{}
    \fancyfoot[C]{Approuvé pour usage interne à l'Université de Rennes, page \thepage}
    \chapter{Fonctions de $\bR$ dans $\bR$}

    Soit $D \in \bR$, soit $f \in \bR^{D}$ 
    \section{Limite}
    \subsection{Adhérence}
    \begin{definition}
      On appelle adhérence de $D$ le plus petit ensemble fermé qui contient D. Noté $\bar{D}$ 
    \end{definition}
    \subsection{Limite}
    Soit $f$ définie sur D, Soit $a \in \bar{D}$, Soit $l \in \bR$ 
    \begin{definition}
      On dit que $f$ a pour limite $l$ quand $x$ tends vers $a$ si
      \begin{align*}
        \forall \varepsilon > 0, \exists \eta > 0 | |x-a| < \eta \Rightarrow |f(x) - l| < \varepsilon
      \end{align*}
    \end{definition}
    \subsection{Fonctions négligeables}
    \begin{definition}
      Soit $f,g \in \bR^{D}$ et $a \in \bar{D}$ on dit que $f = o_{a}(g)$ si $\dfrac{f(x)}{g(x)} \to_{a} 0$
    \end{definition}
    \begin{ex}
      en 0 on a 
      \begin{align}
        \dfrac{f(x)}{g(x)} &= \dfrac{x}{\sqrt{x}} \\ 
                           &\to_{0^{+}} 0 \\ 
        f = o_{O^{+}}(g)
      \end{align}
    \end{ex}

    \subsection{Croissance comparée}
    \begin{theorem}[Croissances Comparées]
      Soient $(\alpha,\beta,\gamma) \in R^{+*}$ avec $\gamma > 1$ avec 
      \begin{align*}
        f : x &\mapsto (\log x)^{\alpha} \\ 
        g : x &\mapsto x^{\beta} \\ 
        h : x &\mapsto \gamma^{x}
      \end{align*} 
      alors on a 
      \begin{align*}
        g = o_{\infty}(f) \\ 
        h = o_{\infty}(g)
      \end{align*}
      c'est à dire 
      \begin{align*}
        \dfrac{(\log x)^\alpha}{x^\beta} &\to_{\infty} 0 \\ 
        \dfrac{x^{\beta}}{\gamma^x} &\to_{\infty} 0
      \end{align*}
    \end{theorem}

    \subsection{Fonctions Équivalentes}
    \begin{definition}
      Soit $f,g \in \bR^{D}$ et $a \in \bar{D}$ on dit que $f$ est équivalente à $g$ 
      quand $x$ tends vers $a$ si $\dfrac{f}{g} \to_{a} 1$.  

      On note $f \equiv_a g$ 
    \end{definition}
    \begin{ex}
      \begin{itemize}
        \item Un polynome est équivalent à son monôme de plus haut degrès (resp bas) quand $x$ tends vers $\infty$ (resp $0$)
        \item $sin x \equiv_{0} x$
        \item $ln(1+x) \equiv_{0} x$
      \end{itemize}
    \end{ex}
    \subsection{Opération sur les équivalents}
    Soient $f_1,g_1,f_2,g_2 \in \bR^D$ soit $a \in \bar{D}$ soit $\alpha \in \bR$ 
    \begin{align*}
      f_1 &\equiv_{a} g_1 \\ 
      f_2 &\equiv_{a} g_2 \\
    \end{align*}
    \begin{itemize}
      \item \begin{align*}
      f_1 \cdot f_2 &\equiv_{a} g_1 \cdot g_2 \\ 
      \dfrac{f_1}{f_2} &\equiv_{a} \dfrac{g_1}{g_2} \\
      f_1^\alpha &\equiv_{a} g_1^\alpha
    \end{align*}
    \item \begin{equation}
      f = o_{a} g \Rightarrow f + g \equiv_{a} g 
    \end{equation}
    \item Si $f \equiv_{a} g$ et $\lim_{x \to a} f(x) = l$ alors $\lim_{x \to a} g(x) = l$
    \item \begin{prop}
      Si $f \equiv_a g$ et $\lim_a f \not= 1$ alors $\log f \equiv_a \log g$
      \begin{proof}
        \begin{align*}
          \dfrac{\log g(x)}{\log f(x)} - 1 &= \dfrac{\log g(x) - \log f(x)}{\log f(x)} \\
          &= \dfrac{\log \left(\dfrac{g(x)}{f(x)}\right)}{\log f(x)} && \text{or } f \equiv_a g \\
          &\to_{a} \dfrac{0}{\log f(a)} && \text{par passage à la limite car } \lim_a f \not= 1 \\
          &= 0
        \end{align*}
        Donc $\lim_{x \to a} \dfrac{\log f(x)}{\log g(x)} = 1$ donc $\log f \equiv_a \log g$
      \end{proof}
    \end{prop}
    Cas particulier où $l=1$ 
    \begin{ex}
      $f(x) = 1+x$ et $g(x) = 1 + \sqrt{x}$ on a bien $f \equiv_0 g$ et $f \to_0 1$ 
      on a aussi $\log f(x) = \log 1+x \equiv_0 x$ et $\log g(x) = \log 1+\sqrt{x} \equiv_0 \sqrt{x}$ et $x \not= \sqrt{x}$
    \end{ex}
  \end{itemize}
  \section{Continuité}
  \begin{definition}
    Soit $f$ définie sur un ouvert $D$ de $\bR$ et $a \in D$.  
    On dit que $f$ est continue en $a$ si et seulement si $\lim_{x \to a} f(x) = f(a)$.  
    On note $\mathcal{C}^0$ l'ensemble des fonctions continues, c'est un espace vectoriel.
  \end{definition}
  \section{Dérivabilité}
  \begin{definition}
    Soit $f$ définie sur un ouvert $D$ de $\bR$ et $a \in D$.  
    On dit que $f$ est dérivable en $a$ si et seulement si $\lim_{x \to a} \dfrac{f(x) - f(a)}{x - a}$ existe dans $\bR$.  
    On note $f'$ la fonction $a \mapsto \lim_{x \to a} \dfrac{f(x) - f(a)}{x - a}$ définie sur l'ensemble des valeurs dérivables de $f$.
  \end{definition}
  \subsection{Dérivée successives}
  On peut ensuite étudier la dérivabilité des dérivées successives de f
  \section{Développements Limités (DL)}
  \begin{definition}
    On appelle Développement Limité (DL) à l'ordre $n$ et au point $a \in I$ d'une fonction $f$ défini sur un interval ouvert $I$ de $\bR$, un polynome $P$ tel que
    \begin{align*}
      \deg P &= n \\ 
      f(x) &= P(x-a) + o_0((x-a)^n)
    \end{align*}
    C'est une propriété \textbf{locale} de $f$ en $a$ 
  \end{definition}
  \subsection{Taylor-Young}
  \begin{theorem}[Formule de Taylor-Young]
    Soit $f$ une fonction définie de $I$ dans $\bR$, $n$ fois dérivable, alors $f$ admet un $DL_n$ pour un point $a$ de la forme 
    \begin{equation*}
      f(x) = \sum_{k=0}^n \dfrac{f^{(k)}(a)}{k!} (x-a)^k + o((x-a)^n)
    \end{equation*} 
  \end{theorem}
  \begin{rmq}
    Dans la majorité des cas pratiques, on prend $a=0$ ce qui donne
    \begin{equation*}
      f(x) = \sum_{k=0}^n \dfrac{f^{(k)}(0)x^k}{k!} + o(x^n)
    \end{equation*}
  \end{rmq}
  \begin{ex}
    En exemple on prend $f = \exp$, $\exp \in \mathcal{C}^{\infty}$ et on a $\forall n \in \bN, f^{(n)} = \exp$ donc $\forall n \in \bN, f^{(n)}(0) = 1$
    donc d'après le théorème de Taylor-Young, $\forall n \in \bN, \exp$ admet un $DL_n$ de la forme 
    \begin{align*}
      \exp(x) &= \sum_{k=0}^n \dfrac{\exp^{(k)}(0)}{k!} x^k + o(x^n) \\ 
      \exp(x) &= \sum_{k=0}^n \dfrac{x^k}{k!} + o(x^n)
    \end{align*}
  \end{ex}
  \begin{rmq}
    La formule de Taylor-Young permet aussi de faire l'inverse, de trouver la valeur d'une dérivée en un point si l'on connaît le DL de la fonction. 
    \begin{ex}
      Un exemple pour la valeur en $0$ de la dérivée quatrième de $\dfrac{1}{1-x}$ 
      \begin{equation*}
        \dfrac{1}{1-x} = 1 + x + x^2 + x^3 + x^4 + o(x^4)
      \end{equation*}
      Et d'après Taylor-Young on a 
      \begin{equation*}
        \dfrac{1}{1-x} = \dfrac{f(0)}{1} + \dfrac{f'(0)}{1}x + \dfrac{f''(0)}{2}x^2 + \dfrac{f^{(3)}(0)}{3!}x^3 + \dfrac{f^{(4)}(0)}{4!}x^4 + o(x^4)
      \end{equation*}
      Or les deux DL sont égaux, donc les polynômes aussi, et donc par identification des coefficients on a
      \begin{equation*}
        \dfrac{f^{(4)}(0)}{4!} = 1
      \end{equation*}
      ce qui donne 
      \begin{align*}
        \dfrac{f^{(4)}(0)}{4!} &= 1 \\ 
        f^{(4)}(0) = 4! = 24
      \end{align*}
      On a donc la valeur de la dérivée quatrième en $O$ sans avoir à dériver la fonction.
    \end{ex}
    En pratique ça permet l'étude des dérivées en un point sur des fonctions bien plus complexes.
  \end{rmq}
  \subsection{DL usuels}
  \begin{prop}
    Les développements limités usuels en 0 sont les suivants
    \begin{align*}
      e^x &= \sum_{k=0}^{n} \dfrac{x^k}{k!} + o(x^n) \\
      \sin x &= \sum_{k=0}^{n} \dfrac{(-1)^{k} x^{2k+1}}{(2k+1)!} + o(x^{2n+1}) \\ 
      \cos x &= \sum_{k=0}^{n} \dfrac{(-1)^{k} x^{2k}}{(2k)!} + o(x^{2n}) \\
      \dfrac{1}{1-x} &= \sum_{k=0}^{n} x^k + o(x^n) \\ 
      \dfrac{1}{1+x} &= \sum_{k=0}^{n} (-1)^k x^k + o(x^n) \\ 
      \log (1+x) &= \sum_{k=0}^n \dfrac{(-1)^k x^k}{k} + o(x^n) \\ 
      (1+x)^{\alpha} &= \sum_{k=0}^{n} \sigma_{\alpha}(k) x^k + o(x^n) && \text{avec} \\
      \alpha &\in \bR && \text{et} \\
      \sigma_{\alpha}(k) &= 
      \begin{dcases}
        1 ,& \text{si } k=0 \\ 
        \dfrac{\prod_{i=0}^{k-1} (\alpha - i)}{k!} ,& \text{sinon}
      \end{dcases}
    \end{align*}
  \end{prop}
  \begin{rmq}
    Les DL de fonctions paires (resp impaires) ne contiennent que des coefficients sur les degrès pairs (resp impairs)
    \begin{ex}
      Exemple, la fonction $\cos$ est paire
    \end{ex}
  \end{rmq}
  \subsection{Opération sur les DL}
  Sans perte de généralité, les DL sont ici en $0$  \newline
  Soit $P,Q \in R[X]$ et $f,g \in \bR^{I}$ tels que 
  \begin{align*}
    \deg P &= \deg Q = n \\ 
    f(x) &= P(x) + o(x^n) \\ 
    g(x) &= Q(x) + o(x^n)
  \end{align*}
  \subsubsection{Troncage}
  \begin{definition}
    On appelle "troncage" à l'ordre $k \leq n$ d'un DL, le polynome tronqué $F_k$ de degrès $k$ tel que 
    tous les coefficients de $F_k$ sont égaux à ceux de $F$ jusqu'au coefficient de $x^k$ et tel que 
    \begin{equation*}
      f(x) = F_k(x) + o(x^k)
    \end{equation*}
  \end{definition}
  \begin{ex}
    On a 
    \begin{equation*}
      e^x = 1 + x + \dfrac{x^2}{2} + \dfrac{x^3}{3!} + \dfrac{x^4}{4!} + \dfrac{x^5}{5!} + o(x^5)
    \end{equation*}
    le $DL_5$ de $exp$ alors on peut le "tronquer" à l'ordre $k=3\leq 5$ pour avoir le $DL_3$ de exp 
    \begin{equation*}
      e^x = 1 + x + \dfrac{x^2}{2} + \dfrac{x^3}{3!} + o(x^3)
    \end{equation*}
  \end{ex}
  \subsubsection{Somme}
  \begin{prop}
    Le $DL_n$ de la fonction $f+g$ est la somme des $DL_n$ de $f$ et de $g$ 
    \begin{equation*}
      (f+g)(x) = P(x)+Q(x) + o(x^n)
    \end{equation*}
  \end{prop}
  \subsubsection{Produit}
  \begin{prop}
    Le $DL_n$ de la fonction $fg$ est le produit des $DL_n$ de $f$ et de $g$ tronqué à l'ordre $n$ 
    \begin{equation*}
      (fg)(x) = PQ_{n}(x) + o(x^n)
    \end{equation*}
  \end{prop}
  \subsubsection{Composée}
  \begin{prop}
    Si $g(0) = 0$ alors on peut composer les $DL_n$ et le $DL_n$ de $f \circ g$ est la composition des $DL_n$ de $f$ et de $g$ tronqué à l'ordre $n$
    \begin{equation*}
      (f\circ g)(x) = (P \circ Q)_{n}(x) + o(x^n)
    \end{equation*}
  \end{prop}
  \begin{ex}
    Exemple $DL_3$ de $\sqrt{1 + \sin x}$. On a bien $\sin 0 = 0$.
    \begin{align*}
      \sin x &= x - \dfrac{x^3}{6} + o(x^3) \\ 
      (1+X)^{\alpha} &= 1 + \alpha X + \dfrac{\alpha(\alpha-1)x^2}{2} X^2 + \dfrac{\alpha(\alpha-1)(\alpha-2)}{6} X^3 + o(X^3) && \text{donc} \\ 
      (1 + \sin x)^{\frac{1}{2}} &= 1 + \dfrac{1}{2} \left(x - \dfrac{x^3}{6}\right) - \dfrac{1}{8} \left(x - \dfrac{x^3}{6}\right)^2 + \dfrac{3}{48} \left(x - \dfrac{x^3}{6}\right)^3 + o(x^9) \\ 
      (1 + \sin x)^{\frac{1}{2}} &= 1 + \dfrac{1}{2} x - \dfrac{x^3}{12} - \dfrac{1}{8} x^2 + \dfrac{3}{48} x^3 + o(x^3) && \text{tronquage} \\ 
      (1 + \sin x)^{\frac{1}{2}} &= 1 + \dfrac{1}{2} x - \dfrac{1}{8} x^2 - \dfrac{1}{48} x^3 + o(x^3) 
    \end{align*}
  \end{ex}
  \subsection{Application au calcul de dérivé}
  Les DL sont utiles pour résoudre des formes indéterminées lors du calcul de limite 
  \begin{ex}
    Calcul de la limite en 0 de la fonction $f : $ $x \mapsto \dfrac{e^{x^2} - \cos x}{x^2}$  
    On calcule les différents DL à l'ordre 4
    \begin{align*}
      e^{x^2} &= 1 + (x^2) + \dfrac{(x^2)^2}{2} + o(x^4) \\
      \cos x &= 1 - \dfrac{x^2}{2} + \dfrac{x^4}{24} + o(x^4) \\
      e^{x^2} - \cos x &= \dfrac{3}{2}x^2 + o(x^2) && \text{tronquage, inutile au delà} \\ 
      f(x) &= \dfrac{\dfrac{3}{2}x^2 + o(x^2)}{x^2} \\ 
      f(x) &= \dfrac{3}{2} + o(1) && \text{d'où} \\
      \lim_{x \to 0} f(x) &= \dfrac{3}{2}
    \end{align*}
    On voit après que l'ordre 2 aurait suffit, l'intuition peut aider pour savoir à quel ordre calculer.
  \end{ex}

  \chapter{Intégration}
  \section{Intégrales de Riemann}
  Explication des notations,
  \begin{align*}
    \int_a^b f &= \int_a^b f(x) \text{d}x \\ 
    \int_{[a,b]} f &= \int_a^b f
  \end{align*}
  \subsection{Introduction}
  Soit $a,b \in \bR$ tels que $a < b$. Soit $f$ définie et bornée sur $[a,b]$ et $d=(x_1,\cdots,x_n) \subset [a,b]$ une subdivision de $[a,b]$ pour $n \in \bN$.  
  On définie 
  \begin{align*}
    M_i &= \sup_{x\in [x_{i-1},x_i]} f(x) \\ 
    m_i &= \inf_{x\in [x_{i-1},x_i]} f(x) \\ 
    S(d) &= \sum_{i=1}^n M_i \cdot (x_i-x_{i-1}) \\ 
    s(d) &= \sum_{i=1}^n m_i \cdot (x_i-x_{i-1})
  \end{align*}
  Le but est double 
  \begin{itemize}
    \item Approcher $f$ par des fonctions en escalier 
    \item Augmenter $n$ pour augmenter la précision de l'approche
  \end{itemize}
  Et pour $d'$ une subdivision plus fine que $d$ on a 
  \begin{equation*}
    s(d) \leq s(d') \leq S(d') \leq S(d)
  \end{equation*}
  On peut définir des suites convergentes, et à l'infini on note 
  \begin{align*}
    I &= \sup_{[a,b]} S(d) \\ 
    J &= \inf_{[a,b]} s(d)
  \end{align*}
  \begin{definition}
    Une fonction $f$ est Riemann-intégrable si $I_f=J_f=\int_{a}^b f$
  \end{definition}
  \subsection{Propriétés de l'intégrale}
  On prend $f,g$ deux fonctions Riemann-intégrable définie sur $[a,b]$
  \begin{prop}
    On a
    \begin{align*}
      \int_a^a f &= 0 \\ 
      \int_b^a f &= - \int_a^b f
    \end{align*}
  \end{prop}
  \begin{prop}[Relation de Chales]
    Soit $c \in [a,b]$,
    \begin{equation*}
      \int_a^b f = \int_a^c f + \int_c^b f
    \end{equation*}
  \end{prop}
  \begin{ex}
    Exemple d'une fonction non-Riemann-intégrable. Soit $f$ la fonction indicatrice de $\bQ$ sur $[0,1]$ alors on a 
    \begin{align*}
      M_i = \sup f &= 1 \\ 
      m_i = \inf f &= 0
    \end{align*}
    D'où 
    \begin{align*}
      S(d) = x_n - x_0 &= 1 \\
      s(d) = 0
    \end{align*}
    Donc 
    \begin{equation*}
      I \not= J
    \end{equation*}
    Par conséquence, $f$ n'est pas Riemann-intégrable sur $[0,1]$
  \end{ex}
  \begin{theorem}
    Soit $f$ une fonction définie sur $[a,b]$
    \begin{enumerate}
      \item Si $f$ est $\mathcal{C}^0$ alors $f$ est Riemann-intégrable
      \item Théorème des singularités supprimable, si on modifie $f$ sur un nombre fini de point, l'intégrale n'est pas modifiée
      \item Par conséquence, les fonctions continues par morceaux ($\mathcal{M}^0$) sont aussi Riemann-intégrable
    \end{enumerate}
  \end{theorem}
  \subsection{Opération sur les intégrales}
  \begin{prop}
    Soient $f,g$ Riemann-intégrables sur $I$
    \begin{itemize}
      \item Soit $\lambda \in \bR$, alors $\int_I \lambda f = \lambda \int_I f$
      \item La fonction $(f+g)$ est Riemann-intégrable et $\int_I (f+g) = \int_I f + \int_I g$
      \item La fonction $\abs{f}$ est Riemann-intégrable
      \item La fonction $(fg)$ est Riemann-intégrable
    \end{itemize}
  \end{prop}
  \subsection{Positivité de l'intégrale}
  \begin{prop}
    Soit $f$ Riemann-intégrable sur $I$
    \begin{itemize}
      \item Si $\forall x \in I, f(x) \geq 0$ alors $\int_I f \geq 0$
      \item Si $\forall x \in I, f(x) \leq 0$ alors $\int_I f \leq 0$
    \end{itemize}
  \end{prop}
  \begin{theorem}[Positivité de l'intégrale]
    Soit $f,g$ Riemann-intégrable sur $I$ telles que 
    \begin{equation*}
      \forall x \in I, f(x) \leq g(x)
    \end{equation*}
    Alors il vient de la prop précédente que 
    \begin{equation*}
      \int_I f \leq \int_I g
    \end{equation*}
  \end{theorem}
  \begin{prop}[Généralisation de l'inégalitée triangulaire]
    Soit $f$ Riemann-intégrable sur I, on a alors 
    \begin{equation*}
      \abs{\int_I f} \leq \int_I \abs{f}
    \end{equation*}
  \end{prop}
  \subsection{Moyenne}
  Soit $f,g$ Riemann-intégrable sur $I$, on note 
  \begin{align*}
    m &= \inf_I f \\
    M &= \sup_I f
  \end{align*}
  \begin{prop}
    Si $g$ est de signe constant sur $I$ alors $\exists \mu \in [m,M], \int_I fg = \mu \int_I g$
    \begin{proof}
      On a, $\forall x \in I, m \leq f(x) \leq M$, on considère sans perte de généralité que $\forall x \in I, g(x) \geq 0$ et 
      que $\int_I g \not= 0$ 
      alors on a 
      \begin{align*}
        \forall x \in I,& m \leq f(x) \leq M \\ 
        &m g(x) \leq f(x)g(x) \leq Mg(x) \\ 
        &m\int_I g \leq \int_I fg \leq M \int_I g && \text{g est positive} \\ 
        &m \leq \dfrac{\int_I fg}{\int_I g} \leq M && \text{car }\int_I g \not= 0
      \end{align*}
      On pose $\dfrac{\int_I fg}{\int_I g} = \mu$, il vient que $\mu \in [m,M]$ et que $\mu \int_I g = \int_I fg$
    \end{proof}
  \end{prop}
  \begin{rmq}
    On prend le cas particulier où $g=1$ on a $\int_a^b f = \mu \int_a^b 1$ ce qui donne finalement
    \begin{equation*}
      \mu = \dfrac{1}{(b-a)} \int_a^b f
    \end{equation*}
    On appelle alors $\mu$ la valeur moyenne de la fonction $f$ sur $[a,b]$ 
  \end{rmq}
  \subsection{Théorème fondamental de l'analyse}
  
  \begin{prop}
    Soit $f : [a,b] \to \bR$ Riemann-intégrable, et on définie $g : [a,b] \in \bR$ telle que 
    \begin{equation*}
    \forall x \in [a,b], g(x) = \int_a^x f
    \end{equation*}
    Alors
    \begin{itemize}
      \item Si $f$ est Riemann-intégrable alors $g$ est continue
      \item Si $f$ est continue en $x_0 \in [a,b]$ alors $g$ est dérivable en $x_0$
      \item Si $f$ est continue sur $[a,b]$ alors $g$ est dérivable sur $[a,b]$ et $g' = f$
    \end{itemize}
  \end{prop}

  \begin{theorem}[Théorème fondamental de l'analyse]
    Soit $f$ une fonction $\mathcal{C}^0$ sur $I$ un interval de $\bR$, et soit $\alpha \in I$ alors $f$ admet une unique primitive $F_{\alpha}$ 
    telle que $F_{\alpha}' = f$ s'annulant en $x=\alpha$.
    De plus pour toute fonction $F$ primitive de $f$ on a $\int_a^b f = F(b) - F(a)$
  \end{theorem}
  \subsection{Primitives usuelles}
  \begin{prop}
    Les primitives usuelles sont les suivantes, par abus de notation toutes les fonctions suivantes 
    sont marquées selon leur procédure, par example $x^{\alpha}$ réfère à la fonction $(x \mapsto x^{\alpha})$ sur son plus grand interval de 
    définition, $c$ désigne une constante réelle.
    \begin{align*}
      f = x^{\alpha}, F &= \dfrac{x^{\alpha+1}}{\alpha+1} + c && \alpha \not= -1 \\ 
      f = \dfrac{1}{x}, F &= ln \abs{x} + c \\ 
      f = \dfrac{1}{\sqrt{x}}, F &= 2\sqrt{x} + c \\ 
      f = e^x, F &= e^x + c \\
      f = \cos (ax+b), F &= \dfrac{1}{a} \sin (ax+b) + c && a \not= 0 \\ 
      f = \sin (ax+b), F &= - \dfrac{1}{a} \cos(ax+b) + c && a \not= 0 \\
      f = \dfrac{1}{\cos^2 x}, F &= \tan x \\ 
      f = \dfrac{1}{x^2+a^2}, F &= \dfrac{1}{a} \arctan \dfrac{x}{a} + c && a \not= 0 
    \end{align*}
    Pour les fonctions, il faut pas oublier la règle de la composée qui donne par example
    \begin{align*}
      f = u^{\alpha} \cdot u', F &= \dfrac{u^{\alpha+1}}{\alpha+1} && \alpha \not=-1 \\ 
      f = \dfrac{u'}{u}, F &= ln \abs{u} \\
      f = \dfrac{u'}{\sqrt{u}}, F &= 2\sqrt{u}
    \end{align*}
  \end{prop}
  \subsection{Changement de variable}
  \begin{theorem}[Théorème de changement de variable]
    Soit $\varphi [a,b] \in \bR, \mathcal{C}^1$ sur $[a,b]$, et soit $f : I \in \bR \mathcal{C^0}$ sur $I$ alors on a la formule suivante
    \begin{equation*}
      \int_a^b f \circ \varphi \cdot \varphi' = \int_{\varphi(a)}^{\varphi(b)} f
    \end{equation*}
  \end{theorem}
  \begin{ex}
    Calculons, $I = \int_0^1 \dfrac{x \text{d}x}{\sqrt{d - x^2}}$
    On pose $t = 2 - x^2$ ce qui est bien $\mathcal{C}^1$ alors on a $\text{d}t = -2x \text{d}x$ 
    donc par changement de variable,
    \begin{align*}
      I &= \int_2^1 \dfrac{\text{d}t}{-2\sqrt{t}} \\ 
        &= \int_1^2 \dfrac{\text{d}t}{2\sqrt{t}} \\ 
        &= \left[2\sqrt{x}\right]_1^2 \\ 
        &= \sqrt{2} - 1
    \end{align*}
  \end{ex}
  \subsection{Intégration par parties}
  \begin{theorem}[Théorème d'intégration par parties]
    Soit $u,v$, $\mathcal{C}^1$ sur $[a,b]$ alors on a 
    \begin{equation*}
      \int_a^b uv' = [uv]_a^b - \int_a^b u'v
    \end{equation*}
  \end{theorem}
  \begin{ex}
    Exemple calculons $I = \int_0^1 xe^x \text{d}x$
    On pose $u(x) = x$ donc $u'(x) = 1$ et donc $v'(x) = e^x$ ce qui donne $v(x) = e^x$ ce qui sont bien $C^1$,
    donc par IPP on a 
    \begin{align*}
      I &= [xe^x]_0^1 - \int_0^1 e^x \text{d}x \\ 
      &= e - (e - 1) \\ 
      &= 1
    \end{align*}
  \end{ex}
  \section{Intégrales Généralisées}
  Il existe deux cas d'intégrales généralisées
  \begin{enumerate}
    \item Le cas où l'on intègre une fonction bornée sur un intervalle non borné (de forme $[a,b[$)
    \item Le cas où l'on intègre une fonction non bornée sur un intervalle bornée (de forme $[a,b]$)
  \end{enumerate}
  \begin{definition}
    Soit $[a,b[$ tel que $-\infty < a < b \leq +\infty$.
    Soit $f : [a,b[ \to \bR$. On prend l'application $I(\lambda) = \int_a^{\lambda} f$ définie sur $[a,b[$
    \begin{itemize}
      \item Si $I(\lambda)$ converge en $b^{-}$ alors $f$ est intégrable sur $[a,b[$, 
      on note $\lim_{\lambda \to b^{-}} I(\lambda) = \int_a^b f$ et on appelle 
      le scalaire $\int_a^b f$ \textbf{intégrale généralisée} de $f$ sur $[a,b[$
      \item Si $I(\lambda)$ diverge en $b^{-}$ alors $f$ n'est pas intégrable sur $[a,b[$
    \end{itemize}
  \end{definition}
  \begin{ex}
    On cherche à connaître la nature de l'intégrale de $\left(x \mapsto \dfrac{1}{x^2}\right)$ sur $[1,+\infty[$
    \begin{align*}
      I(\lambda) &= \int_1^{\lambda} \dfrac{\text{d}x}{x^2} \\ 
                &= - \left[\dfrac{1}{x}\right]_1^{\lambda} \\ 
                &= - \dfrac{1}{\lambda} + 1 \to_{\infty} 1
    \end{align*}
    Donc $\int_1^{\infty} \dfrac{\text{d}x}{x^2}$ existe et vaut $1$
  \end{ex}
  \begin{ex}
    On cherche à connaître la nature de l'intégrale de $\left(x \mapsto \cos x\right)$ sur $[0,\infty[$.
    \begin{align*}
      I(\lambda) &= \int_1^{\lambda} \cos x \text{d}x \\ 
                &= \left[\sin x\right]_1^{\lambda} \\ 
                &= - \sin \lambda && \text{DV}
    \end{align*}
    Donc $\left(x \mapsto \cos x\right)$ n'est pas intégrable sur $[0,\infty[$
  \end{ex}
  \begin{rmq}
    Soit $c \in [a,b[$ alors $\int_a^b f$ et $\int_c^b f$ sont de même nature, et sont notés en général $\int^b f$
  \end{rmq}
  \begin{rmq}
    Si on a $a = \infty$ ou $f$ non définie en $a$ on sépare l'étude en plusieurs sous problèmes
  \end{rmq}
  \subsection{Cas des fonctions réelles positives}
  Dans la section $f$ est une fonction réelle positive définie sur $[a,b[$
  \subsubsection{Majoration}
  \begin{prop}
    l'intégrale de $f$ sur $[a,b[$ CV $\Leftrightarrow \int_a^{\lambda} f$ majorée
    \begin{proof}
      \begin{equation*}
        I(\lambda) = \int_{a}^{\lambda} f
      \end{equation*}
      On a $I$ qui est croissante sur $[a,b[$ d'après le théorème des limites monotones alors 
      \begin{itemize}
        \item si $I$ est majorée alors $I(\lambda) \to \mu \in \bR$ et $f$ est intégrable sur $[a,b[$
        \item si $I$ n'est pas majorée alors $I(\lambda) \to \infty$ donc $f$ n'est pas intégrable sur $[a,b[$
      \end{itemize}
    \end{proof}
  \end{prop}
  \subsubsection{Comparaison}
  \begin{prop}[Théorème de comparaison]
    Soit $g : [a,b[ \to \bR$ tel que $0 \leq f \leq g$ alors 
    \begin{itemize}
      \item Si $g$ est intégrable sur $[a,b[$ alors $f$ l'est 
      \item Si $f$ n'est pas intégrable sur $[a,b[$ alors $g$ ne l'est pas
    \end{itemize}
  \end{prop}
  \subsubsection{Equivalent}
  \begin{prop}
    Soit $g : [a,b[ \to \bR$ tel que $f \equiv_b g$ alors $\int^b f$ et $\int^b g$ sont de même nature
  \end{prop}
  \subsection{Cas des fonction réelles positives et où $b=\infty$}
  \begin{prop}
    Si $f \not\to 0$ alors $f$ n'est pas intégrable sur $[a,\infty[$
    \begin{proof}
      Supposons que $f \to l \not= 0$ alors $f \equiv l$ donc $\int^{\infty} f$ est de même nature que $\int^{\infty} l \text{d}x$ donc 
      $\int^{\infty} f$ DV
    \end{proof}
  \end{prop}
  \subsubsection{Critère de Riemann}
  \begin{theorem}[Critère de Riemann]
    La fonction $(x \mapsto \frac{1}{x^{\alpha}})$ est : 
    \begin{itemize}
      \item intégrable $\Leftrightarrow \alpha > 1$
      \item pas intégrable $\Leftrightarrow \alpha \leq 1$ 
    \end{itemize}
    \begin{proof}
      \begin{align*}
        I(\lambda) &= \int_1^{\lambda} \frac{\text{d}x}{x^{\alpha}} && \alpha \not= 1 \\
                  &= \left[\frac{x^{1 - \alpha}}{1 - \alpha}\right]_1^{\lambda} \\ 
                  &= \frac{1}{(1-\alpha)\lambda^{\alpha-1}} - \frac{1}{1-\alpha}
      \end{align*}
      Donc $I$ ne converge que si $\alpha > 1$ et en retour si $\alpha > 1$ alors $I$ converge.
    \end{proof}
  \end{theorem}
  \begin{ex}
    Cherchons la nature de $\int_0^{\infty} \frac{2x+1}{\sqrt{x^4+8}}$ 
    \begin{align*}
      \frac{2x+1}{\sqrt{x^4+8}} &\equiv \frac{2x}{\sqrt{x^4}} \\ 
                                &\equiv \frac{2}{x}
    \end{align*}
    Donc d'après le critère de Riemann $\int_0^{\infty} \frac{2x+1}{\sqrt{x^4+8}}$ DV
  \end{ex}
  \subsubsection{Règle de Riemann}
  \begin{prop}[Règle de Riemann]
    Soit $f$ une fonction définie sur $[a,\infty[$
    \begin{itemize}
      \item Si il existe $\alpha > 1$ tel que $x^{\alpha}f(x) \to l \in \bR$ alors $\int^{\infty} f$ CV
      \item Si il existe $\alpha \leq 1$ tel que $x^{\alpha}f(x) \to l \in \bar{\bR^{*}}$ alors $f$ n'est pas intégrable     
    \end{itemize}
    \begin{proof}
      Conséquence du critère de Riemann.
    \end{proof}
  \end{prop}
  \begin{ex}
    Est-ce que $(x \mapsto \sqrt{x}e^{-x})$ est intégrable sur $[0,\infty[$
    \begin{align*}
      x^2*\sqrt{x}e^{-x} &= \frac{x^{\frac{5}{2}}}{e^x} \\ 
                        &\to 0 && \text{CC} 
    \end{align*}
    Donc d'après la règle de Riemann, $\int^{\infty} \sqrt{x}e^{-x}$ CV
  \end{ex}
  \subsection{Cas où b est fini}
  On note 
  \begin{equation*}
    g(x) = \frac{1}{(b-x)^{\alpha}}, \alpha > 0
  \end{equation*}
  \begin{prop}
    Nature de $\int_a^b g$. 
    \begin{align*}
      I(\lambda) &= - \frac{1}{(1-\alpha)} \left((b-\lambda)^{1-\alpha} - (b-a)^{1 - \alpha}\right)
    \end{align*}
    Donc
    \begin{itemize}
      \item si $\alpha < 1$ alors $(b - \lambda)^{1-\alpha} \to 0$ donc $I$ CV
      \item si $\alpha > 1$ alors $(b - \lambda)^{1-\alpha} \to \infty$ donc $I$ DV
      \item si $\alpha = 1$ alors $I \to \infty$ donc $I$ DV
    \end{itemize}
  \end{prop}
  \subsubsection{Critère de Riemann}
  \begin{theorem}[Critère de Riemann - Version finie]
    \begin{equation*}
      \int_a^b \frac{\text{d}x}{(b-x)^{\alpha}} \text{ CV } \Leftrightarrow \alpha < 1
    \end{equation*}.
    \begin{proof}
      Voir ci dessus.
    \end{proof}
  \end{theorem}
  \begin{prop}
    Par conséquent si $f \equiv \frac{A}{(b-x)^{\alpha}}$ alors $\int^b f$ CV ssi $\alpha < 1$
    \begin{proof}
      Conséquence directe du critère de Riemann.
    \end{proof}
  \end{prop}
  \begin{ex}
    Nature de $\int_0^1 \frac{x+1}{\sqrt{x}}$ on a 
    \begin{align*}
      \frac{x+1}{\sqrt{x}} &\equiv \frac{2}{\sqrt{x}} \\
                          &\equiv \frac{2}{x^{\frac{1}{2}}}
    \end{align*}
    or $\frac{1}{2} < 1$ donc $\int_0^1 \frac{x+1}{\sqrt{x}}$ CV
  \end{ex}
  \subsubsection{Règle de Riemann}
  \begin{prop}[Règle de Riemann - Version finie]
    Soit $f$ une fonction définie sur $[a,b[$ 
    \begin{itemize}
      \item Si il existe $\alpha < 1$ tel que $x^{\alpha}f(x) \to l \in \bR$ alors $\int^{\infty} f$ CV
      \item Si il existe $\alpha \geq 1$ tel que $x^{\alpha}f(x) \to l \in \bar{\bR^{*}}$ alors $f$ n'est pas intégrable     
    \end{itemize}
    \begin{proof}
      Conséquence du critère de Riemann.
    \end{proof}
  \end{prop}
  \subsection{Cas des fonctions de signes qql}
  \begin{definition}
    On dit que l'intégrale de $f$ est simplement convergente si et seulement si $I(\lambda)$ a une limite et si
    \begin{equation*}
      \lim_{x \to b^{-}} \int_a^x f \in \bR
    \end{equation*}
  \end{definition}
  \begin{definition}
    On dit que l'intégrale de $f$ est absolument convergente si et seulement si $\int_a^x \abs{f} \to \mu \in \bR$ 
  \end{definition}
  \begin{theorem}[Comparaison]
    Soient $a \in \bR$, $b \in \bar{\bR}$ tel que $a < b$ et $f$ une fonction définie sur $[a,b[$
    \begin{enumerate}
      \item Si l'intégrale de $f$ est absolument convergente alors l'intégrale de $f$ est simplement convergente
      \item Le résultat $\abs{\int f} \leq \int \abs{f}$ est étendu aux intégrales généralisées
    \end{enumerate}
    \begin{proof}
      Cette preuve est \textit{\textbf{hors du programme présenté lors du cours}}, je la trouve utile mais on peut la sauter c'était juste pour donner une idée 
      de à quoi peut ressembler ce genre de preuve.\newline 


      \begin{enumerate}
        \item Notons, $f^{-} = \max(-f,0)$ et $f^{+} = \max(f,0)$. On observe que 
      \begin{align*}
        f^{-} + f^{+} &= \max(-f,0) + \max(f,0) \\ 
                      &= \abs{f}
      \end{align*}
      Et on a aussi
      \begin{align*}
        f^{+} - f^{-} &= \max(f,0) - \max(-f,0) \\ 
                     &= f
      \end{align*}
      On suppose que $\int \abs{f}$ est convergente donc 
      \begin{equation*}
        \exists \mu \in \bR, \lim_{x \to b^{-}} \int_a^x (f^{+}+f^{-}) = \mu
      \end{equation*}
      Or Les fonctions $f^{+}$ et $f^{-}$ sont à valeurs positives, donc
      \begin{align*}
        \exists (\mu_1,\mu_2) \in \bR^2, &\lim_{x \to b^{-}} \int_a^x (f^{+}) = \mu_1 && \text{et} \\ 
                                         &\lim_{x \to b^{-}} \int_a^x (f^{-}) = \mu_2
      \end{align*}
      Donc par linéarité, 
      \begin{equation*}
        \lim_{x \to b^{-}} \int_a^x (f^{+}-f^{-}) = \mu_1 - \mu_2 \in \bR
      \end{equation*}
      Ce qui revient à 
      \begin{equation*}
        \exists \lambda \in \bR, \lim_{x \to b^{-}} \int_a^x f = \lambda
      \end{equation*}
      Donc l'intégrale de $f$ est simplement convergente.\newline

        \item Supposons que l'intégrale de $f$ est absolument convergente, rappelons d'abord que sur les intégrales de Riemann on a 
      pour $(\alpha,\beta) \in \bR^2$ tels que $a < b$ et $g$ $\mM^0$ sur $[a,b]$
      \begin{equation}
        \abs{\int_{\alpha}^{\beta} g} \leq \int_{\alpha}^{\beta} \abs{g}
      \end{equation}
      Puis,
      \begin{align*}
        \forall x \in [a,b[, \abs{\int_a^x f} &= \abs{\int_a^x (f^{+} - f^{-})} \\ 
        &\leq \abs{\int_a^x f^{+}} + \abs{\int_a^x f^{-}} && \textit{IT} \\ 
        &\leq \int_a^x \abs{f^{+}} + \int_a^x \abs{f^{-}} && (1)\\ 
        &\leq \int_a^x f^{+} + f^{-} && \text{valeurs positives} \\
        &\leq \int_a^x \abs{f}
      \end{align*}
      Enfin par passage à la limite avec $x \to b^{-}$ (les limites existe avec la démonstration du 1.)
      \begin{equation*}
        \abs{\int_a^b f} \leq \int_a^b \abs{f}
      \end{equation*}
    \end{enumerate}
    \end{proof}
  \end{theorem}
  \begin{ex}
    Nature de $\int_0^1 \sin \frac{1}{x} \text{d}x$. 
    On a 
    \begin{equation*}
      \forall x \in ]0,1] \leq 1 
    \end{equation*}
    Donc l'intégrale est absolument convergente donc l'intégrale est convergente
  \end{ex}
  \begin{ex}
    Nature de $\int_0^{\infty} \frac{\cos x}{1+x^2}$ par le même raisonnement absolument convergente donc convergente.
  \end{ex}



\chapter{Séries numériques}
\section{Introduction aux séries numériques}
\begin{definition}
  Soit $(u_n) \in \bR^{\bN}$ et $S_n = \sum_{i=0}^n u_i$ alors la suite $(S_n)_{n \in \bN}$ est appélée \textit{série de terme général $u_n$} 
  notée $\sum u_n$.
  
  De plus si la suite $(S_n)$ converge alors on note $S = \ln \sum_{k=0}^n u_k$, notée aussi $\sum_{k=0}^{\infty} u_k$ 
  appelée \textit{somme de la série}. 
  
  On dit alors que la série de terme général $u_n$ est convergente. Dans le cas contraire on dit qu'elle est divergente. 
\end{definition}
\begin{definition}
  On appelle $S_n = \sum_{k=0}^n u_k$ somme partielle de la série de terme général $u_n$, et si la série est convergente alors on note 
  $R_n = S - S_n$ le reste de la série.
\end{definition}
\begin{rmq}
  On a donc $\ln R_n = 0$
\end{rmq}
\begin{rmq}
  Les séries $\sum_{0} u_n$ et $\sum_{n_0} u_n$ sont de même nature d'où la notation $\sum u_n$ 
\end{rmq}
\begin{prop}[Condition nécessaire de convergence]
  \begin{equation*}
    \left(\sum u_n \text{ CV}\right) \Rightarrow \ln u_n = 0
  \end{equation*}

  
  \begin{proof}
    On observe que 
    \begin{equation*}
      S_{n+1} - S_n = u_n
    \end{equation*}
    Donc, en supposant que $\sum u_n$ CV on a 
    \begin{align*}
      u_n &\to S - S \\ 
          &= 0
    \end{align*}
    Donc $\ln u_n = 0$
  \end{proof}
\end{prop}
\begin{rmq}
  Cette condition est surtout utilisé pour montrer qu'il n'y a pas convergence (contraposée), lorsque $u_n \not\to 0$ on dit 
  que $\sum u_n$ est \textit{grossièrement divergente}.
\end{rmq}
\begin{ex}
  Soit $u_n = \dfrac{1}{n(n+1)}$. Soit $n \geq 1$ on a alors
  \begin{equation*}
    \dfrac{1}{n(n+1)} = \dfrac{1}{n} - \dfrac{1}{n+1}
  \end{equation*}
  Donc par somme téléscopique on obtient que 
  \begin{equation*}
    \sum_{k=1}^{n} \dfrac{1}{n(n+1)} = 1 - \dfrac{1}{n+1} \to_{n \to \infty} 1 
  \end{equation*}
  Donc la série de terme générale $u_n$ est convergente et sa somme vaut $1$
\end{ex}
\begin{ex}
  Soit $u_n = \log \left(1 + \frac{1}{n}\right)$, alors on a 
  \begin{align*}
    u_n &= \log \frac{n+1}{n} \\ 
        &= \log n+1 - \log n
  \end{align*}
  Donc,
  \begin{align*}
    \sum_{k=1}^{n} u_n &= \sum_{k=1}^n \log n+1 - \log n \\
                       &= \log n+1 - \log 1 && \text{somme téléscopique} \\ 
                       &= \log n+1 \\
                       &\to \infty
  \end{align*}
  Donc la série de terme générale $u_n$ est divergente
\end{ex}
\subsection{Sommes de séries numériques}
\begin{prop}[Somme]
  Soit $(v_n) \in \rN$ et $(u_n) \in \rN$ et $w_n = u_n + v_n$ 
  \begin{itemize}
    \item Si $\sum u_n$ et $\sum v_n$ convergent alors $\sum w_n$ converge
    \item Si $\sum u_n$ CV (resp DV) et $\sum v_n$ DV (resp CV) alors $\sum w_n$ diverge
    \item Si $\sum u_n$ et $\sum v_n$ divergent alors on ne peut rien dire de $\sum w_n$
  \end{itemize}
\end{prop}
\section{Séries géométriques}
\begin{definition}
  Soit $r \in \bR$ alors on appelle suite géométrique une suite de la forme $u_n = r^n$ et alors 
  la série de terme général $u_n$ est appelée \textit{série géométrique} de raison $r$
\end{definition}
\begin{theorem}[Théorèmes des Séries Géométriques]
  Soit $u_n = r^n$ et $\sum u_n$ la série associée, alors on a 
  \begin{equation*}
    \sum u_n \text{ CV } \Leftrightarrow \abs{r} < 1 
  \end{equation*}
  Et dans ce cas alors on a 
  \begin{equation*}
    \sum_{n=0}^{\infty} u_n = \frac{1}{1-r}
  \end{equation*}
  \begin{proof}
    Soit $r \in \bR$, soit $n \in \bN$ 
    \begin{align*}
      (1-r)S_n &= (1-r)\cdot \sum_{i=0}^n r^n \\
               &= \sum_{i=0}^n (1-r)r^n && (1-r) \in \bR \\ 
               &= \sum_{i=0}^n r^n - r^{n+1} \\ 
               &= 1 - r^{n+1} && \text{somme téléscopique} \\
               &\to \deq{1}{\abs{r} < 1}{\text{DV}}
  \end{align*}
  D'où dans le cas convergent 
  \begin{equation*}
    S = \frac{1}{1-r}
  \end{equation*}
  \end{proof}
\end{theorem}
\section{Séries À Termes Positifs (SATP)}
\begin{definition}
  Soit $(u_n) \in \rNp$ on appelle alors la série de terme générale $\sum u_n$ une \textit{série à terme positifs}, abrégés \textit{SATP}
\end{definition}
\subsection{Introduction}
\begin{prop}
  Soit $(u_n) \in \rNp$ 
  \begin{equation*}
    \left(\sum u_n \text{ CV}\right) \Leftrightarrow (S_n)_{n\in \bN} \text{ majorée}
  \end{equation*}
  \begin{proof}
    $\sum (u_n)$ est une \satp donc la suite $(S_n)_{n\in \bN}$ des sommes partielles est croissante donc par 
    théorème des limites monotones $(S_n)_{n\in \bN}$ converge si et seulement si elle est majorée
  \end{proof}
\end{prop}
\subsection{Comparaison}
\begin{theorem}[Théorème de Comparaison des SATP]
  Soit $(u_n) \in \rNp$ et $(v_n) \in \rNp$ tels que 
  \begin{equation*}
    \forall n \in \bN, 0 \leq u_n \leq v_n
  \end{equation*}
  On a,
  \begin{enumerate}
    \item Si $\sum v_n$ CV alors $\sum u_n$ CV
    \item Si $\sum u_n$ DV alors $\sum v_n$ DV
  \end{enumerate}
  \begin{proof}
    Conséquence du théorème des limites monotones 
  \end{proof}
\end{theorem}
\begin{prop}
  Soit $(u_n),(v_n) \in \rNp$ telles que 
  \begin{equation*}
    u_n \equiv_{\infty} v_n
  \end{equation*}
  Alors $\sum u_n$ et $\sum v_n$ sont de même nature
\end{prop}
\begin{ex}
  Soit $u_n = \ln \left(1 + \frac{1}{n}\right)$ on a $u_n \equiv \frac{1}{n}$ et $\sum u_n$ DV donc la série 
  $\sum \frac{1}{n}$ est divergente
  \begin{rmq}
    La série $\sum \frac{1}{n}$ est appelée \textit{série harmonique}
  \end{rmq}
\end{ex}

\subsection{Liaison séries intégrales}
\begin{theorem}[Comparaison Série/Intégrale]
  Soit $f\geq 0$, $\mC^0$, et décroissante à partir d'un réel $x_0$, alors $\sum f(n)$ et $\int^{\infty} f$ sont de même nature
  \begin{proof}
    Soit $f\geq 0$, $\mC^0$ et décroissante, soit $n \in \bN$ on observe que 
    \begin{align*}
      I(n+1) &\leq S_n && {et} \\ 
      S_n &\leq f(0) + I(n)
    \end{align*}
    Donc si $\sum f(n)$ converge alors par comparaison à termes positifs on a $I(n)$ qui converge par la première inégalité, et 
    si $\sum f(n)$ diverge alors par la deuxième inégalité $I(n)$ diverge. Donc $I$ et $\sum f(n)$ sont de même nature
  \end{proof}
\end{theorem}

\subsection{Séries de Riemann}
\begin{theorem}[Critère de Riemann]
  \begin{equation*}
    \sum \frac{1}{n^{\alpha}} \text{ CV} \Leftrightarrow \alpha > 1 \text{.}
  \end{equation*}
  \begin{proof}
    Soit $\alpha \in \bR$ soit $f(x) = \frac{1}{x^{\alpha}}$, $f$ est $\mC^0$, à terme positifs et décroissante 
    donc par comparaison série intégrale $\int f$ et $\sum f(n)$ sont de même nature, or par critère de Riemann dans les 
    intégrales $\int f$ converge si et seulement si $\alpha > 1$ donc $\sum f(n) = \sum \frac{1}{n^{\alpha}}$ converge si et 
    seulement si $\alpha > 1$.
  \end{proof}
\end{theorem}
\begin{rmq}
  Si $u_n \equiv \frac{A}{n^{\alpha}}$ alors $\sum u_n$ CV si et seulement si $\alpha > 1$ 
\end{rmq}
\begin{prop}[Règle de Riemann]
  Soit $(u_n) \in \rNp$, alors 
  \begin{itemize}
    \item Si il existe $\alpha > 1$ tel que $n^{\alpha}u_n \to l \in \bR$ alors $\sum u_n$ CV
    \item Si il existe $\alpha \leq 1$ tel que $n^{\alpha}u_n \to l \in \bar{\bR^{*}}$ alors $\sum u_n$ DV
  \end{itemize}
  \begin{proof}
    Conséquence du critère de Riemann.
  \end{proof}
\end{prop}
\subsection{Règle de Cauchy}
\begin{theorem}[Règle de Cauchy]
  Soit $(u_n) \in \rNp$, telle que 
  \begin{equation*}
    \ln \sqrt[n]{u_n} = l \in \bR
  \end{equation*}
  On a 
  \begin{itemize}
    \item Si $l < 1$ alors $\sum u_n$ CV
    \item Si $l > 1$ alors $\sum u_n$ DV 
    \item Si $l=1$ on ne peut pas déterminer la nature de $\sum u_n$ par cette méthode
  \end{itemize}
  \begin{proof}
    Soit $(u_n) \in \rNp$ telle que $\ln \sqrt[n]{u_n} = l \in \bR$ alors 
    \begin{equation*}
      \forall \varepsilon > 0, \exists n_0 \in \bN, \forall n \geq n_0, \abs{\sqrt[n]{u_n} - l} \leq \varepsilon
    \end{equation*}
    Donc 
    \begin{equation*}
      \forall \varepsilon > 0, \exists n_0 \in \bN, \forall n \geq n_0, l - \varepsilon \leq \sqrt[n]{u_n} \leq l + \varepsilon
    \end{equation*}


    Supposons $l < 1$ alors prenons $\varepsilon$ tel que $\varepsilon+l < 1$, on prend le $n_0$ associé, et soit $n \geq n_0$
    \begin{align*}
      \sqrt[n]{u_n} \leq l + \varepsilon < 1
    \end{align*}
    Donc 
    \begin{align*}
      (\sqrt[n]{u_n})^{n} &\leq \left(l + \varepsilon\right)^n \\
      u_n &\leq \left(l + \varepsilon\right)^n
    \end{align*}
    Or $\abs{l +\varepsilon} < 1$ donc par théorème des séries géométriques $\sum (l+\varepsilon)^n$ converge, donc 
    par comparaison de \satp, $\sum u_n$ converge.\newline

    
    Supposons maintenant que $l > 1$ alors prenons $\varepsilon$ tel que $l - \varepsilon > 1$, et soit $n \geq n_0$
    \begin{align*}
      \left(l - \varepsilon\right) &\leq \sqrt[n]{u_n} \\ 
      \left(l - \varepsilon\right)^n &\leq \sqrt[n]{u_n}^n \\
      \left(l - \varepsilon\right)^n &\leq u_n
    \end{align*}
    Or $\abs{l - \varepsilon} > 1$ donc par théorèmes des séries géométriques $\sum (l-\varepsilon)^n$ diverge, donc 
    par comparaisons de \satp, $\sum u_n$ diverge.
  \end{proof}
\end{theorem}
\subsection{Règle d'Alembert}
\begin{theorem}[Règle d'Alembert]
  Soit $(u_n) \in \rNp$ tel que 
  \begin{equation*}
    \ln \frac{u_{n+1}}{u_n} = l \in \bR
  \end{equation*}
  On a 
  \begin{itemize}
    \item Si $l < 1$ alors $\sum u_n$ CV 
    \item Si $l > 1$ alors $\sum u_n$ DV 
    \item Si $l=1$ alors on ne peut pas déterminer la nature de $\sum u_n$ par cette méthode
  \end{itemize}
  \begin{proof}
    Soit $(u_n) \in \rNp$ telle que $\ln \frac{u_{n+1}}{u_{n}} \to l \in \bR$.\newline

    
    Supposons que $l > 1$ alors a que $\exists n_0 \in \bN, \forall n \geq n_0, u_{n+1} > u_{n}$. 
    Donc $(u_n)$ est strictement croissante \apcr, donc par le théorème des limites monotones $\ln u_n = \infty$ donc 
    la série $\sum u_n$ est grossièrement divergente.\newline


    Supposons que $l < 1$ alors soit $q \in \bR$ tel que $l < q < 1$. On a qu'il existe un rang $n_0$ tel que 
    $l < q$ ainsi on a \apcr
    \begin{equation*}
      \frac{u_{n+1}}{u_n} < q 
    \end{equation*}
    Donc par produit téléscopique on a que la suite $(u_n)$ est majorée par la suite $v = (u_{n_0}q^{n-n_0})$, 
    or $\abs{q} < 1$ donc par théorème des séries géométriques $\sum v_n$ converge, donc par comparaison de séries à termes 
    positifs $\sum u_n$ converge. 
  \end{proof}
\end{theorem}
\section{Séries de signes non constant}
\begin{definition}
  Soit $(u_n) \in \rN$, Soit $\sum u_n$ la série de terme général $u_n$.
  \begin{itemize}
    \item Si $\exists \lambda \in \bR, \ln \sum_{k=0}^n u_k = \lambda$ alors $\sum u_n$ est dite \textit{simplement convergente}
    \item Si $\exists \mu \in \bR, \ln \sum_{k=0}^n \abs{u_k} = \mu$ alors $\sum u_n$ est dite \textit{absolument convergente}
  \end{itemize}
\end{definition}
  \begin{theorem}
    La convergence absolue implique la convergence simple 
    \begin{proof}
      voir la preuve du théorème analogue pour les suites
    \end{proof}
  \end{theorem}
  \begin{ex}
    Soit $u_n = \frac{\cos n^2}{n^4}$.\newline 

    On a $\forall n \in \bN, \abs{\frac{\cos n^2}{n^4}} \leq \frac{1}{n^4}$, or par critère de Riemann la série $\sum \frac{1}{n^4}$ converge 
    donc par comparaison de série à terme positive la série $\sum \abs{\frac{\cos n^2}{n^4}}$ converge donc la série
    $\sum \frac{\cos n^2}{n^4}$ est absolument convergente donc elle converge simplement.
  \end{ex}
  \subsection{Séries Alternées}
  \begin{definition}
    Soit $(u_n) \in \rN$ on dit que la série $\sum u_n$ est \textit{alternée} si et seulement si la suite $(-1)^nu_n$ est de signe constant
  \end{definition}
  \begin{theorem}[Critère Spécial des Séries Alternées (CSSA)]
    Soit $(u_n) \in \rN$ telle que $\sum u_n$ soit une série alternée, 
    si la suite $(\abs{u_n})$ est décroissante et que $\ln u_n = 0$ alors $\sum u_n$ converge
    \begin{proof}
      Considérons, sans perte de généralitée que $u_0 > 0$ alors on a 

      \begin{align*}
        S_n &= u_0 + u_1 + \cdots + u_n \\ 
         &= \abs{u_0} - \abs{u_1} \cdots (-1)^n u_n && \text{il vient} \\ 
        S_{n} - S_{n-2} &= (-1)^{n-1} \abs{u_{n-1}} + (-1)^n \abs{u_n} \\ 
        &= (-1)^n \left(\abs{u_n} - \abs{u_{n-1}}\right)
      \end{align*}
      
      Or par hypothèse $\abs{u_n}$ est décroissante donc la quantité $\abs{u_n} - \abs{u_{n-1}}$ est négative.\newline 
      \begin{itemize}
        \item Si $n$ est pair alors $n=2p$ et la quantité $S_{2p} - S_{2p-2}$ est négative donc la suite $(S_{2p})$ est décroissante
        \item Si $n$ est impair alors $n=2p+1$ et la quantité $S_{2p+1} - S_{2p-1}$ est positive donc la suite $(S_{2p+1})$ est croissante
        \item De plus $S_{2p+1}-S_{2p} = u_{2p+1} \to 0$ 
      \end{itemize}
      Donc les suites $(S_{2p})$ et $(S_{2p+1})$ sont adjacente, donc 
      \begin{equation}
        \exists S \in \bR, \lim_{p \to \infty} S_{2p} = \lim_{p \to \infty} S_{2p+1} = S
      \end{equation}
      Donc par théorème des indices pair et impair la suite $(S_n)$ converge donc la série $\sum u_n$ est convergente
    \end{proof}
  \end{theorem}
  \begin{ex}
    Soit $u_n = \frac{(-1)^n}{n+1}$ on a $\abs{u_n} = \frac{1}{n+1}$ donc décroissante, et $\ln u_n = 0$ car $(-1)^n$ est bornée
    donc par critère spécial des séries alternées la série $\sum u_n$ converge 
  \end{ex}

\chapter{Séries Entières}
\begin{definition}
  Soit $(u_n) \in \rN$ on définie $(f_n)_{n\in\bN}$ par \vfunc{f_n}{\bR}{\bR}{x}{a_nx^n} on appelle la série de terme générale $f_n(x)$ 
  soit la suite $\left(\sum_{k=0}^n f_k(x)\right)_{n\in \bN}$ \textit{série entière centrée en 0} ou plus simplement \textit{série entière}.\newline

  Le but de l'étude est de trouver l'ensemble $D$ pour lesquels $\forall x \in D, \sum f_n(x)$ converge, autrement dit 
  l'ensemble de définition de la fonction $\left(x \mapsto \sum_{n=0}^{\infty} a_n x^n\right)$ 
\end{definition}
\begin{rmq}
  Toute série entière converge pour $x=0$
\end{rmq}
\begin{rmq}
  Soit $(x_1,x_2) \in \bR^2$ tels que $\abs{x_1} < \abs{x^2}$ alors $\abs{a_nx_1^n} < \abs{a_nx_2^n}$ donc on en tire les conclusions suivantes 
  \begin{itemize}
    \item Si $\sum a_n x_2^n$ est absolument convergente alors $\sum a_n x_1^n$ est ACV 
    \item Si $\sum a_n x_1^n$ est divergente alors $\sum a_nx_2^n$ est DV
  \end{itemize}
\end{rmq}
\section{Domaine de convergence}
\begin{theorem}[Lemme d'Abel]
  Si il existe $x_0 \not=0$ tel que $(a_nx_0^n)$ soit borné alors $\forall x \in \bR$ tels que $\abs{x} < \abs{x_0}$, $\sum a_n x^n$ ACV 
  \begin{proof}
    Soit $x \in \bR$ tel que $\abs{x} < \abs{x_0}$ alors 
    \begin{align*}
      \abs{a_nx^n} &= \abs{a_nx_0^n} \cdot \abs{\frac{x}{x_0}}^n \\ 
      &\leq M \abs{\frac{x}{x_0}}^n
    \end{align*}
    Or $\sum \left(\frac{x}{x_0}\right)^n$ est ACV car $\abs{\frac{x}{x_0}} < 1$ donc $\sum a_n x^n$ est absolument convergente.
  \end{proof} 
\end{theorem}
\begin{rmq}
  Si il existe $x_0 \not=0$ tel que $\sum (a_nx_0^n)$ soit simplement convergente alors $\forall x \in \bR$ tels que $\abs{x} < \abs{x_0}$, $\sum a_n x^n$ ACV 
  \begin{proof}
    La convergence simple implique que la suite est borné \apcr
  \end{proof}
\end{rmq}
\begin{rmq}
  Par contraposée on obtient que si il existe $x_0 \not 0$ tel que $\sum a_n x_0^n$ DV alors $\forall x \in \bR$ tels que $\abs{x} > \abs{x_0}$, $\sum a_n x^n$ DV
\end{rmq}
\section{Rayon et intervalle de convergence}
\begin{definition}
  Pour toute série entière $\sum a_n x^n$ il existe $R \in \bar{\bR}$ tel que $R >= 0$ et 
  \begin{itemize}
    \item $\forall x \in \bR$ tels que $\abs{x} < R$ la série entière $\sum a_n x^n$ est ACV
    \item $\forall x \in \bR$ tels que $\abs{x} > R$ la série entière $\sum a_n x^n$ est DV
  \end{itemize}
  On appelle $R$ le \textit{rayon de convergence} de la série entière $\sum a_n x^n$ et l'intervalle $]-R,R[$ est appelé \textit{intervalle de convergence}.
\end{definition}
\begin{rmq}
  Pour une petite apparté, d'un point de vue général pour $(c_n) \in \bC^{\bN}$ et $a \in \bC$ on a que la série $\sum c_n (z-a)^n$ est la série 
  entière centrée en a de $(c_n)$. On a alors convergence si $\abs{z - a} < R$ et divergence si $\abs{z - a} > R$ ce qui dans le plan complexe représente 
  un disque de centre $a$ et de rayon $R$ (disque sans le bord), d'où l'appellation \textit{rayon de convergence}
\end{rmq}
\section{Calcul du rayon de convergence}
On peut utiliser la règle d'Alembert (ou de la même manière la règle de cauchy) pour calculer l'inverse du rayon de convergence
\begin{prop}

  Soit $(a_n) \in \rN$ telle que $\ln \abs{\frac{a_{n+1}}{a_n}} = l \in \bar{\bR}$, $l >= 0$, on a 
  \begin{itemize}
    \item si $l\abs{x} < 1$ alors $\sum \abs{a_nx^n}$ CV 
    \item si $l\abs{x} > 1$ alors $\sum \abs{a_nx^n}$ DV
  \end{itemize}
  \begin{proof}
    
  \begin{equation}
    \frac{\abs{a_{n+1}x^{n+1}}}{\abs{a_nx^n}} = \abs{\frac{a_{n+1}}{a_n}}\abs{x}
  \end{equation}
  D'après les hypothèses on a que $\ln \abs{\frac{a_{n+1}}{a_n}} = l \in \bar{\bR}$ et $l >= 0$
  on a alors d'après la règle d'Alembert on obtient le résultat donné en énoncé.
\end{proof}
\end{prop}
\begin{rmq}
  De la même manière on peut calculer $l$ avec la règle de Cauchy.
\end{rmq}
\begin{prop}
  Soit $l \in \bar{\bR}$
  \begin{itemize}
    \item $R = \frac{1}{l}$ si $l \not= 0$
    \item $R = \infty$ si $l=0$ 
    \item $R = 0$ si $l = \infty$
  \end{itemize}
  \begin{proof}
    Conséquence de la proposition précédente
  \end{proof}
\end{prop}

\begin{ex}
  Soit la série entière $\sum \frac{x^n}{n2^n}$ on a alors $a_n = \frac{1}{n2^n}$
  \begin{align*}
    \frac{a_{n+1}}{a_n} &= \frac{\frac{1}{(n+1)2^{n+1}}}{\frac{1}{n2^n}} \\ 
    &= \frac{n2^n}{(n+1)2^{n+1}} \\
    &= \frac{1}{2} \cdot \frac{n}{n+1}
    \to \frac{1}{2}
  \end{align*}
  Donc $l = \frac{1}{2}$ ce qui donne $R = 2$ donc la série entière est convergente sur au moins $]-2,2[$\newline

  Observons maintenant en $x=2$ on a alors 
  \begin{align*}
    \left(\sum \frac{x^n}{n2^n}\right)(2) &= \sum \frac{2^n}{n2^n} \\ 
    &= \sum \frac{1}{n}
  \end{align*}
  ce qui est divergent par le critère de Riemann, donc la série entière est divergente en 2.\newline

  Enfin, en $x=-2$
  \begin{align*}
    \left(\sum \frac{x^n}{n2^n}\right)(-2) &= \sum \frac{(-2)^n}{n2^n} \\ 
    &= \sum \frac{(-1)^n}{n}
  \end{align*}
  Ce qui est convergent par le critère spécial des séries alternées, donc la série entière est convergente en -2.\newline


  On a donc enfin que l'intervalle de convergence de $\sum \frac{x^n}{n2^n}$ est $[-2,2[$
\end{ex}
\section{Somme des Séries Entières}
\begin{definition}
  Soit $a = (a_n) \in \rN$, alors on note la \textit{fonction somme de la série entière} $\sum a_n x^n$ la fonction suivante
  \vfunc{S_a(x)}{]-R,R[}{\bR}{x}{\sum_{n=0}^{\infty} a_n x^n}
\end{definition}
\begin{prop}
  La fonction $S_a$ est $\mC^0$ sur $]-R,R[$.
\end{prop}
\begin{rmq}
  Si la SE est définie en $x=R$ (resp $x=-R$) alors on peut prolonger par continuité $S_a$ en $x=R$ (resp $x=-R$)
\end{rmq}

\subsection{Dérivation et Intégration des Séries Entières}
\begin{definition}
  Soit $(a_n) \in \rN$, on dispose de la série entière $SE = \sum a_n x^n$ de rayon de convergence $R$ alors 
  \begin{itemize}
    \item La série entière $\sum n a_n x^{n-1}$ est nommée \textit{série dérivée} de SE, de rayon de convergence $R_d$
    \item La série entière $\sum a_n \frac{x^{n+1}}{n+1}$ est nommée \textit{série intégrée} de SE, de rayon de convergence $R_i$
  \end{itemize}
\end{definition}
\begin{theorem}
  \begin{equation*}
    R = R_d = R_i
  \end{equation*}


  \begin{proof}
    Soit $(a_n) \in \rN$, nous allons montrer que $R = R_d$
    \begin{itemize}
      \item Soit $x$ tel que $\abs{x} < R_d$ donc $\sum n a_n x^{n-1}$ est ACV, $x \in \bR$ donc une multiplication par $x$ ne change pas 
      la nature de la suite donc $\sum n a_n x^n$ est ACV. Or
      \begin{equation*}
        \forall n \geq 1, \abs{a_n x^n} \leq \abs{n a_n x^n}
      \end{equation*}
      Donc par comparaison de SATP, la série $\sum a_n x^n$ est ACV donc $\abs{x} \leq R$ par propriété du rayon de convergence donc $R \leq R_d$
      \item Soit $x$ tel que $\abs{x} < R$ soit $\rho \in ]\abs{x},R[ \not= \emptyset$ on a 
      \begin{equation*}
        \abs{n a_n x^{n-1}} = \abs{a_n \rho^n} \cdot \abs{\frac{nx^{n-1}}{\rho^{n-1}} \cdot \frac{1}{\rho}}
      \end{equation*}
      Or 
      \begin{equation*}
        \ln \abs{\frac{nx^{n-1}}{\rho^{n-1}} \cdot \frac{1}{\rho}} = 0
      \end{equation*}
      car $\abs{\frac{x}{\rho}} < 1$
      donc
      \begin{equation*}
        \exists N_0 \in \bN, \forall n \geq N_0, \abs{n a_n x^{n-1}} \leq \abs{a_n \rho^n}
      \end{equation*}
      Or $\rho < R$ donc $\sum a_n \rho^n$ est ACV donc par comparaison de SATP $\sum n a_n x^{n-1}$ est ACV donc $R_d \leq R$
    \end{itemize}
    On a $R_d \leq R$ et $R \leq R_d$ donc $R = R_d$ par anti-symétrie de $(\leq)$.\newline 

    La série $SE$ est la série intégrée, puis dérivée donc naturellement on a $R = R_i$ par le point précédent, donc finalement
    \begin{equation*}
      R = R_i = R_d
    \end{equation*}
  \end{proof}
\end{theorem}
\begin{ex}
  On a que $\sum \frac{x^n}{n2^n}$ est $R=2$ donc $\sum \frac{x^{n-1}}{2^n}$ a $R=2$ par dérivation etc.
\end{ex}
\begin{rmq}
  Attention, les propriétés aux bord $R$ et $-R$ ne sont pas conservées.
\end{rmq}
\section{Développement en Séries Entières}
\begin{definition}
  Soit $\func{f}{D \subset \bR}{\bR}$, on dit que $f$ est \textit{développable en série entière}, si
  \begin{equation*}
    \exists (a_n) \in \rN, \lim_{n \to \infty} \sum_{k=0}^n a_k x^k = f
  \end{equation*}
\end{definition}
\subsection{Conditions nécessaires}
\begin{theorem}
  Soit $f$ DSE, alors $\forall n \in \bN f^{n}$ est définie en $0$
  \begin{proof}
  \begin{equation*}
    \exists (a_n) \in \rN, \exists R \in \bR, \forall x \text{ tq } \abs{x} < R, f(x) = \sum_{n=0}^{\infty} a_n x^n
  \end{equation*}
  D'où
  \begin{equation*}
    f(0) = a_0
  \end{equation*}
  Puis par dérivation
  \begin{equation*}
    f'(0) = a_1
  \end{equation*}
  Enfin par récurrence 
  \begin{equation*}
    \forall n \in \bN, f^{(n)}(0) = n! a_n \textit{ (Taylor) }
  \end{equation*}
\end{proof}
\end{theorem}
\begin{ex}
  Par exemple la fonction $\left(x \mapsto \abs{x}\right)$ n'est pas dérivable en $0$ donc elle n'est pas DSE
\end{ex}
\begin{rmq}
  En remaniant l'équation on obtient
  \begin{equation*}
    \forall n \in \bN, a_n = \frac{f^{(n)}}{n!}
  \end{equation*}
  Ce qui garantie l'unicité d'un développement en série entière 
\end{rmq}
\begin{prop}
  Si $f$ est DSE alors $f \in \mC^{\infty}(]-R,R[)$
  \begin{proof}
    Conséquence du théorème précédent
  \end{proof}
\end{prop}
\begin{rmq}
  La réciproque est fausse.
  \begin{proof}
    Soit \vfunc{f}{\bR}{\bR}{x}{\deq{e^{\frac{-1}{x^2}}}{x \not= 0}{0}}, on suppose qu'elle est DSE.
    Alors on voit que par récurrence 
    \begin{equation*}
      \forall n \geq 1, \lim_{x\to0} f^{(n)}(x) = 0
    \end{equation*}
    Donc toutes les dérivées peuvent être prolongées par continuité en $0$ et on obtient (car $f(0)=0$)
    \begin{equation*}
      \forall n \geq 0, f^{(n)}(0) = 0
    \end{equation*}
    Or d'après le point précédent ça veut dire que 
    \begin{equation*}
      \forall n \in \bN, a_n = 0
    \end{equation*}
    donc que 
    \begin{align*}
      \forall x \in \bR, f(x) &= \sum a_n x^n \\ 
      &= \sum 0 \\
      &= 0
    \end{align*}
    Or $f$ n'est pas la fonction nulle, par exemple $f(1) = e^{-1} \not= 0$ donc c'est absurde, donc $f$ n'est pas DSE.
  \end{proof}
\end{rmq}
\section{Opération sur les DSE}
\begin{prop}
  Soit $f,g$ DSE on a 
  \begin{itemize}
    \item $(f+g)$ DSE 
    \item $\forall \lambda \in \bR, (f+\lambda g)$ DSE 
    \item $(fg)$ DSE 
    \item $\forall k \in \bN, f^k$ DSE
    \item $\forall k \in \bN, \left(x \mapsto f(x^k)\right)$ DSE
  \end{itemize}
\end{prop}
\begin{rmq}
  Attention aux intervalles de convergences quand $R_f \not= R_g$ 
\end{rmq}
\section{DSE usuels}
\begin{prop}
  \begin{align*}
    e^{x} &= \sum_{k=0}^{\infty} \frac{x^k}{k!} && R=+\infty \\
    \sin x &= \sum_{k=0}^{\infty} \frac{(-1)^k x^{2k+1}}{(2k+1)!} && R=+\infty \\
    \cos x &= \sum_{k=0}^{\infty} \frac{(-1)^{k} x^{2k}}{(2k)!} && R=+\infty \\
    \frac{1}{1-x} &= \sum_{k=0}^{\infty} x^k && R=1 \\
    \log (1-x) &= \sum_{k=1}^{\infty} - \frac{x^k}{k} && R=1 \\
    \frac{1}{1+x} &= \sum_{k=0}^{\infty} (-1)^k x^k && R=1 \\ 
    \log (1+x) &= \sum_{k=1}^{\infty} \frac{(-1)^{k+1} x^k}{k} && R=1 \\ 
    \frac{1}{1+x^2} &= \sum_{k=0}^{\infty} (-1)^k x^{2k} && R=1 \\
    \arctan x &= \sum_{k=0}^{\infty} \frac{(-1)^k x^{2k+1}}{(2k+1)!} && R=1 \\ 
    (1+x)^{\alpha} &= DL_{\infty} && R=1
  \end{align*}
\end{prop}
\begin{rmq}
  Pour $\log (1-x)$ et $\arctan(x)$ par le CSSA elles convergent en $x=1$
\end{rmq}
\subsection{Application à la somme d'une série numérique}
\begin{rmq}
  On peut utiliser les DSE pour calculer une série numérique, exemple avec $\sum \frac{n}{2^n}$
  \begin{align*}
    \forall x \in ]-1,1[, &\sum x^n = \frac{1}{1-x} \\ 
    &\sum nx^{n-1} = \frac{1}{(1-x)^2} && \text{dérivation} \\ 
    &\sum nx^{n} = \frac{x}{(1-x)^2} && \text{multiplication par }x \\ 
    &=2 && \text{en } x=\frac{1}{2}
  \end{align*}
\end{rmq}

\chapter{Notations, rappels, et Hors Programme (HP)}
\section{Ensembles}
\begin{rmq}
  Soit $D$ un ensemble
  \begin{itemize}
    \item $\bar{D}$ est l'adhérence de $D$ c'est à dire le plus 
    petit ensemble fermé contenant $D$, par exemple $\bar{R} = \bR \cup \{-\infty,+\infty\}$
    \item Soit $\bK$ un corps, alors $\bK[X]$ est l'ensemble des polynomes 
    à coefficient dans $\bK$ a une indéterminée (en gros, variable)
  \end{itemize}
\end{rmq}
\section{Fonctions}
\subsection{Ensembles de fonctions}
\begin{rmq}
  Soit $E,F$ deux ensembles, et soit $I$ un interval de $\bR$
  \begin{itemize}
    \item $E^{F}$ est l'ensemble des applications (fonctions) de $F$ dans $E$
    \item En particulier $\rN$ est l'ensemble des suites réelles
    \item $\mC^0(I)$ est l'ensemble des fonctions continues sur $I$
    \item Dans le cas général $\mC^n(I)$ est l'ensemble des fonctions dérivable $n$ fois sur $I$ et 
    dont la n-ème dérivée est continue sur $I$
    \item On note $\mC^{\infty}(I)$ l'ensemble $\bigcup_{n=0}^{\infty} \mC^n(I)$. En pratique ces fonctions 
    sont dérivable une infinité de fois (par exemple les polynomes, exponentielle etc.)
    \item $\mM^0(I)$ est l'ensemble des fonctions continues par morceaux sur $I$
  \end{itemize}
\end{rmq}
\subsection{Opérations entre fonctions et fonctions et fonctions et scalaires}
\begin{rmq}
  Soit $f,g$ deux fonctions, Soit $\lambda \in \bR$
  \begin{itemize}
    \item $\lambda f$ est la fonction $\left(x \mapsto \lambda \cdot f(x)\right)$
    \item $f+g$ est la fonction $\left(x \mapsto f(x)+g(x)\right)$
    \item $fg$ est la fonction $\left(x \mapsto f(x)g(x)\right)$
    \item $f \circ g$ est la fonction $\left(x \mapsto f(g(x))\right)$
  \end{itemize}
\end{rmq}
\subsection{Comparaison entre fonctions et fonctions et fonctions et scalaires}
\begin{rmq}
  Soit $f,g$ deux fonctions et $\lambda \in \bR$
  \begin{itemize}
  \item $f \geq \lambda$ (resp $>, \leq, <$) représente $\forall x \in I, f(x) \geq \lambda$ (resp $>,\leq,<$)
  \item $f \geq g$ (resp $>,\leq,<$) représente $\forall x \in I, f(x) \geq g(x)$ (resp $>,\leq,<$)
  \item $f = o_a(g) \Leftrightarrow \lim_a \frac{f}{g} = 0$
  \item $f = \mathcal{O}_a(g) \Leftrightarrow \lim_a \frac{f}{g} \in \bR$
  \item $f \equiv_a g \Leftrightarrow \lim_a \frac{f}{g} = 1$
  \end{itemize}
\end{rmq}
\subsection{Limites, continuité et dérivabilité}
\begin{rmq}
  Soit $f$ une fonction définie sur $I$ et $a \in I$
  \begin{itemize}
    \item Définition de la limite de $f$ au point $a$
    \begin{equation*}
      \left(\lim_{x \to a} f(x) = l\right) \Leftrightarrow \left(\forall \varepsilon > 0, \exists 
      \nu > 0, \abs{x-a} < \nu \Rightarrow \abs{f(x) - l} < \varepsilon\right)
    \end{equation*}
    \item $\lim_a f = \lim_{x \to a} f(x)$ 
    \item $f$ est continue en $a$ si $\lim_a f = f(a)$ 
    \item $f$ est continue sur $I$ si $\forall x \in I, f$ est continue en $x$
    \item $f$ est dérivable en $a$ si le quotient $\frac{f(x) - f(a)}{x - a}$ admet une limite finie quand $x \to a$
    \item $f$ est dérivable sur $I$ si $\forall x \in I, f$ est dérivable en $x$
  \end{itemize}
\end{rmq}
\section{Suites, et séries}
\subsection{Suites de scalaires}
\begin{rmq}
  Soit $(a_n)_{n\in\bN}$ une suite de complexe (ou réels)
  \begin{itemize}
    \item L'ensemble de toutes les suites est noté $\rN$. $E = (\rN,+,\cdot)$ où $(\cdot)$ est la loi de composition externe suivante 
    \vfunc{\cdot}{\bK\times \rN}{\rN}{(\lambda,(a_n))}{(\lambda a_n)}
    est un espace vectoriel
    \item Une suite $(u_n)$ est dite convergente si $u_n \to \lambda \in \bR$ 
    \item Une suite $(u_n)$ est dite absolument convergente si $\abs{u_n} \to \lambda \in \bR$
  \end{itemize}
\end{rmq}
\subsection{Suites et série d'application}
\begin{rmq}
  Soit $I$ un interval de $\bR$, $f$ une fonction de $\bR^{I}$ et $(f_n) \in \left(\bR^{I}\right)^{\bN}$ une suite d'application.
  \begin{itemize}
    \item \textbf{HP} On dit que $(f_n)$ converge simplement sur $I$ vers $f$ si 
    \begin{equation*}
      \forall x \in I, \ln f_n(x) = f(x)
    \end{equation*}
    ce qui est équivalent à 
    \begin{equation*}
      \forall x \in I, \forall \varepsilon > 0, \exists N_x \in \bN, \forall n \geq N_x, \abs{f_n(x) - f(x)} < \varepsilon
    \end{equation*}
    \item \textbf{HP} On dit que $(f_n)$ converge uniformément sur $I$ vers $f$ si 
    \begin{equation*}
      \forall \varepsilon > 0, \exists N \in \bN, \forall n \geq N, \forall x \in I, \abs{f_n(x) - f(x)} < \varepsilon
    \end{equation*}
    La différence avec la définition de la convergence simple est dans l'emplacement du $\forall x \in I$ le $N$ ne dépend plus de $x$
    c'est le même pour tous!
    \item \textbf{HP} On dit que $\sum f_n$ converge absolument en tout point de $I$ si 
    \begin{equation*}
      \forall x \in I, \sum \abs{f_n(x)} \text{ CV}
    \end{equation*} 
    \item \textbf{HP} On dit que $\sum f_n$ converge normalement sur $A$ si 
    \begin{equation*}
      \sum \norm{f_n}^A_{\infty} \text{ CV}
    \end{equation*}
    La notation $\norm{f}_{\infty}$ étant la \textbf{norme infinie} de $f$ définie par 
    \begin{equation*}
      \norm{f}^A_{\infty} = \sup_{x \in A} \abs{f(x)}
    \end{equation*}
  \end{itemize}
\end{rmq}
\subsection{Séries}
\begin{rmq}
  Soit $(u_n) \in \rN$
  \begin{itemize}
    \item $\sum u_n$ est la suite $(S_n)_{n\in\bN}$ avec pour $n \in \bN$, $S_n = \sum_{k=0}^n u_k$ et 
    est \textit{la série de terme générale $u_n$}.
    \item Une SATP est une série dont tous les termes sont positifs
    \item On se ramène à l'étude d'une SATP en étudiant le module d'une série
  \end{itemize}
\end{rmq}
\subsection{Séries Entières}
\begin{rmq}
  Soit $(a_n) \in \rN$ 
  \begin{itemize}
    \item On note $\sum a_n x^n$ la suite de fonctions $\left(\sum_{k=0}^n a_k x^k\right)_{n\in \bN}$, appelée
    \textit{série entière} de terme général $a_n$
    \item $\sum a_n x^n$ est une suite de \textbf{fonctions}, alors que pour $\lambda \in D$ la suite $\left(\left(\sum a_n x^n\right)(\lambda)\right)_{n \in \bN}$
    est une suite de \textbf{réel} des fonctions évalués en $\lambda$ elle est égale à la suite de réelle $\left(\sum a_n \lambda^n\right)_{n \in \bN}$
    \end{itemize}
\end{rmq}
\section{Autre}
\begin{rmq}
  Soit $f$ une fonction définie sur $I$ un interval de $\bR$ tel que $I = [a,b]$
  \begin{itemize}
    \item $\int_I f = \int_a^b f = \int_a^b f(x) \text{d}x$
  \end{itemize}
\end{rmq}
\end{document}