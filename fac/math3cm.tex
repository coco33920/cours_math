%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Define Article %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[11pt,colorlinks]{book}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Using Packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{amsthm}
\usepackage{empheq}
\usepackage{mdframed}
\usepackage{booktabs}
\usepackage{changepage}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{psfrag}
\usepackage[T1]{fontenc}
\usepackage{physics}
\usepackage{pgfplots}
\usepackage{fancyhdr}
\usepackage[french]{babel}
\usepackage{bm}

\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Other Settings

%%%%%%%%%%%%%%%%%%%%%%%%%% Page Setting %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\geometry{margin=1.8cm,head=14.5pt}
%%%%%%%%%%%%%%%%%%%%%%%%%% Define some useful colors %%%%%%%%%%%%%%%%%%%%%%%%%%
\definecolor{ocre}{RGB}{243,102,25}
\definecolor{mygray}{RGB}{243,243,244}
\definecolor{deepGreen}{RGB}{26,111,0}
\definecolor{shallowGreen}{RGB}{235,255,255}
\definecolor{deepBlue}{RGB}{61,124,222}
\definecolor{shallowBlue}{RGB}{235,249,255}
\definecolor{deepRed}{RGB}{133,1,1}
\definecolor{shallowRed}{RGB}{255,127,127}
\definecolor{lilac}{HTML}{c8a2c8}
\definecolor{deepPurple}{HTML}{643a64}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%% Define an orangebox command %%%%%%%%%%%%%%%%%%%%%%%%
\newcommand\orangebox[1]{\fcolorbox{ocre}{mygray}{\hspace{1em}#1\hspace{1em}}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%% English Environments %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheoremstyle{mytheoremstyle}
  {3pt}{3pt}
  {\normalfont}{0cm}
  {\rmfamily\bfseries}{}
  {\newline }{{\color{black}\thmname{#1}~\thmnumber{#2}}\thmnote{\,--\,#3}}
\newtheoremstyle{myproblemstyle}{3pt}{3pt}{\normalfont}{0cm}{\rmfamily\bfseries}{}{1em}{{\color{black}\thmname{#1}~\thmnumber{#2}}\thmnote{\,--\,#3}}
\theoremstyle{mytheoremstyle}
\newmdtheoremenv[everyline=true,linewidth=1pt,backgroundcolor=shallowGreen,linecolor=deepGreen,innertopmargin=1pt,leftmargin=1pt,innerleftmargin=20pt,innerrightmargin=20pt]{theorem}{Théorème}[section]
\theoremstyle{mytheoremstyle}
\newmdtheoremenv[everyline=true,linewidth=1pt,backgroundcolor=shallowRed,linecolor=deepRed,innertopmargin=1pt,leftmargin=1pt,innerleftmargin=20pt,innerrightmargin=20pt,]{prop}{Proposition}[section]
\theoremstyle{mytheoremstyle}
\newmdtheoremenv[everyline=true,linewidth=1pt,backgroundcolor=lilac,linecolor=deepPurple,innertopmargin=1pt,leftmargin=1pt,innerleftmargin=20pt,innerrightmargin=20pt,]{definition}{Définition}[section]
\theoremstyle{mytheoremstyle}
\newmdtheoremenv[everyline=true,linewidth=1pt,backgroundcolor=shallowBlue,linecolor=deepBlue,innertopmargin=1pt,leftmargin=1pt,innerleftmargin=20pt,innerrightmargin=20pt,]{ef}{EF}[section]
\theoremstyle{mytheoremstyle}
\newmdtheoremenv[everyline=true,linewidth=1pt,backgroundcolor=shallowBlue, linecolor=deepBlue,innertopmargin=1pt,leftmargin=1pt, innerleftmargin=10pt, innerrightmargin=10pt,]{st}{S}[section]
\theoremstyle{mytheoremstyle}
\newmdtheoremenv[everyline=true,linewidth=1pt,backgroundcolor=mygray,linecolor=black,leftmargin=1pt,innertopmargin=1pt,innerleftmargin=20pt,innerrightmargin=20pt,]{ex}{}[section]

\theoremstyle{mytheoremstyle}
\newmdtheoremenv[everyline=true,linewidth=1pt,backgroundcolor=shallowBlue,linecolor=deepBlue,innertopmargin=1pt,leftmargin=1pt,innerleftmargin=20pt,innerrightmargin=20pt,]{rmq}{Remarque}[section]

\theoremstyle{mytheoremstyle}
\newmdtheoremenv[everyline=true,linewidth=1pt,backgroundcolor=mygray,linecolor=black,innertopmargin=1pt,leftmargin=1pt,innerleftmargin=20pt,innerrightmargin=20pt,]{exe}{Exercice}[section]


\theoremstyle{myproblemstyle}
\newmdtheoremenv[everyline=true,linecolor=black,leftmargin=1pt,innerleftmargin=10pt,innerrightmargin=10pt,]{problem}{Problem}[section]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Plotting Settings %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepgfplotslibrary{colorbrewer}
\pgfplotsset{width=8cm,compat=1.9}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%
% Required to support mathematical unicode
\usepackage[warnunknown, fasterrors, mathletters]{ucs}
\usepackage[utf8x]{inputenc}
\allowdisplaybreaks[1]
% Always typeset math in display style
\everymath{\displaystyle}

% Use a larger font size
\usepackage[fontsize=14pt]{scrextend}

% Standard mathematical typesetting packages
\usepackage{amsfonts, amsthm, amsmath, amssymb}
\usepackage{mathtools}  % Extension to amsmath

% Symbol and utility packages
\usepackage{cancel, textcomp}
\usepackage[mathscr]{euscript}
\usepackage[nointegrals]{wasysym}

% Extras
\usepackage{physics}  % Lots of useful shortcuts and macros
\usepackage{tikz-cd}  % For drawing commutative diagrams easily
\usepackage{color}  % Add some colour to life
\usepackage{microtype}  % Minature font tweaks

% Common shortcuts
\def\mbb#1{\mathbb{#1}}
\def\mfk#1{\mathfrak{#1}}
\def\mfc#1{\mathcal{#1}}


\def\bN{\mbb{N}}
\def\bC{\mbb{C}}
\def\bR{\mbb{R}}
\def\bQ{\mbb{Q}}
\def\bZ{\mbb{Z}}
\def\mR{\mfc{R}}
\def\mC{\mathcal{C}}
\def\mM{\mathcal{M}}
\def\L{\mfc{L}^1(I,\bK)}
\def\Li#1{\mfc{L}^{#1}(I,\bK)}
\def\LI#1{\mfc{L}^1(#1,\bK)}
\def\ib#1{\int_{a}^{b} #1}
\def\ig#1{\int_{a}^{\infty} #1}
\def\bK{\mbb{K}}
\def\af{[a,\infty[}
\def\ab{[a,b[}
\def\abc{]a,b]}
\def\abd{]a,b[}
\def\x{$x \in \bR$}
\def\z{$z \in \bC$}
\def\n{$n \in \bN$}
\def\is#1{\sum_{n=0}^\infty #1}
\def\iss#1#2{\sum_{n=#1}^\infty #2}
\def\se{\sum a_n z^n}
\def\ser{\sum a_n t^n}
\def\seq#1{\sum a_n z_{#1}^n}
\def\seb#1{\sum #1_n z^n}
\def\fn{\forall n \in \bN,}
\def\born{l^{\infty}\left( \bC \right)}
\def\fef{\textbf{FEF}}
\def\ln{\lim_{n \to \infty}}
\def\bO{\mfc{O}}
\def\rN{\bR^{\bN}}
\def\rNp{\left(\bR^{+}\right)^{\bN}}
\def\sn{\sum_{n=0}^{\infty} u_n}
\def\satp{série à terme positifs}
\def\sev{sous espace vectoriel }
\def\ev{espace vectoriel }
\def\mor{\mfc{L}(E,F)}
\def\endo{\mfc{L}(E)}
\def\apcr{à partir d'un certain rang}
% Sometimes helpful macros
\newcommand{\func}[3]{#1\colon#2\to#3}
\newcommand{\cvs}[2]{converge simplement sur $#1$ vers $#2$}
\newcommand{\cvu}[2]{converge uniformément sur $#1$ vers $#2$}
\newcommand{\ppl}[1]{par passage à la limite lorsque #1}
\newcommand{\ppln}[1]{par passage à la limite lorsque $n \to \infty$}
\newcommand{\de}[4]{\begin{cases}
    #1 & \text{si } #2 \\
    #3 & \text{si } #4
\end{cases}}
\newcommand{\deq}[3]{\begin{cases}
    #1 & \text{si } #2 \\
    #3 & \text{sinon}
\end{cases}}
\newcommand{\vfunc}[5]{
  \begin{align*}
    #1 \colon #2 &\to #3\\
    #4 &\mapsto #5.
  \end{align*}
}
\newcommand{\parenth}[1]{\left(#1\right)}
\newcommand\restr[2]{{% we make the whole thing an ordinary symbol
  \left.\kern-\nulldelimiterspace % automatically resize the bar with \right
  #1 % the function
  \vphantom{\big|} % pretend it's a little taller at normal size
  \right|_{#2} % this is the delimiter
  }}
%%
\renewcommand{\equiv}{\sim}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Title & Author %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Math3 CM}
\author{Cours de L. PASQUEREAU \\ Note de C. THOMAS}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
    \maketitle
    \tableofcontents
    \pagestyle{fancy}
    \fancyfoot{}
    \fancyfoot[C]{Approuvé pour usage interne à l'Université de Rennes, page \thepage}
    \chapter{Fonctions de $\bR$ dans $\bR$}

    Soit $D \in \bR$, soit $f \in \bR^{D}$ 
    \section{Limite}
    \subsection{Adhérence}
    \begin{definition}
      On appelle adhérence de $D$ le plus petit ensemble fermé qui contient D. Noté $\bar{D}$ 
    \end{definition}
    \subsection{Limite}
    Soit $f$ définie sur D, Soit $a \in \bar{D}$, Soit $l \in \bR$ 
    \begin{definition}
      On dit que $f$ a pour limite $l$ quand $x$ tends vers $a$ si
      \begin{align*}
        \forall \varepsilon > 0, \exists \eta > 0 | |x-a| < \eta \Rightarrow |f(x) - l| < \varepsilon
      \end{align*}
    \end{definition}
    \subsection{Fonctions négligeables}
    \begin{definition}
      Soit $f,g \in \bR^{D}$ et $a \in \bar{D}$ on dit que $f = o_{a}(g)$ si $\dfrac{f(x)}{g(x)} \to_{a} 0$
    \end{definition}
    \begin{ex}
      en 0 on a 
      \begin{align}
        \dfrac{f(x)}{g(x)} &= \dfrac{x}{\sqrt{x}} \\ 
                           &\to_{0^{+}} 0 \\ 
        f = o_{O^{+}}(g)
      \end{align}
    \end{ex}

    \subsection{Croissance comparée}
    \begin{theorem}[Croissances Comparées]
      Soient $(\alpha,\beta,\gamma) \in R^{+*}$ avec $\gamma > 1$ avec 
      \begin{align*}
        f : x &\mapsto (\log x)^{\alpha} \\ 
        g : x &\mapsto x^{\beta} \\ 
        h : x &\mapsto \gamma^{x}
      \end{align*} 
      alors on a 
      \begin{align*}
        g = o_{\infty}(f) \\ 
        h = o_{\infty}(g)
      \end{align*}
      c'est à dire 
      \begin{align*}
        \dfrac{(\log x)^\alpha}{x^\beta} &\to_{\infty} 0 \\ 
        \dfrac{x^{\beta}}{\gamma^x} &\to_{\infty} 0
      \end{align*}
    \end{theorem}

    \subsection{Fonctions Équivalentes}
    \begin{definition}
      Soit $f,g \in \bR^{D}$ et $a \in \bar{D}$ on dit que $f$ est équivalente à $g$ 
      quand $x$ tends vers $a$ si $\dfrac{f}{g} \to_{a} 1$.  

      On note $f \equiv_a g$ 
    \end{definition}
    \begin{ex}
      \begin{itemize}
        \item Un polynome est équivalent à son monôme de plus haut degrès (resp bas) quand $x$ tends vers $\infty$ (resp $0$)
        \item $sin x \equiv_{0} x$
        \item $ln(1+x) \equiv_{0} x$
      \end{itemize}
    \end{ex}
    \subsection{Opération sur les équivalents}
    Soient $f_1,g_1,f_2,g_2 \in \bR^D$ soit $a \in \bar{D}$ soit $\alpha \in \bR$ 
    \begin{align*}
      f_1 &\equiv_{a} g_1 \\ 
      f_2 &\equiv_{a} g_2 \\
    \end{align*}
    \begin{itemize}
      \item \begin{align*}
      f_1 \cdot f_2 &\equiv_{a} g_1 \cdot g_2 \\ 
      \dfrac{f_1}{f_2} &\equiv_{a} \dfrac{g_1}{g_2} \\
      f_1^\alpha &\equiv_{a} g_1^\alpha
    \end{align*}
    \item \begin{equation}
      f = o_{a} g \Rightarrow f + g \equiv_{a} g 
    \end{equation}
    \item Si $f \equiv_{a} g$ et $\lim_{x \to a} f(x) = l$ alors $\lim_{x \to a} g(x) = l$
    \item \begin{prop}
      Si $f \equiv_a g$ et $\lim_a f \not= 1$ alors $\log f \equiv_a \log g$
      \begin{proof}
        \begin{align*}
          \dfrac{\log g(x)}{\log f(x)} - 1 &= \dfrac{\log g(x) - \log f(x)}{\log f(x)} \\
          &= \dfrac{\log \left(\dfrac{g(x)}{f(x)}\right)}{\log f(x)} && \text{or } f \equiv_a g \\
          &\to_{a} \dfrac{0}{\log f(a)} && \text{par passage à la limite car } \lim_a f \not= 1 \\
          &= 0
        \end{align*}
        Donc $\lim_{x \to a} \dfrac{\log f(x)}{\log g(x)} = 1$ donc $\log f \equiv_a \log g$
      \end{proof}
    \end{prop}
    Cas particulier où $l=1$ 
    \begin{ex}
      $f(x) = 1+x$ et $g(x) = 1 + \sqrt{x}$ on a bien $f \equiv_0 g$ et $f \to_0 1$ 
      on a aussi $\log f(x) = \log 1+x \equiv_0 x$ et $\log g(x) = \log 1+\sqrt{x} \equiv_0 \sqrt{x}$ et $x \not= \sqrt{x}$
    \end{ex}
  \end{itemize}
  \section{Continuité}
  \begin{definition}
    Soit $f$ définie sur un ouvert $D$ de $\bR$ et $a \in D$.  
    On dit que $f$ est continue en $a$ si et seulement si $\lim_{x \to a} f(x) = f(a)$.  
    On note $\mathcal{C}^0$ l'ensemble des fonctions continues, c'est un espace vectoriel.
  \end{definition}
  \section{Dérivabilité}
  \begin{definition}
    Soit $f$ définie sur un ouvert $D$ de $\bR$ et $a \in D$.  
    On dit que $f$ est dérivable en $a$ si et seulement si $\lim_{x \to a} \dfrac{f(x) - f(a)}{x - a}$ existe dans $\bR$.  
    On note $f'$ la fonction $a \mapsto \lim_{x \to a} \dfrac{f(x) - f(a)}{x - a}$ définie sur l'ensemble des valeurs dérivables de $f$.
  \end{definition}
  \subsection{Dérivée successives}
  On peut ensuite étudier la dérivabilité des dérivées successives de f
  \section{Développements Limités (DL)}
  \begin{definition}
    On appelle Développement Limité (DL) à l'ordre $n$ et au point $a \in I$ d'une fonction $f$ défini sur un interval ouvert $I$ de $\bR$, un polynome $P$ tel que
    \begin{align*}
      \deg P &= n \\ 
      f(x) &= P(x-a) + o_0((x-a)^n)
    \end{align*}
    C'est une propriété \textbf{locale} de $f$ en $a$ 
  \end{definition}
  \subsection{Taylor-Young}
  \begin{theorem}[Formule de Taylor-Young]
    Soit $f$ une fonction définie de $I$ dans $\bR$, $n$ fois dérivable, alors $f$ admet un $DL_n$ pour un point $a$ de la forme 
    \begin{equation*}
      f(x) = \sum_{k=0}^n \dfrac{f^{(k)}(a)}{k!} (x-a)^k + o((x-a)^n)
    \end{equation*} 
  \end{theorem}
  \begin{rmq}
    Dans la majorité des cas pratiques, on prend $a=0$ ce qui donne
    \begin{equation*}
      f(x) = \sum_{k=0}^n \dfrac{f^{(k)}(0)x^k}{k!} + o(x^n)
    \end{equation*}
  \end{rmq}
  \begin{ex}
    En exemple on prend $f = \exp$, $\exp \in \mathcal{C}^{\infty}$ et on a $\forall n \in \bN, f^{(n)} = \exp$ donc $\forall n \in \bN, f^{(n)}(0) = 1$
    donc d'après le théorème de Taylor-Young, $\forall n \in \bN, \exp$ admet un $DL_n$ de la forme 
    \begin{align*}
      \exp(x) &= \sum_{k=0}^n \dfrac{\exp^{(k)}(0)}{k!} x^k + o(x^n) \\ 
      \exp(x) &= \sum_{k=0}^n \dfrac{x^k}{k!} + o(x^n)
    \end{align*}
  \end{ex}
  \begin{rmq}
    La formule de Taylor-Young permet aussi de faire l'inverse, de trouver la valeur d'une dérivée en un point si l'on connaît le DL de la fonction. 
    \begin{ex}
      Un exemple pour la valeur en $0$ de la dérivée quatrième de $\dfrac{1}{1-x}$ 
      \begin{equation*}
        \dfrac{1}{1-x} = 1 + x + x^2 + x^3 + x^4 + o(x^4)
      \end{equation*}
      Et d'après Taylor-Young on a 
      \begin{equation*}
        \dfrac{1}{1-x} = \dfrac{f(0)}{1} + \dfrac{f'(0)}{1}x + \dfrac{f''(0)}{2}x^2 + \dfrac{f^{(3)}(0)}{3!}x^3 + \dfrac{f^{(4)}(0)}{4!}x^4 + o(x^4)
      \end{equation*}
      Or les deux DL sont égaux, donc les polynômes aussi, et donc par identification des coefficients on a
      \begin{equation*}
        \dfrac{f^{(4)}(0)}{4!} = 1
      \end{equation*}
      ce qui donne 
      \begin{align*}
        \dfrac{f^{(4)}(0)}{4!} &= 1 \\ 
        f^{(4)}(0) = 4! = 24
      \end{align*}
      On a donc la valeur de la dérivée quatrième en $O$ sans avoir à dériver la fonction.
    \end{ex}
    En pratique ça permet l'étude des dérivées en un point sur des fonctions bien plus complexes.
  \end{rmq}
  \subsection{DL usuels}
  \begin{prop}
    Les développements limités usuels en 0 sont les suivants
    \begin{align*}
      e^x &= \sum_{k=0}^{n} \dfrac{x^k}{k!} + o(x^n) \\
      \sin x &= \sum_{k=0}^{n} \dfrac{(-1)^{k} x^{2k+1}}{(2k+1)!} + o(x^{2n+1}) \\ 
      \cos x &= \sum_{k=0}^{n} \dfrac{(-1)^{k} x^{2k}}{(2k)!} + o(x^{2n}) \\
      \dfrac{1}{1-x} &= \sum_{k=0}^{n} x^k + o(x^n) \\ 
      \dfrac{1}{1+x} &= \sum_{k=0}^{n} (-1)^k x^k + o(x^n) \\ 
      \log (1+x) &= \sum_{k=0}^n \dfrac{(-1)^k x^k}{k} + o(x^n) \\ 
      (1+x)^{\alpha} &= \sum_{k=0}^{n} \sigma_{\alpha}(k) x^k + o(x^n) && \text{avec} \\
      \alpha &\in \bR && \text{et} \\
      \sigma_{\alpha}(k) &= 
      \begin{dcases}
        1 ,& \text{si } k=0 \\ 
        \dfrac{\prod_{i=0}^{k-1} (\alpha - i)}{k!} ,& \text{sinon}
      \end{dcases}
    \end{align*}
  \end{prop}
  \begin{rmq}
    Les DL de fonctions paires (resp impaires) ne contiennent que des coefficients sur les degrès pairs (resp impairs)
    \begin{ex}
      Exemple, la fonction $\cos$ est paire
    \end{ex}
  \end{rmq}
  \subsection{Opération sur les DL}
  Sans perte de généralité, les DL sont ici en $0$  \newline
  Soit $P,Q \in R[X]$ et $f,g \in \bR^{I}$ tels que 
  \begin{align*}
    \deg P &= \deg Q = n \\ 
    f(x) &= P(x) + o(x^n) \\ 
    g(x) &= Q(x) + o(x^n)
  \end{align*}
  \subsubsection{Troncage}
  \begin{definition}
    On appelle "troncage" à l'ordre $k \leq n$ d'un DL, le polynome tronqué $F_k$ de degrès $k$ tel que 
    tous les coefficients de $F_k$ sont égaux à ceux de $F$ jusqu'au coefficient de $x^k$ et tel que 
    \begin{equation*}
      f(x) = F_k(x) + o(x^k)
    \end{equation*}
  \end{definition}
  \begin{ex}
    On a 
    \begin{equation*}
      e^x = 1 + x + \dfrac{x^2}{2} + \dfrac{x^3}{3!} + \dfrac{x^4}{4!} + \dfrac{x^5}{5!} + o(x^5)
    \end{equation*}
    le $DL_5$ de $exp$ alors on peut le "tronquer" à l'ordre $k=3\leq 5$ pour avoir le $DL_3$ de exp 
    \begin{equation*}
      e^x = 1 + x + \dfrac{x^2}{2} + \dfrac{x^3}{3!} + o(x^3)
    \end{equation*}
  \end{ex}
  \subsubsection{Somme}
  \begin{prop}
    Le $DL_n$ de la fonction $f+g$ est la somme des $DL_n$ de $f$ et de $g$ 
    \begin{equation*}
      (f+g)(x) = P(x)+Q(x) + o(x^n)
    \end{equation*}
  \end{prop}
  \subsubsection{Produit}
  \begin{prop}
    Le $DL_n$ de la fonction $fg$ est le produit des $DL_n$ de $f$ et de $g$ tronqué à l'ordre $n$ 
    \begin{equation*}
      (fg)(x) = PQ_{n}(x) + o(x^n)
    \end{equation*}
  \end{prop}
  \subsubsection{Composée}
  \begin{prop}
    Si $g(0) = 0$ alors on peut composer les $DL_n$ et le $DL_n$ de $f \circ g$ est la composition des $DL_n$ de $f$ et de $g$ tronqué à l'ordre $n$
    \begin{equation*}
      (f\circ g)(x) = (P \circ Q)_{n}(x) + o(x^n)
    \end{equation*}
  \end{prop}
  \begin{ex}
    Exemple $DL_3$ de $\sqrt{1 + \sin x}$. On a bien $\sin 0 = 0$.
    \begin{align*}
      \sin x &= x - \dfrac{x^3}{6} + o(x^3) \\ 
      (1+X)^{\alpha} &= 1 + \alpha X + \dfrac{\alpha(\alpha-1)x^2}{2} X^2 + \dfrac{\alpha(\alpha-1)(\alpha-2)}{6} X^3 + o(X^3) && \text{donc} \\ 
      (1 + \sin x)^{\frac{1}{2}} &= 1 + \dfrac{1}{2} \left(x - \dfrac{x^3}{6}\right) - \dfrac{1}{8} \left(x - \dfrac{x^3}{6}\right)^2 + \dfrac{3}{48} \left(x - \dfrac{x^3}{6}\right)^3 + o(x^9) \\ 
      (1 + \sin x)^{\frac{1}{2}} &= 1 + \dfrac{1}{2} x - \dfrac{x^3}{12} - \dfrac{1}{8} x^2 + \dfrac{3}{48} x^3 + o(x^3) && \text{tronquage} \\ 
      (1 + \sin x)^{\frac{1}{2}} &= 1 + \dfrac{1}{2} x - \dfrac{1}{8} x^2 - \dfrac{1}{48} x^3 + o(x^3) 
    \end{align*}
  \end{ex}
  \subsection{Application au calcul de dérivé}
  Les DL sont utiles pour résoudre des formes indéterminées lors du calcul de limite 
  \begin{ex}
    Calcul de la limite en 0 de la fonction $f : $ $x \mapsto \dfrac{e^{x^2} - \cos x}{x^2}$  
    On calcule les différents DL à l'ordre 4
    \begin{align*}
      e^{x^2} &= 1 + (x^2) + \dfrac{(x^2)^2}{2} + o(x^4) \\
      \cos x &= 1 - \dfrac{x^2}{2} + \dfrac{x^4}{24} + o(x^4) \\
      e^{x^2} - \cos x &= \dfrac{3}{2}x^2 + o(x^2) && \text{tronquage, inutile au delà} \\ 
      f(x) &= \dfrac{\dfrac{3}{2}x^2 + o(x^2)}{x^2} \\ 
      f(x) &= \dfrac{3}{2} + o(1) && \text{d'où} \\
      \lim_{x \to 0} f(x) &= \dfrac{3}{2}
    \end{align*}
    On voit après que l'ordre 2 aurait suffit, l'intuition peut aider pour savoir à quel ordre calculer.
  \end{ex}

  \chapter{Intégration}
  \section{Intégrales de Riemann}
  Explication des notations,
  \begin{align*}
    \int_a^b f &= \int_a^b f(x) \text{d}x \\ 
    \int_{[a,b]} f &= \int_a^b f
  \end{align*}
  \subsection{Introduction}
  Soit $a,b \in \bR$ tels que $a < b$. Soit $f$ définie et bornée sur $[a,b]$ et $d=(x_1,\cdots,x_n) \subset [a,b]$ une subdivision de $[a,b]$ pour $n \in \bN$.  
  On définie 
  \begin{align*}
    M_i &= \sup_{x\in [x_{i-1},x_i]} f(x) \\ 
    m_i &= \inf_{x\in [x_{i-1},x_i]} f(x) \\ 
    S(d) &= \sum_{i=1}^n M_i \cdot (x_i-x_{i-1}) \\ 
    s(d) &= \sum_{i=1}^n m_i \cdot (x_i-x_{i-1})
  \end{align*}
  Le but est double 
  \begin{itemize}
    \item Approcher $f$ par des fonctions en escalier 
    \item Augmenter $n$ pour augmenter la précision de l'approche
  \end{itemize}
  Et pour $d'$ une subdivision plus fine que $d$ on a 
  \begin{equation*}
    s(d) \leq s(d') \leq S(d') \leq S(d)
  \end{equation*}
  On peut définir des suites convergentes, et à l'infini on note 
  \begin{align*}
    I &= \sup_{[a,b]} S(d) \\ 
    J &= \inf_{[a,b]} s(d)
  \end{align*}
  \begin{definition}
    Une fonction $f$ est Riemann-intégrable si $I_f=J_f=\int_{a}^b f$
  \end{definition}
  \subsection{Propriétés de l'intégrale}
  On prend $f,g$ deux fonctions Riemann-intégrable définie sur $[a,b]$
  \begin{prop}
    On a
    \begin{align*}
      \int_a^a f &= 0 \\ 
      \int_b^a f &= - \int_a^b f
    \end{align*}
  \end{prop}
  \begin{prop}[Relation de Chales]
    Soit $c \in [a,b]$,
    \begin{equation*}
      \int_a^b f = \int_a^c f + \int_c^b f
    \end{equation*}
  \end{prop}
  \begin{ex}
    Exemple d'une fonction non-Riemann-intégrable. Soit $f$ la fonction indicatrice de $\bQ$ sur $[0,1]$ alors on a 
    \begin{align*}
      M_i = \sup f &= 1 \\ 
      m_i = \inf f &= 0
    \end{align*}
    D'où 
    \begin{align*}
      S(d) = x_n - x_0 &= 1 \\
      s(d) = 0
    \end{align*}
    Donc 
    \begin{equation*}
      I \not= J
    \end{equation*}
    Par conséquence, $f$ n'est pas Riemann-intégrable sur $[0,1]$
  \end{ex}
  \begin{theorem}
    Soit $f$ une fonction définie sur $[a,b]$
    \begin{enumerate}
      \item Si $f$ est $\mathcal{C}^0$ alors $f$ est Riemann-intégrable
      \item Théorème des singularités supprimable, si on modifie $f$ sur un nombre fini de point, l'intégrale n'est pas modifiée
      \item Par conséquence, les fonctions continues par morceaux ($\mathcal{M}^0$) sont aussi Riemann-intégrable
    \end{enumerate}
  \end{theorem}
  \subsection{Opération sur les intégrales}
  \begin{prop}
    Soient $f,g$ Riemann-intégrables sur $I$
    \begin{itemize}
      \item Soit $\lambda \in \bR$, alors $\int_I \lambda f = \lambda \int_I f$
      \item La fonction $(f+g)$ est Riemann-intégrable et $\int_I (f+g) = \int_I f + \int_I g$
      \item La fonction $\abs{f}$ est Riemann-intégrable
      \item La fonction $(fg)$ est Riemann-intégrable
    \end{itemize}
  \end{prop}
  \subsection{Positivité de l'intégrale}
  \begin{prop}
    Soit $f$ Riemann-intégrable sur $I$
    \begin{itemize}
      \item Si $\forall x \in I, f(x) \geq 0$ alors $\int_I f \geq 0$
      \item Si $\forall x \in I, f(x) \leq 0$ alors $\int_I f \leq 0$
    \end{itemize}
  \end{prop}
  \begin{theorem}[Positivité de l'intégrale]
    Soit $f,g$ Riemann-intégrable sur $I$ telles que 
    \begin{equation*}
      \forall x \in I, f(x) \leq g(x)
    \end{equation*}
    Alors il vient de la prop précédente que 
    \begin{equation*}
      \int_I f \leq \int_I g
    \end{equation*}
  \end{theorem}
  \begin{prop}[Généralisation de l'inégalitée triangulaire]
    Soit $f$ Riemann-intégrable sur I, on a alors 
    \begin{equation*}
      \abs{\int_I f} \leq \int_I \abs{f}
    \end{equation*}
  \end{prop}
  \subsection{Moyenne}
  Soit $f,g$ Riemann-intégrable sur $I$, on note 
  \begin{align*}
    m &= \inf_I f \\
    M &= \sup_I f
  \end{align*}
  \begin{prop}
    Si $g$ est de signe constant sur $I$ alors $\exists \mu \in [m,M], \int_I fg = \mu \int_I g$
    \begin{proof}
      On a, $\forall x \in I, m \leq f(x) \leq M$, on considère sans perte de généralité que $\forall x \in I, g(x) \geq 0$ et 
      que $\int_I g \not= 0$ 
      alors on a 
      \begin{align*}
        \forall x \in I,& m \leq f(x) \leq M \\ 
        &m g(x) \leq f(x)g(x) \leq Mg(x) \\ 
        &m\int_I g \leq \int_I fg \leq M \int_I g && \text{g est positive} \\ 
        &m \leq \dfrac{\int_I fg}{\int_I g} \leq M && \text{car }\int_I g \not= 0
      \end{align*}
      On pose $\dfrac{\int_I fg}{\int_I g} = \mu$, il vient que $\mu \in [m,M]$ et que $\mu \int_I g = \int_I fg$
    \end{proof}
  \end{prop}
  \begin{rmq}
    On prend le cas particulier où $g=1$ on a $\int_a^b f = \mu \int_a^b 1$ ce qui donne finalement
    \begin{equation*}
      \mu = \dfrac{1}{(b-a)} \int_a^b f
    \end{equation*}
    On appelle alors $\mu$ la valeur moyenne de la fonction $f$ sur $[a,b]$ 
  \end{rmq}
  \subsection{Théorème fondamental de l'analyse}
  
  \begin{prop}
    Soit $f : [a,b] \to \bR$ Riemann-intégrable, et on définie $g : [a,b] \in \bR$ telle que 
    \begin{equation*}
    \forall x \in [a,b], g(x) = \int_a^x f
    \end{equation*}
    Alors
    \begin{itemize}
      \item Si $f$ est Riemann-intégrable alors $g$ est continue
      \item Si $f$ est continue en $x_0 \in [a,b]$ alors $g$ est dérivable en $x_0$
      \item Si $f$ est continue sur $[a,b]$ alors $g$ est dérivable sur $[a,b]$ et $g' = f$
    \end{itemize}
  \end{prop}

  \begin{theorem}[Théorème fondamental de l'analyse]
    Soit $f$ une fonction $\mathcal{C}^0$ sur $I$ un interval de $\bR$, et soit $\alpha \in I$ alors $f$ admet une unique primitive $F_{\alpha}$ 
    telle que $F_{\alpha}' = f$ s'annulant en $x=\alpha$.
    De plus pour toute fonction $F$ primitive de $f$ on a $\int_a^b f = F(b) - F(a)$
  \end{theorem}
  \subsection{Primitives usuelles}
  \begin{prop}
    Les primitives usuelles sont les suivantes, par abus de notation toutes les fonctions suivantes 
    sont marquées selon leur procédure, par example $x^{\alpha}$ réfère à la fonction $(x \mapsto x^{\alpha})$ sur son plus grand interval de 
    définition, $c$ désigne une constante réelle.
    \begin{align*}
      f = x^{\alpha}, F &= \dfrac{x^{\alpha+1}}{\alpha+1} + c && \alpha \not= -1 \\ 
      f = \dfrac{1}{x}, F &= ln \abs{x} + c \\ 
      f = \dfrac{1}{\sqrt{x}}, F &= 2\sqrt{x} + c \\ 
      f = e^x, F &= e^x + c \\
      f = \cos (ax+b), F &= \dfrac{1}{a} \sin (ax+b) + c && a \not= 0 \\ 
      f = \sin (ax+b), F &= - \dfrac{1}{a} \cos(ax+b) + c && a \not= 0 \\
      f = \dfrac{1}{\cos^2 x}, F &= \tan x \\ 
      f = \dfrac{1}{x^2+a^2}, F &= \dfrac{1}{a} \arctan \dfrac{x}{a} + c && a \not= 0 
    \end{align*}
    Pour les fonctions, il faut pas oublier la règle de la composée qui donne par example
    \begin{align*}
      f = u^{\alpha} \cdot u', F &= \dfrac{u^{\alpha+1}}{\alpha+1} && \alpha \not=-1 \\ 
      f = \dfrac{u'}{u}, F &= ln \abs{u} \\
      f = \dfrac{u'}{\sqrt{u}}, F &= 2\sqrt{u}
    \end{align*}
  \end{prop}
  \subsection{Changement de variable}
  \begin{theorem}[Théorème de changement de variable]
    Soit $\varphi [a,b] \in \bR, \mathcal{C}^1$ sur $[a,b]$, et soit $f : I \in \bR \mathcal{C^0}$ sur $I$ alors on a la formule suivante
    \begin{equation*}
      \int_a^b f \circ \varphi \cdot \varphi' = \int_{\varphi(a)}^{\varphi(b)} f
    \end{equation*}
  \end{theorem}
  \begin{ex}
    Calculons, $I = \int_0^1 \dfrac{x \text{d}x}{\sqrt{d - x^2}}$
    On pose $t = 2 - x^2$ ce qui est bien $\mathcal{C}^1$ alors on a $\text{d}t = -2x \text{d}x$ 
    donc par changement de variable,
    \begin{align*}
      I &= \int_2^1 \dfrac{\text{d}t}{-2\sqrt{t}} \\ 
        &= \int_1^2 \dfrac{\text{d}t}{2\sqrt{t}} \\ 
        &= \left[2\sqrt{x}\right]_1^2 \\ 
        &= \sqrt{2} - 1
    \end{align*}
  \end{ex}
  \subsection{Intégration par parties}
  \begin{theorem}[Théorème d'intégration par parties]
    Soit $u,v$, $\mathcal{C}^1$ sur $[a,b]$ alors on a 
    \begin{equation*}
      \int_a^b uv' = [uv]_a^b - \int_a^b u'v
    \end{equation*}
  \end{theorem}
  \begin{ex}
    Exemple calculons $I = \int_0^1 xe^x \text{d}x$
    On pose $u(x) = x$ donc $u'(x) = 1$ et donc $v'(x) = e^x$ ce qui donne $v(x) = e^x$ ce qui sont bien $C^1$,
    donc par IPP on a 
    \begin{align*}
      I &= [xe^x]_0^1 - \int_0^1 e^x \text{d}x \\ 
      &= e - (e - 1) \\ 
      &= 1
    \end{align*}
  \end{ex}
  \section{Intégrales Généralisées}
  Il existe deux cas d'intégrales généralisées
  \begin{enumerate}
    \item Le cas où l'on intègre une fonction bornée sur un intervalle non borné (de forme $[a,b[$)
    \item Le cas où l'on intègre une fonction non bornée sur un intervalle bornée (de forme $[a,b]$)
  \end{enumerate}
  \begin{definition}
    Soit $[a,b[$ tel que $-\infty < a < b \leq +\infty$.
    Soit $f : [a,b[ \to \bR$. On prend l'application $I(\lambda) = \int_a^{\lambda} f$ définie sur $[a,b[$
    \begin{itemize}
      \item Si $I(\lambda)$ converge en $b^{-}$ alors $f$ est intégrable sur $[a,b[$, 
      on note $\lim_{\lambda \to b^{-}} I(\lambda) = \int_a^b f$ et on appelle 
      le scalaire $\int_a^b f$ \textbf{intégrale généralisée} de $f$ sur $[a,b[$
      \item Si $I(\lambda)$ diverge en $b^{-}$ alors $f$ n'est pas intégrable sur $[a,b[$
    \end{itemize}
  \end{definition}
  \begin{ex}
    On cherche à connaître la nature de l'intégrale de $\left(x \mapsto \dfrac{1}{x^2}\right)$ sur $[1,+\infty[$
    \begin{align*}
      I(\lambda) &= \int_1^{\lambda} \dfrac{\text{d}x}{x^2} \\ 
                &= - \left[\dfrac{1}{x}\right]_1^{\lambda} \\ 
                &= - \dfrac{1}{\lambda} + 1 \to_{\infty} 1
    \end{align*}
    Donc $\int_1^{\infty} \dfrac{\text{d}x}{x^2}$ existe et vaut $1$
  \end{ex}
  \begin{ex}
    On cherche à connaître la nature de l'intégrale de $\left(x \mapsto \cos x\right)$ sur $[0,\infty[$.
    \begin{align*}
      I(\lambda) &= \int_1^{\lambda} \cos x \text{d}x \\ 
                &= \left[\sin x\right]_1^{\lambda} \\ 
                &= - \sin \lambda && \text{DV}
    \end{align*}
    Donc $\left(x \mapsto \cos x\right)$ n'est pas intégrable sur $[0,\infty[$
  \end{ex}
  \begin{rmq}
    Soit $c \in [a,b[$ alors $\int_a^b f$ et $\int_c^b f$ sont de même nature, et sont notés en général $\int^b f$
  \end{rmq}
  \begin{rmq}
    Si on a $a = \infty$ ou $f$ non définie en $a$ on sépare l'étude en plusieurs sous problèmes
  \end{rmq}
  \subsection{Cas des fonctions réelles positives}
  Dans la section $f$ est une fonction réelle positive définie sur $[a,b[$
  \subsubsection{Majoration}
  \begin{prop}
    l'intégrale de $f$ sur $[a,b[$ CV $\Leftrightarrow \int_a^{\lambda} f$ majorée
    \begin{proof}
      \begin{equation*}
        I(\lambda) = \int_{a}^{\lambda} f
      \end{equation*}
      On a $I$ qui est croissante sur $[a,b[$ d'après le théorème des limites monotones alors 
      \begin{itemize}
        \item si $I$ est majorée alors $I(\lambda) \to \mu \in \bR$ et $f$ est intégrable sur $[a,b[$
        \item si $I$ n'est pas majorée alors $I(\lambda) \to \infty$ donc $f$ n'est pas intégrable sur $[a,b[$
      \end{itemize}
    \end{proof}
  \end{prop}
  \subsubsection{Comparaison}
  \begin{prop}[Théorème de comparaison]
    Soit $g : [a,b[ \to \bR$ tel que $0 \leq f \leq g$ alors 
    \begin{itemize}
      \item Si $g$ est intégrable sur $[a,b[$ alors $f$ l'est 
      \item Si $f$ n'est pas intégrable sur $[a,b[$ alors $g$ ne l'est pas
    \end{itemize}
  \end{prop}
  \subsubsection{Equivalent}
  \begin{prop}
    Soit $g : [a,b[ \to \bR$ tel que $f \equiv_b g$ alors $\int^b f$ et $\int^b g$ sont de même nature
  \end{prop}
  \subsection{Cas des fonction réelles positives et où $b=\infty$}
  \begin{prop}
    Si $f \not\to 0$ alors $f$ n'est pas intégrable sur $[a,\infty[$
    \begin{proof}
      Supposons que $f \to l \not= 0$ alors $f \equiv l$ donc $\int^{\infty} f$ est de même nature que $\int^{\infty} l \text{d}x$ donc 
      $\int^{\infty} f$ DV
    \end{proof}
  \end{prop}
  \subsubsection{Critère de Riemann}
  \begin{theorem}[Critère de Riemann]
    La fonction $(x \mapsto \frac{1}{x^{\alpha}})$ est : 
    \begin{itemize}
      \item intégrable $\Leftrightarrow \alpha > 1$
      \item pas intégrable $\Leftrightarrow \alpha \leq 1$ 
    \end{itemize}
    \begin{proof}
      \begin{align*}
        I(\lambda) &= \int_1^{\lambda} \frac{\text{d}x}{x^{\alpha}} && \alpha \not= 1 \\
                  &= \left[\frac{x^{1 - \alpha}}{1 - \alpha}\right]_1^{\lambda} \\ 
                  &= \frac{1}{(1-\alpha)\lambda^{\alpha-1}} - \frac{1}{1-\alpha}
      \end{align*}
      Donc $I$ ne converge que si $\alpha > 1$ et en retour si $\alpha > 1$ alors $I$ converge.
    \end{proof}
  \end{theorem}
  \begin{ex}
    Cherchons la nature de $\int_0^{\infty} \frac{2x+1}{\sqrt{x^4+8}}$ 
    \begin{align*}
      \frac{2x+1}{\sqrt{x^4+8}} &\equiv \frac{2x}{\sqrt{x^4}} \\ 
                                &\equiv \frac{2}{x}
    \end{align*}
    Donc d'après le critère de Riemann $\int_0^{\infty} \frac{2x+1}{\sqrt{x^4+8}}$ DV
  \end{ex}
  \subsubsection{Règle de Riemann}
  \begin{prop}[Règle de Riemann]
    Soit $f$ une fonction définie sur $[a,\infty[$
    \begin{itemize}
      \item Si il existe $\alpha > 1$ tel que $x^{\alpha}f(x) \to l \in \bR$ alors $\int^{\infty} f$ CV
      \item Si il existe $\alpha \leq 1$ tel que $x^{\alpha}f(x) \to l \in \bar{\bR^{*}}$ alors $f$ n'est pas intégrable     
    \end{itemize}
    \begin{proof}
      Conséquence du critère de Riemann.
    \end{proof}
  \end{prop}
  \begin{ex}
    Est-ce que $(x \mapsto \sqrt{x}e^{-x})$ est intégrable sur $[0,\infty[$
    \begin{align*}
      x^2*\sqrt{x}e^{-x} &= \frac{x^{\frac{5}{2}}}{e^x} \\ 
                        &\to 0 && \text{CC} 
    \end{align*}
    Donc d'après la règle de Riemann, $\int^{\infty} \sqrt{x}e^{-x}$ CV
  \end{ex}
  \subsection{Cas où b est fini}
  On note 
  \begin{equation*}
    g(x) = \frac{1}{(b-x)^{\alpha}}, \alpha > 0
  \end{equation*}
  \begin{prop}
    Nature de $\int_a^b g$. 
    \begin{align*}
      I(\lambda) &= - \frac{1}{(1-\alpha)} \left((b-\lambda)^{1-\alpha} - (b-a)^{1 - \alpha}\right)
    \end{align*}
    Donc
    \begin{itemize}
      \item si $\alpha < 1$ alors $(b - \lambda)^{1-\alpha} \to 0$ donc $I$ CV
      \item si $\alpha > 1$ alors $(b - \lambda)^{1-\alpha} \to \infty$ donc $I$ DV
      \item si $\alpha = 1$ alors $I \to \infty$ donc $I$ DV
    \end{itemize}
  \end{prop}
  \subsubsection{Critère de Riemann}
  \begin{theorem}[Critère de Riemann - Version finie]
    \begin{equation*}
      \int_a^b \frac{\text{d}x}{(b-x)^{\alpha}} \text{ CV } \Leftrightarrow \alpha < 1
    \end{equation*}.
    \begin{proof}
      Voir ci dessus.
    \end{proof}
  \end{theorem}
  \begin{prop}
    Par conséquent si $f \equiv \frac{A}{(b-x)^{\alpha}}$ alors $\int^b f$ CV ssi $\alpha < 1$
    \begin{proof}
      Conséquence directe du critère de Riemann.
    \end{proof}
  \end{prop}
  \begin{ex}
    Nature de $\int_0^1 \frac{x+1}{\sqrt{x}}$ on a 
    \begin{align*}
      \frac{x+1}{\sqrt{x}} &\equiv \frac{2}{\sqrt{x}} \\
                          &\equiv \frac{2}{x^{\frac{1}{2}}}
    \end{align*}
    or $\frac{1}{2} < 1$ donc $\int_0^1 \frac{x+1}{\sqrt{x}}$ CV
  \end{ex}
  \subsubsection{Règle de Riemann}
  \begin{prop}[Règle de Riemann - Version finie]
    Soit $f$ une fonction définie sur $[a,b[$ 
    \begin{itemize}
      \item Si il existe $\alpha < 1$ tel que $x^{\alpha}f(x) \to l \in \bR$ alors $\int^{\infty} f$ CV
      \item Si il existe $\alpha \geq 1$ tel que $x^{\alpha}f(x) \to l \in \bar{\bR^{*}}$ alors $f$ n'est pas intégrable     
    \end{itemize}
    \begin{proof}
      Conséquence du critère de Riemann.
    \end{proof}
  \end{prop}
  \subsection{Cas des fonctions de signes qql}
  \begin{definition}
    On dit que l'intégrale de $f$ est simplement convergente si et seulement si $I(\lambda)$ a une limite et si
    \begin{equation*}
      \lim_{x \to b^{-}} \int_a^x f \in \bR
    \end{equation*}
  \end{definition}
  \begin{definition}
    On dit que l'intégrale de $f$ est absolument convergente si et seulement si $\int_a^x \abs{f} \to \mu \in \bR$ 
  \end{definition}
  \begin{theorem}[Comparaison]
    Soient $a \in \bR$, $b \in \bar{\bR}$ tel que $a < b$ et $f$ une fonction définie sur $[a,b[$
    \begin{enumerate}
      \item Si l'intégrale de $f$ est absolument convergente alors l'intégrale de $f$ est simplement convergente
      \item Le résultat $\abs{\int f} \leq \int \abs{f}$ est étendu aux intégrales généralisées
    \end{enumerate}
    \begin{proof}
      Cette preuve est \textit{\textbf{hors du programme présenté lors du cours}}, je la trouve utile mais on peut la sauter c'était juste pour donner une idée 
      de à quoi peut ressembler ce genre de preuve.\newline 


      \begin{enumerate}
        \item Notons, $f^{-} = \max(-f,0)$ et $f^{+} = \max(f,0)$. On observe que 
      \begin{align*}
        f^{-} + f^{+} &= \max(-f,0) + \max(f,0) \\ 
                      &= \abs{f}
      \end{align*}
      Et on a aussi
      \begin{align*}
        f^{+} - f^{-} &= \max(f,0) - \max(-f,0) \\ 
                     &= f
      \end{align*}
      On suppose que $\int \abs{f}$ est convergente donc 
      \begin{equation*}
        \exists \mu \in \bR, \lim_{x \to b^{-}} \int_a^x (f^{+}+f^{-}) = \mu
      \end{equation*}
      Or Les fonctions $f^{+}$ et $f^{-}$ sont à valeurs positives, donc
      \begin{align*}
        \exists (\mu_1,\mu_2) \in \bR^2, &\lim_{x \to b^{-}} \int_a^x (f^{+}) = \mu_1 && \text{et} \\ 
                                         &\lim_{x \to b^{-}} \int_a^x (f^{-}) = \mu_2
      \end{align*}
      Donc par linéarité, 
      \begin{equation*}
        \lim_{x \to b^{-}} \int_a^x (f^{+}-f^{-}) = \mu_1 - \mu_2 \in \bR
      \end{equation*}
      Ce qui revient à 
      \begin{equation*}
        \exists \lambda \in \bR, \lim_{x \to b^{-}} \int_a^x f = \lambda
      \end{equation*}
      Donc l'intégrale de $f$ est simplement convergente.\newline

        \item Supposons que l'intégrale de $f$ est absolument convergente, rappelons d'abord que sur les intégrales de Riemann on a 
      pour $(\alpha,\beta) \in \bR^2$ tels que $a < b$ et $g$ $\mM^0$ sur $[a,b]$
      \begin{equation}
        \abs{\int_{\alpha}^{\beta} g} \leq \int_{\alpha}^{\beta} \abs{g}
      \end{equation}
      Puis,
      \begin{align*}
        \forall x \in [a,b[, \abs{\int_a^x f} &= \abs{\int_a^x (f^{+} - f^{-})} \\ 
        &\leq \abs{\int_a^x f^{+}} + \abs{\int_a^x f^{-}} && \textit{IT} \\ 
        &\leq \int_a^x \abs{f^{+}} + \int_a^x \abs{f^{-}} && (1)\\ 
        &\leq \int_a^x f^{+} + f^{-} && \text{valeurs positives} \\
        &\leq \int_a^x \abs{f}
      \end{align*}
      Enfin par passage à la limite avec $x \to b^{-}$ (les limites existe avec la démonstration du 1.)
      \begin{equation*}
        \abs{\int_a^b f} \leq \int_a^b \abs{f}
      \end{equation*}
    \end{enumerate}
    \end{proof}
  \end{theorem}
  \begin{ex}
    Nature de $\int_0^1 \sin \frac{1}{x} \text{d}x$. 
    On a 
    \begin{equation*}
      \forall x \in ]0,1] \leq 1 
    \end{equation*}
    Donc l'intégrale est absolument convergente donc l'intégrale est convergente
  \end{ex}
  \begin{ex}
    Nature de $\int_0^{\infty} \frac{\cos x}{1+x^2}$ par le même raisonnement absolument convergente donc convergente.
  \end{ex}



\chapter{Séries numériques}
\section{Introduction aux séries numériques}
\begin{definition}
  Soit $(u_n) \in \bR^{\bN}$ et $S_n = \sum_{i=0}^n u_i$ alors la suite $(S_n)_{n \in \bN}$ est appélée \textit{série de terme général $u_n$} 
  notée $\sum u_n$.
  
  De plus si la suite $(S_n)$ converge alors on note $S = \ln \sum_{k=0}^n u_k$, notée aussi $\sum_{k=0}^{\infty} u_k$ 
  appelée \textit{somme de la série}. 
  
  On dit alors que la série de terme général $u_n$ est convergente. Dans le cas contraire on dit qu'elle est divergente. 
\end{definition}
\begin{definition}
  On appelle $S_n = \sum_{k=0}^n u_k$ somme partielle de la série de terme général $u_n$, et si la série est convergente alors on note 
  $R_n = S - S_n$ le reste de la série.
\end{definition}
\begin{rmq}
  On a donc $\ln R_n = 0$
\end{rmq}
\begin{rmq}
  Les séries $\sum_{0} u_n$ et $\sum_{n_0} u_n$ sont de même nature d'où la notation $\sum u_n$ 
\end{rmq}
\begin{prop}[Condition nécessaire de convergence]
  \begin{equation*}
    \left(\sum u_n \text{ CV}\right) \Rightarrow \ln u_n = 0
  \end{equation*}

  
  \begin{proof}
    On observe que 
    \begin{equation*}
      S_{n+1} - S_n = u_n
    \end{equation*}
    Donc, en supposant que $\sum u_n$ CV on a 
    \begin{align*}
      u_n &\to S - S \\ 
          &= 0
    \end{align*}
    Donc $\ln u_n = 0$
  \end{proof}
\end{prop}
\begin{rmq}
  Cette condition est surtout utilisé pour montrer qu'il n'y a pas convergence (contraposée), lorsque $u_n \not\to 0$ 
  (en pratique plutôt lorsque $\ln u_n = \infty$) on dit que $\sum u_n$ est \textit{grossièrement divergente}.
\end{rmq}
\begin{ex}
  Soit $u_n = \dfrac{1}{n(n+1)}$. Soit $n \geq 1$ on a alors
  \begin{equation*}
    \dfrac{1}{n(n+1)} = \dfrac{1}{n} - \dfrac{1}{n+1}
  \end{equation*}
  Donc par somme téléscopique on obtient que 
  \begin{equation*}
    \sum_{k=1}^{n} \dfrac{1}{n(n+1)} = 1 - \dfrac{1}{n+1} \to_{n \to \infty} 1 
  \end{equation*}
  Donc la série de terme générale $u_n$ est convergente et sa somme vaut $1$
\end{ex}
\begin{ex}
  Soit $u_n = \log \left(1 + \frac{1}{n}\right)$, alors on a 
  \begin{align*}
    u_n &= \log \frac{n+1}{n} \\ 
        &= \log n+1 - \log n
  \end{align*}
  Donc,
  \begin{align*}
    \sum_{k=1}^{n} u_n &= \sum_{k=1}^n \log n+1 - \log n \\
                       &= \log n+1 - \log 1 && \text{somme téléscopique} \\ 
                       &= \log n+1 \\
                       &\to \infty
  \end{align*}
  Donc la série de terme générale $u_n$ est divergente
\end{ex}
\subsection{Sommes de séries numériques}
\begin{prop}[Somme]
  Soit $(v_n) \in \rN$ et $(u_n) \in \rN$ et $w_n = u_n + v_n$ 
  \begin{itemize}
    \item Si $\sum u_n$ et $\sum v_n$ convergent alors $\sum w_n$ converge
    \item Si $\sum u_n$ CV (resp DV) et $\sum v_n$ DV (resp CV) alors $\sum w_n$ diverge
    \item Si $\sum u_n$ et $\sum v_n$ divergent alors on ne peut rien dire de $\sum w_n$
  \end{itemize}
\end{prop}
\section{Séries géométriques}
\begin{definition}
  Soit $r \in \bR$ alors on appelle suite géométrique une suite de la forme $u_n = r^n$ et alors 
  la série de terme général $u_n$ est appelée \textit{série géométrique} de raison $r$
\end{definition}
\begin{theorem}[Théorèmes des Séries Géométriques]
  Soit $u_n = r^n$ et $\sum u_n$ la série associée, alors on a 
  \begin{equation*}
    \sum u_n \text{ CV } \Leftrightarrow \abs{r} < 1 
  \end{equation*}
  Et dans ce cas alors on a 
  \begin{equation*}
    \sum_{n=0}^{\infty} u_n = \frac{1}{1-r}
  \end{equation*}
  \begin{proof}
    Soit $r \in \bR$, soit $n \in \bN$ 
    \begin{align*}
      (1-r)S_n &= (1-r)\cdot \sum_{i=0}^n r^n \\
               &= \sum_{i=0}^n (1-r)r^n && (1-r) \in \bR \\ 
               &= \sum_{i=0}^n r^n - r^{n+1} \\ 
               &= 1 - r^{n+1} && \text{somme téléscopique} \\
               &\to \deq{1}{\abs{r} < 1}{\text{DV}}
  \end{align*}
  D'où dans le cas convergent 
  \begin{equation*}
    S = \frac{1}{1-r}
  \end{equation*}
  \end{proof}
\end{theorem}
\section{Séries À Termes Positifs (SATP)}
\begin{definition}
  Soit $(u_n) \in \rNp$ on appelle alors la série de terme générale $\sum u_n$ une \textit{série à terme positifs}, abrégés \textit{SATP}
\end{definition}
\subsection{Introduction}
\begin{prop}
  Soit $(u_n) \in \rNp$ 
  \begin{equation*}
    \left(\sum u_n \text{ CV}\right) \Leftrightarrow (S_n)_{n\in \bN} \text{ majorée}
  \end{equation*}
  \begin{proof}
    $\sum (u_n)$ est une \satp donc la suite $(S_n)_{n\in \bN}$ des sommes partielles est croissante donc par 
    théorème des limites monotones $(S_n)_{n\in \bN}$ converge si et seulement si elle est majorée
  \end{proof}
\end{prop}
\subsection{Comparaison}
\begin{theorem}[Théorème de Comparaison des SATP]
  Soit $(u_n) \in \rNp$ et $(v_n) \in \rNp$ tels que 
  \begin{equation*}
    \forall n \in \bN, 0 \leq u_n \leq v_n
  \end{equation*}
  On a,
  \begin{enumerate}
    \item Si $\sum v_n$ CV alors $\sum u_n$ CV
    \item Si $\sum u_n$ DV alors $\sum v_n$ DV
  \end{enumerate}
  \begin{proof}
    Conséquence du théorème des limites monotones 
  \end{proof}
\end{theorem}
\begin{prop}
  Soit $(u_n),(v_n) \in \rNp$ telles que 
  \begin{equation*}
    u_n \equiv_{\infty} v_n
  \end{equation*}
  Alors $\sum u_n$ et $\sum v_n$ sont de même nature
\end{prop}
\begin{ex}
  Soit $u_n = \ln \left(1 + \frac{1}{n}\right)$ on a $u_n \equiv \frac{1}{n}$ et $\sum u_n$ DV donc la série 
  $\sum \frac{1}{n}$ est divergente
  \begin{rmq}
    La série $\sum \frac{1}{n}$ est appelée \textit{série harmonique}
  \end{rmq}
\end{ex}

\subsection{Liaison séries intégrales}
\begin{theorem}[Comparaison Série/Intégrale]
  Soit $f\geq 0$, $\mC^0$, et décroissante à partir d'un réel $x_0$, alors $\sum f(n)$ et $\int^{\infty} f$ sont de même nature
  \begin{proof}
    Soit $f\geq 0$, $\mC^0$ et décroissante, soit $n \in \bN$ on observe que 
    \begin{align*}
      I(n+1) &\leq S_n && {et} \\ 
      S_n &\leq f(0) + I(n)
    \end{align*}
    Donc si $\sum f(n)$ converge alors par comparaison à termes positifs on a $I(n)$ qui converge par la première inégalité, et 
    si $\sum f(n)$ diverge alors par la deuxième inégalité $I(n)$ diverge. Donc $I$ et $\sum f(n)$ sont de même nature
  \end{proof}
\end{theorem}

\subsection{Séries de Riemann}
\begin{theorem}[Critère de Riemann]
  \begin{equation*}
    \sum \frac{1}{n^{\alpha}} \text{ CV} \Leftrightarrow \alpha > 1 \text{.}
  \end{equation*}
  \begin{proof}
    Soit $\alpha \in \bR$ soit $f(x) = \frac{1}{x^{\alpha}}$, $f$ est $\mC^0$, à terme positifs et décroissante 
    donc par comparaison série intégrale $\int f$ et $\sum f(n)$ sont de même nature, or par critère de Riemann dans les 
    intégrales $\int f$ converge si et seulement si $\alpha > 1$ donc $\sum f(n) = \sum \frac{1}{n^{\alpha}}$ converge si et 
    seulement si $\alpha > 1$.
  \end{proof}
\end{theorem}
\begin{rmq}
  Si $u_n \equiv \frac{A}{n^{\alpha}}$ alors $\sum u_n$ CV si et seulement si $\alpha > 1$ 
\end{rmq}
\begin{prop}[Règle de Riemann]
  Soit $(u_n) \in \rNp$, alors 
  \begin{itemize}
    \item Si il existe $\alpha > 1$ tel que $n^{\alpha}u_n \to l \in \bR$ alors $\sum u_n$ CV
    \item Si il existe $\alpha \leq 1$ tel que $n^{\alpha}u_n \to l \in \bar{\bR^{*}}$ alors $\sum u_n$ DV
  \end{itemize}
  \begin{proof}
    Conséquence du critère de Riemann.
  \end{proof}
\end{prop}
\subsection{Règle de Cauchy}
\begin{theorem}[Règle de Cauchy]
  Soit $(u_n) \in \rNp$, telle que 
  \begin{equation*}
    \ln \sqrt[n]{u_n} = l \in \bR
  \end{equation*}
  On a 
  \begin{itemize}
    \item Si $l < 1$ alors $\sum u_n$ CV
    \item Si $l > 1$ alors $\sum u_n$ DV 
    \item Si $l=1$ on ne peut pas déterminer la nature de $\sum u_n$ par cette méthode
  \end{itemize}
  \begin{proof}
    Soit $(u_n) \in \rNp$ telle que $\ln \sqrt[n]{u_n} = l \in \bR$ alors 
    \begin{equation*}
      \forall \varepsilon > 0, \exists n_0 \in \bN, \forall n \geq n_0, \abs{\sqrt[n]{u_n} - l} \leq \varepsilon
    \end{equation*}
    Donc 
    \begin{equation*}
      \forall \varepsilon > 0, \exists n_0 \in \bN, \forall n \geq n_0, l - \varepsilon \leq \sqrt[n]{u_n} \leq l + \varepsilon
    \end{equation*}


    Supposons $l < 1$ alors prenons $\varepsilon$ tel que $\varepsilon+l < 1$, on prend le $n_0$ associé, et soit $n \geq n_0$
    \begin{align*}
      \sqrt[n]{u_n} \leq l + \varepsilon < 1
    \end{align*}
    Donc 
    \begin{align*}
      (\sqrt[n]{u_n})^{n} &\leq \left(l + \varepsilon\right)^n \\
      u_n &\leq \left(l + \varepsilon\right)^n
    \end{align*}
    Or $\abs{l +\varepsilon} < 1$ donc par théorème des séries géométriques $\sum (l+\varepsilon)^n$ converge, donc 
    par comparaison de \satp, $\sum u_n$ converge.\newline

    
    Supposons maintenant que $l > 1$ alors prenons $\varepsilon$ tel que $l - \varepsilon > 1$, et soit $n \geq n_0$
    \begin{align*}
      \left(l - \varepsilon\right) &\leq \sqrt[n]{u_n} \\ 
      \left(l - \varepsilon\right)^n &\leq \sqrt[n]{u_n}^n \\
      \left(l - \varepsilon\right)^n &\leq u_n
    \end{align*}
    Or $\abs{l - \varepsilon} > 1$ donc par théorèmes des séries géométriques $\sum (l-\varepsilon)^n$ diverge, donc 
    par comparaisons de \satp, $\sum u_n$ diverge.
  \end{proof}
\end{theorem}
\subsection{Règle d'Alembert}
\begin{theorem}[Règle d'Alembert]
  Soit $(u_n) \in \rNp$ tel que 
  \begin{equation*}
    \ln \frac{u_{n+1}}{u_n} = l \in \bR
  \end{equation*}
  On a 
  \begin{itemize}
    \item Si $l < 1$ alors $\sum u_n$ CV 
    \item Si $l > 1$ alors $\sum u_n$ DV 
    \item Si $l=1$ alors on ne peut pas déterminer la nature de $\sum u_n$ par cette méthode
  \end{itemize}
  \begin{proof}
    Soit $(u_n) \in \rNp$ telle que $\ln \frac{u_{n+1}}{u_{n}} \to l \in \bR$.\newline

    
    Supposons que $l > 1$ alors a que $\exists n_0 \in \bN, \forall n \geq n_0, u_{n+1} > u_{n}$. 
    Donc $(u_n)$ est strictement croissante \apcr, donc par le théorème des limites monotones $\ln u_n = \infty$ donc 
    la série $\sum u_n$ est grossièrement divergente.\newline


    Supposons que $l < 1$ alors soit $q \in \bR$ tel que $l < q < 1$. On a qu'il existe un rang $n_0$ tel que 
    $l < q$ ainsi on a \apcr
    \begin{equation*}
      \frac{u_{n+1}}{u_n} < q 
    \end{equation*}
    Donc par produit téléscopique on a que la suite $(u_n)$ est majorée par la suite $v = (u_{n_0}q^{n-n_0})$, 
    or $\abs{q} < 1$ donc par théorème des séries géométriques $\sum v_n$ converge, donc par comparaison de séries à termes 
    positifs $\sum u_n$ converge. 
  \end{proof}
\end{theorem}
\section{Séries de signes non constant}
\begin{definition}
  Soit $(u_n) \in \rN$, Soit $\sum u_n$ la série de terme général $u_n$.
  \begin{itemize}
    \item Si $\exists \lambda \in \bR, \ln \sum_{k=0}^n u_k = \lambda$ alors $\sum u_n$ est dite \textit{simplement convergente}
    \item Si $\exists \mu \in \bR, \ln \sum_{k=0}^n \abs{u_k} = \mu$ alors $\sum u_n$ est dite \textit{absolument convergente}
  \end{itemize}
\end{definition}
  \begin{theorem}
    La convergence absolue implique la convergence simple 
    \begin{proof}
      voir la preuve du théorème analogue pour les suites
    \end{proof}
  \end{theorem}
  \begin{ex}
    Soit $u_n = \frac{\cos n^2}{n^4}$.\newline 

    On a $\forall n \in \bN, \abs{\frac{\cos n^2}{n^4}} \leq \frac{1}{n^4}$, or par critère de Riemann la série $\sum \frac{1}{n^4}$ converge 
    donc par comparaison de série à terme positive la série $\sum \abs{\frac{\cos n^2}{n^4}}$ converge donc la série
    $\sum \frac{\cos n^2}{n^4}$ est absolument convergente donc elle converge simplement.
  \end{ex}
  \subsection{Séries Alternées}
  \begin{definition}
    Soit $(u_n) \in \rN$ on dit que la série $\sum u_n$ est \textit{alternée} si et seulement si la suite $(-1)^nu_n$ est de signe constant
  \end{definition}
  \begin{theorem}[Critère Spécial des Séries Alternées (CSSA)]
    Soit $(u_n) \in \rN$ telle que $\sum u_n$ soit une série alternée, 
    si la suite $(\abs{u_n})$ est décroissante et que $\ln u_n = 0$ alors $\sum u_n$ converge
    \begin{proof}
      Considérons, sans perte de généralitée que $u_0 > 0$ alors on a 

      \begin{align*}
        S_n &= u_0 + u_1 + \cdots + u_n \\ 
         &= \abs{u_0} - \abs{u_1} \cdots (-1)^n u_n && \text{il vient} \\ 
        S_{n} - S_{n-2} &= (-1)^{n-1} \abs{u_{n-1}} + (-1)^n \abs{u_n} \\ 
        &= (-1)^n \left(\abs{u_n} - \abs{u_{n-1}}\right)
      \end{align*}
      
      Or par hypothèse $\abs{u_n}$ est décroissante donc la quantité $\abs{u_n} - \abs{u_{n-1}}$ est négative.\newline 
      \begin{itemize}
        \item Si $n$ est pair alors $n=2p$ et la quantité $S_{2p} - S_{2p-2}$ est négative donc la suite $(S_{2p})$ est décroissante
        \item Si $n$ est impair alors $n=2p+1$ et la quantité $S_{2p+1} - S_{2p-1}$ est positive donc la suite $(S_{2p+1})$ est croissante
        \item De plus $S_{2p+1}-S_{2p} = u_{2p+1} \to 0$ 
      \end{itemize}
      Donc les suites $(S_{2p})$ et $(S_{2p+1})$ sont adjacente, donc 
      \begin{equation}
        \exists S \in \bR, \lim_{p \to \infty} S_{2p} = \lim_{p \to \infty} S_{2p+1} = S
      \end{equation}
      Donc par théorème des indices pair et impair la suite $(S_n)$ converge donc la série $\sum u_n$ est convergente
    \end{proof}
  \end{theorem}
  \begin{ex}
    Soit $u_n = \frac{(-1)^n}{n+1}$ on a $\abs{u_n} = \frac{1}{n+1}$ donc décroissante, et $\ln u_n = 0$ car $(-1)^n$ est bornée
    donc par critère spécial des séries alternées la série $\sum u_n$ converge 
  \end{ex}

\chapter{Séries Entières}
\begin{definition}
  Soit $(u_n) \in \rN$ on définie $(f_n)_{n\in\bN}$ par \vfunc{f_n}{\bR}{\bR}{x}{a_nx^n} on appelle la série de terme générale $f_n(x)$ 
  soit la suite $\left(\sum_{k=0}^n f_k(x)\right)_{n\in \bN}$ \textit{série entière centrée en 0} ou plus simplement \textit{série entière}.\newline

  Le but de l'étude est de trouver l'ensemble $D$ pour lesquels $\forall x \in D, \sum f_n(x)$ converge, autrement dit 
  l'ensemble de définition de la fonction $\left(x \mapsto \sum_{n=0}^{\infty} a_n x^n\right)$ 
\end{definition}
\begin{rmq}
  Toute série entière converge pour $x=0$
\end{rmq}
\begin{rmq}
  Soit $(x_1,x_2) \in \bR^2$ tels que $\abs{x_1} < \abs{x^2}$ alors $\abs{a_nx_1^n} < \abs{a_nx_2^n}$ donc on en tire les conclusions suivantes 
  \begin{itemize}
    \item Si $\sum a_n x_2^n$ est absolument convergente alors $\sum a_n x_1^n$ est ACV 
    \item Si $\sum a_n x_1^n$ est divergente alors $\sum a_nx_2^n$ est DV
  \end{itemize}
\end{rmq}
\section{Domaine de convergence}
\begin{theorem}[Lemme d'Abel]
  Si il existe $x_0 \not=0$ tel que $(a_nx_0^n)$ soit borné alors $\forall x \in \bR$ tels que $\abs{x} < \abs{x_0}$, $\sum a_n x^n$ ACV 
  \begin{proof}
    Soit $x \in \bR$ tel que $\abs{x} < \abs{x_0}$ alors 
    \begin{align*}
      \abs{a_nx^n} &= \abs{a_nx_0^n} \cdot \abs{\frac{x}{x_0}}^n \\ 
      &\leq M \abs{\frac{x}{x_0}}^n
    \end{align*}
    Or $\sum \left(\frac{x}{x_0}\right)^n$ est ACV car $\abs{\frac{x}{x_0}} < 1$ donc $\sum a_n x^n$ est absolument convergente.
  \end{proof} 
\end{theorem}
\begin{rmq}
  Si il existe $x_0 \not=0$ tel que $\sum (a_nx_0^n)$ soit simplement convergente alors $\forall x \in \bR$ tels que $\abs{x} < \abs{x_0}$, $\sum a_n x^n$ ACV 
  \begin{proof}
    La convergence simple implique que la suite est borné \apcr
  \end{proof}
\end{rmq}
\begin{rmq}
  Par contraposée on obtient que si il existe $x_0 \not 0$ tel que $\sum a_n x_0^n$ DV alors $\forall x \in \bR$ tels que $\abs{x} > \abs{x_0}$, $\sum a_n x^n$ DV
\end{rmq}
\section{Rayon et intervalle de convergence}
\begin{definition}
  Pour toute série entière $\sum a_n x^n$ il existe $R \in \bar{\bR}$ tel que $R >= 0$ et 
  \begin{itemize}
    \item $\forall x \in \bR$ tels que $\abs{x} < R$ la série entière $\sum a_n x^n$ est ACV
    \item $\forall x \in \bR$ tels que $\abs{x} > R$ la série entière $\sum a_n x^n$ est DV
  \end{itemize}
  On appelle $R$ le \textit{rayon de convergence} de la série entière $\sum a_n x^n$ et l'intervalle $]-R,R[$ est appelé \textit{intervalle de convergence}.
\end{definition}
\begin{rmq}
  Pour une petite apparté, d'un point de vue général pour $(c_n) \in \bC^{\bN}$ et $a \in \bC$ on a que la série $\sum c_n (z-a)^n$ est la série 
  entière centrée en a de $(c_n)$. On a alors convergence si $\abs{z - a} < R$ et divergence si $\abs{z - a} > R$ ce qui dans le plan complexe représente 
  un disque de centre $a$ et de rayon $R$ (disque sans le bord), d'où l'appellation \textit{rayon de convergence}
\end{rmq}
\section{Calcul du rayon de convergence}
On peut utiliser la règle d'Alembert (ou de la même manière la règle de cauchy) pour calculer l'inverse du rayon de convergence
\begin{prop}

  Soit $(a_n) \in \rN$ telle que $\ln \abs{\frac{a_{n+1}}{a_n}} = l \in \bar{\bR}$, $l >= 0$, on a 
  \begin{itemize}
    \item si $l\abs{x} < 1$ alors $\sum \abs{a_nx^n}$ CV 
    \item si $l\abs{x} > 1$ alors $\sum \abs{a_nx^n}$ DV
  \end{itemize}
  \begin{proof}
    
  \begin{equation}
    \frac{\abs{a_{n+1}x^{n+1}}}{\abs{a_nx^n}} = \abs{\frac{a_{n+1}}{a_n}}\abs{x}
  \end{equation}
  D'après les hypothèses on a que $\ln \abs{\frac{a_{n+1}}{a_n}} = l \in \bar{\bR}$ et $l >= 0$
  on a alors d'après la règle d'Alembert on obtient le résultat donné en énoncé.
\end{proof}
\end{prop}
\begin{rmq}
  De la même manière on peut calculer $l$ avec la règle de Cauchy.
\end{rmq}
\begin{prop}
  Soit $l \in \bar{\bR}$
  \begin{itemize}
    \item $R = \frac{1}{l}$ si $l \not= 0$
    \item $R = \infty$ si $l=0$ 
    \item $R = 0$ si $l = \infty$
  \end{itemize}
  \begin{proof}
    Conséquence de la proposition précédente
  \end{proof}
\end{prop}

\begin{ex}
  Soit la série entière $\sum \frac{x^n}{n2^n}$ on a alors $a_n = \frac{1}{n2^n}$
  \begin{align*}
    \frac{a_{n+1}}{a_n} &= \frac{\frac{1}{(n+1)2^{n+1}}}{\frac{1}{n2^n}} \\ 
    &= \frac{n2^n}{(n+1)2^{n+1}} \\
    &= \frac{1}{2} \cdot \frac{n}{n+1}
    \to \frac{1}{2}
  \end{align*}
  Donc $l = \frac{1}{2}$ ce qui donne $R = 2$ donc la série entière est convergente sur au moins $]-2,2[$\newline

  Observons maintenant en $x=2$ on a alors 
  \begin{align*}
    \left(\sum \frac{x^n}{n2^n}\right)(2) &= \sum \frac{2^n}{n2^n} \\ 
    &= \sum \frac{1}{n}
  \end{align*}
  ce qui est divergent par le critère de Riemann, donc la série entière est divergente en 2.\newline

  Enfin, en $x=-2$
  \begin{align*}
    \left(\sum \frac{x^n}{n2^n}\right)(-2) &= \sum \frac{(-2)^n}{n2^n} \\ 
    &= \sum \frac{(-1)^n}{n}
  \end{align*}
  Ce qui est convergent par le critère spécial des séries alternées, donc la série entière est convergente en -2.\newline


  On a donc enfin que l'intervalle de convergence de $\sum \frac{x^n}{n2^n}$ est $[-2,2[$
\end{ex}
\section{Somme des Séries Entières}
\begin{definition}
  Soit $a = (a_n) \in \rN$, alors on note la \textit{fonction somme de la série entière} $\sum a_n x^n$ la fonction suivante
  \vfunc{S_a(x)}{]-R,R[}{\bR}{x}{\sum_{n=0}^{\infty} a_n x^n}
\end{definition}
\begin{prop}
  La fonction $S_a$ est $\mC^0$ sur $]-R,R[$.
\end{prop}
\begin{rmq}
  Si la SE est définie en $x=R$ (resp $x=-R$) alors on peut prolonger par continuité $S_a$ en $x=R$ (resp $x=-R$)
\end{rmq}

\subsection{Dérivation et Intégration des Séries Entières}
\begin{definition}
  Soit $(a_n) \in \rN$, on dispose de la série entière $SE = \sum a_n x^n$ de rayon de convergence $R$ alors 
  \begin{itemize}
    \item La série entière $\sum n a_n x^{n-1}$ est nommée \textit{série dérivée} de SE, de rayon de convergence $R_d$
    \item La série entière $\sum a_n \frac{x^{n+1}}{n+1}$ est nommée \textit{série intégrée} de SE, de rayon de convergence $R_i$
  \end{itemize}
\end{definition}
\begin{theorem}
  \begin{equation*}
    R = R_d = R_i
  \end{equation*}


  \begin{proof}
    Soit $(a_n) \in \rN$, nous allons montrer que $R = R_d$
    \begin{itemize}
      \item Soit $x$ tel que $\abs{x} < R_d$ donc $\sum n a_n x^{n-1}$ est ACV, $x \in \bR$ donc une multiplication par $x$ ne change pas 
      la nature de la suite donc $\sum n a_n x^n$ est ACV. Or
      \begin{equation*}
        \forall n \geq 1, \abs{a_n x^n} \leq \abs{n a_n x^n}
      \end{equation*}
      Donc par comparaison de SATP, la série $\sum a_n x^n$ est ACV donc $\abs{x} \leq R$ par propriété du rayon de convergence donc $R \leq R_d$
      \item Soit $x$ tel que $\abs{x} < R$ soit $\rho \in ]\abs{x},R[ \not= \emptyset$ on a 
      \begin{equation*}
        \abs{n a_n x^{n-1}} = \abs{a_n \rho^n} \cdot \abs{\frac{nx^{n-1}}{\rho^{n-1}} \cdot \frac{1}{\rho}}
      \end{equation*}
      Or 
      \begin{equation*}
        \ln \abs{\frac{nx^{n-1}}{\rho^{n-1}} \cdot \frac{1}{\rho}} = 0
      \end{equation*}
      car $\abs{\frac{x}{\rho}} < 1$
      donc
      \begin{equation*}
        \exists N_0 \in \bN, \forall n \geq N_0, \abs{n a_n x^{n-1}} \leq \abs{a_n \rho^n}
      \end{equation*}
      Or $\rho < R$ donc $\sum a_n \rho^n$ est ACV donc par comparaison de SATP $\sum n a_n x^{n-1}$ est ACV donc $R_d \leq R$
    \end{itemize}
    On a $R_d \leq R$ et $R \leq R_d$ donc $R = R_d$ par anti-symétrie de $(\leq)$.\newline 

    La série $SE$ est la série intégrée, puis dérivée donc naturellement on a $R = R_i$ par le point précédent, donc finalement
    \begin{equation*}
      R = R_i = R_d
    \end{equation*}
  \end{proof}
\end{theorem}
\begin{ex}
  On a que $\sum \frac{x^n}{n2^n}$ est $R=2$ donc $\sum \frac{x^{n-1}}{2^n}$ a $R=2$ par dérivation etc.
\end{ex}
\begin{rmq}
  Attention, les propriétés aux bord $R$ et $-R$ ne sont pas conservées.
\end{rmq}
\section{Développement en Séries Entières}
\begin{definition}
  Soit $\func{f}{D \subset \bR}{\bR}$, on dit que $f$ est \textit{développable en série entière}, si
  \begin{equation*}
    \exists (a_n) \in \rN, \lim_{n \to \infty} \sum_{k=0}^n a_k x^k = f
  \end{equation*}
\end{definition}
\subsection{Conditions nécessaires}
\begin{theorem}
  Soit $f$ DSE, alors $\forall n \in \bN f^{n}$ est définie en $0$
  \begin{proof}
  \begin{equation*}
    \exists (a_n) \in \rN, \exists R \in \bR, \forall x \text{ tq } \abs{x} < R, f(x) = \sum_{n=0}^{\infty} a_n x^n
  \end{equation*}
  D'où
  \begin{equation*}
    f(0) = a_0
  \end{equation*}
  Puis par dérivation
  \begin{equation*}
    f'(0) = a_1
  \end{equation*}
  Enfin par récurrence 
  \begin{equation*}
    \forall n \in \bN, f^{(n)}(0) = n! a_n \textit{ (Taylor) }
  \end{equation*}
\end{proof}
\end{theorem}
\begin{ex}
  Par exemple la fonction $\left(x \mapsto \abs{x}\right)$ n'est pas dérivable en $0$ donc elle n'est pas DSE
\end{ex}
\begin{rmq}
  En remaniant l'équation on obtient
  \begin{equation*}
    \forall n \in \bN, a_n = \frac{f^{(n)}}{n!}
  \end{equation*}
  Ce qui garantie l'unicité d'un développement en série entière 
\end{rmq}
\begin{prop}
  Si $f$ est DSE alors $f \in \mC^{\infty}(]-R,R[)$
  \begin{proof}
    Conséquence du théorème précédent
  \end{proof}
\end{prop}
\begin{rmq}
  La réciproque est fausse.
  \begin{proof}
    Soit \vfunc{f}{\bR}{\bR}{x}{\deq{e^{\frac{-1}{x^2}}}{x \not= 0}{0}}, on suppose qu'elle est DSE.
    Alors on voit que par récurrence 
    \begin{equation*}
      \forall n \geq 1, \lim_{x\to0} f^{(n)}(x) = 0
    \end{equation*}
    Donc toutes les dérivées peuvent être prolongées par continuité en $0$ et on obtient (car $f(0)=0$)
    \begin{equation*}
      \forall n \geq 0, f^{(n)}(0) = 0
    \end{equation*}
    Or d'après le point précédent ça veut dire que 
    \begin{equation*}
      \forall n \in \bN, a_n = 0
    \end{equation*}
    donc que 
    \begin{align*}
      \forall x \in \bR, f(x) &= \sum a_n x^n \\ 
      &= \sum 0 \\
      &= 0
    \end{align*}
    Or $f$ n'est pas la fonction nulle, par exemple $f(1) = e^{-1} \not= 0$ donc c'est absurde, donc $f$ n'est pas DSE.
  \end{proof}
\end{rmq}
\section{Opération sur les DSE}
\begin{prop}
  Soit $f,g$ DSE on a 
  \begin{itemize}
    \item $(f+g)$ DSE 
    \item $\forall \lambda \in \bR, (f+\lambda g)$ DSE 
    \item $(fg)$ DSE 
    \item $\forall k \in \bN, f^k$ DSE
    \item $\forall k \in \bN, \left(x \mapsto f(x^k)\right)$ DSE
  \end{itemize}
\end{prop}
\begin{rmq}
  Attention aux intervalles de convergences quand $R_f \not= R_g$ 
\end{rmq}
\section{DSE usuels}
\begin{prop}
  \begin{align*}
    e^{x} &= \sum_{k=0}^{\infty} \frac{x^k}{k!} && R=+\infty \\
    \sin x &= \sum_{k=0}^{\infty} \frac{(-1)^k x^{2k+1}}{(2k+1)!} && R=+\infty \\
    \cos x &= \sum_{k=0}^{\infty} \frac{(-1)^{k} x^{2k}}{(2k)!} && R=+\infty \\
    \frac{1}{1-x} &= \sum_{k=0}^{\infty} x^k && R=1 \\
    \log (1-x) &= \sum_{k=1}^{\infty} - \frac{x^k}{k} && R=1 \\
    \frac{1}{1+x} &= \sum_{k=0}^{\infty} (-1)^k x^k && R=1 \\ 
    \log (1+x) &= \sum_{k=1}^{\infty} \frac{(-1)^{k+1} x^k}{k} && R=1 \\ 
    \frac{1}{1+x^2} &= \sum_{k=0}^{\infty} (-1)^k x^{2k} && R=1 \\
    \arctan x &= \sum_{k=0}^{\infty} \frac{(-1)^k x^{2k+1}}{(2k+1)!} && R=1 \\ 
    (1+x)^{\alpha} &= DL_{\infty} && R=1
  \end{align*}
\end{prop}
\begin{rmq}
  Pour $\log (1-x)$ et $\arctan(x)$ par le CSSA elles convergent en $x=1$
\end{rmq}
\subsection{Application à la somme d'une série numérique}
\begin{rmq}
  On peut utiliser les DSE pour calculer une série numérique, exemple avec $\sum \frac{n}{2^n}$
  \begin{align*}
    \forall x \in ]-1,1[, &\sum x^n = \frac{1}{1-x} \\ 
    &\sum nx^{n-1} = \frac{1}{(1-x)^2} && \text{dérivation} \\ 
    &\sum nx^{n} = \frac{x}{(1-x)^2} && \text{multiplication par }x \\ 
    &=2 && \text{en } x=\frac{1}{2}
  \end{align*}
\end{rmq}


\chapter{Applications Linéaires}
\section{Définition}
\begin{definition}
  Soit $E$ et $F$ deux espaces vectoriels sur $\bK$, on dit que $f$ est une application linéaire de $E$ dans $F$ si
  \begin{align*}
    &\forall (x,x') \in E^2, f(x+x') = f(x) + f(x') && \text{et} \\
    &\forall \lambda \in \bK, \forall x \in E, f(\lambda x) = \lambda f(x)
  \end{align*}
  On note l'ensemble des applications linéaire (\textit{morphisme}) de $E$ dans $F$, $\mfc{L}(E,F)$
\end{definition}
\begin{rmq}
  Cette définition est équivalente à 
  \begin{equation*}
    (f \in \mfc{L}(E,F)) \Leftrightarrow \forall (x,x') \in E^2, \forall \lambda \in \bK, f(\lambda x + x') = \lambda f(x) + f(x') \in F
  \end{equation*}
\end{rmq}
\begin{rmq}
  On déduit des axiomes que $f(0_E) = 0_F$
\end{rmq}
\begin{rmq}
  Si $E = F$ alors on dit que $f$ est un \textit{endomorphisme} et on note $\mfc{L}(E)$ l'ensemble des endomorphismes de E.
\end{rmq}
\section{Noyaux et Images}
\begin{definition}
  On définie
  \begin{align*}
    &\ker f = \left\{x \in E | f(x) = 0_F\right\} && \text{et} \\ 
    &\Im f = \left\{f(x), x \in E\right\}
  \end{align*}
\end{definition}
\begin{prop}
  On a que $\ker f$ est un \sev  de $E$ et $\Im f$ est un \sev  de $F$
  \begin{proof}
    Conséquences de la linéarité de $f$
  \end{proof}
\end{prop}
\subsection{Propriétés}
\begin{prop}
  Soit $E$ et $F$ deux \ev, et $f \in \mor$ 
  \begin{equation*}
    f \text{ injective } \Leftrightarrow \ker f = \{0_E\}
  \end{equation*}
  \begin{proof}
    Soit $f \in \mor$
    \begin{itemize}
      \item Sens $f$ injective $\Rightarrow \ker f = \{0_E\}$
      \begin{equation*}
        f(0_E) = 0_F 
      \end{equation*}
      Donc
      \begin{equation*}
        0_E \in \ker f
      \end{equation*}
      Soit $x \in \ker f$ donc
      \begin{equation*}
        f(x) = 0_E
      \end{equation*}
      Or par injectivité de $f$ ce $x$ est unique donc il n'existe pas de $y \not= x \in E$ tel que $f(y) = \ker E$ donc
      \begin{equation*}
        \ker f \subset \{0_E\}
      \end{equation*}
      Par double inclusion on a donc 
      \begin{equation*}
        \ker f = \{0_E\}
      \end{equation*}
      \item Sens $\ker f = \{0_E\} \Rightarrow f$ injective.
      \begin{equation*}
        \text{Soit } (x,y) \in E^2 | f(x) = f(y)
      \end{equation*}
      Donc on a 
      \begin{equation*}
        f(x) - f(y) = 0_F
      \end{equation*}
      Donc par linéarité de $f$ 
      \begin{equation*}
        f(x - y) = 0_F
      \end{equation*}
      Or $\ker f = \{0_E\}$ donc 
      \begin{align*}
        &x-y = 0_E \\ 
        &x=y
      \end{align*}
      Donc $f$ est injective
    \end{itemize}
    Donc par double inclusion la propriété est démontrée.
  \end{proof}
\end{prop}
\section{Rang}
\begin{definition}
  Soit $E,F$ deux \ev de dimension finie alors on définie le \textit{rang} de $f \in \mor$ comme 
  \begin{equation*}
    \text{rg}(f) = \dim \Im f
  \end{equation*}
\end{definition}
\begin{theorem}[Théorème du rang]
  Soit $E,F$ deux \ev de dimension finie alors on a 
  \begin{equation*}
    \dim E = \dim \Im f + \dim \ker f
  \end{equation*}
\end{theorem}
\begin{prop}[Corollaire]
  Soit $E,F$ deux \ev de dimension finie alors
  \begin{equation*}
    \forall f \in \mor, \dim \Im f \leq \dim E
  \end{equation*}
  Le cas d'égalité est pour $f \in \mor$ injective
\end{prop}
\section{Matrice d'une application linéaire}
\begin{definition}
  Soit $E,F$ deux \ev de dimensions finies tels que $\dim E = p$ et $\dim F = n$, et soit $f \in \mor$.
  Soit $\mfc{B} = (e_1,\cdots,e_p)$ une base de $E$ et $\mfc{V} = (v_1,\cdots,v_n)$ une base de $f$ 
  alors la matrice de $f$ exprimé de la base $\mfc{B}$ dans la base $\mfc{V}$ est noté
  \begin{equation*}
    \text{Mat}_{\mfc{B},\mfc{V}} f = \begin{bmatrix}
      f(e_1) \cdots f(e_p)
    \end{bmatrix} = \begin{bmatrix}
      a_{1,1} & \cdots & a_{1,p} \\
      a_{2,1} & \cdots & a_{2,p} \\ 
      \vdots & \vdots & \vdots \\ 
      a_{n,1} & \cdots & a_{n,p}
    \end{bmatrix} \in \mfc{M}_{n,p}(\bK)
  \end{equation*}
\end{definition}
\begin{rmq}
  Par la méthode des pivots on se rend compte que 
  \begin{equation*}
    \text{rang}(f) = \text{rang}\left(\text{Mat}_{\mfc{B},\mfc{V}} f\right)
  \end{equation*}
\end{rmq}
\begin{ex}
  Soit $B = (i,j)$ la base canonique de $\bR^2$ et $C = (a,b,c)$ la base canonique de $\bR^3$
  Soit \vfunc{f}{\bR^2}{\bR^3}{\alpha i+ \beta j}{(2\alpha - \beta)a + \alpha b + (7\alpha - 9\beta)c}
  Alors 
  \begin{equation*}
    \text{Mat}_{B,C} f = \begin{bmatrix}
      2 & -3 \\ 
      1 & 0 \\ 
      7 & -9
    \end{bmatrix}
  \end{equation*}
  Car 
  \begin{align*}
    f(i) &= 2a + b + 7c \\ 
    f(j) &= -3a -9c
  \end{align*}
\end{ex}
\begin{ex}
  Soit $E,F$ deux \ev tels que $\dim E = p$ et $\dim F = n$ et $r = \text{rang}(f)$ donc $\dim \ker f = p - r$ 
  On prend comme base de $\ker f$
  \begin{equation*}
    (k_1,\cdots,k_{p-r}) 
  \end{equation*}
  que l'on complète en une base de $E$ grâce au théorème de completion des bases en dimension finie par $(s_1,\cdots,s_r)$ et finalement 
  \begin{equation*}
    \mfc{B} = (k_1,\cdots,k_{p-r},s_1,\cdots,s_r)
  \end{equation*}
  Base de $E$
  de la même manière on prend
  \begin{equation*}
    \mfc{V} = (f(s_1),\cdots,f(s_r),t_{r+1},\cdots,t_{n})
  \end{equation*}
  Base de $F$, la famille $(f(s_1),\cdots,f(s_r))$ étant libre car $(s_1,\cdots,s_r)$ l'est et $f$ est un morphisme. On a alors 
  \begin{equation*}
    \text{Mat}_{\mfc{B},\mfc{V}} = \begin{bmatrix}
      I_r & 0_{p-r} \\ 
      0_{n-r} & 0
    \end{bmatrix}
  \end{equation*}
\end{ex}
\section{Matrice d'un endomorphisme}
\begin{definition}
  Soit $E$ un \ev, $f$ est un endomorphisme de E si $f \in \mfc{L}(E,E)$ on note $f \in \endo$, si $\dim E = n$
  et $\mfc{B}$ la base canonique de $E$ alors 
  \begin{equation*}
    \text{Mat}_{\mfc{B},\mfc{B}} f = \text{Mat}_{\mfc{B}} f \in \mfc{M}_{n}(\bK)
  \end{equation*}
\end{definition}
\subsection{Image d'un vecteur}
\begin{definition}
  Soit $E$ deux \ev, de dimension $p$ soit $\mfc{B}$ une base de $E$,
  soit $x \in E$ alors on défini le vecteur associé de la base $\mfc{B}$ à x comme ceci
  \begin{equation*}
    \text{Mat}_{\mfc{B}} x = \begin{bmatrix}
      x_1 \\ 
      \vdots \\ 
      x_p
    \end{bmatrix}
  \end{equation*}
  où $(x_1,\cdots,x_p)$ sont les coordonnées de $x$ dans la base $\mfc{B}$
\end{definition}
\begin{prop}
  Soit $E,F$ deux \ev de dimensions respectives $p$ et $n$, $\mfc{B}$ une base de $E$, $\mfc{V}$ une base de $F$, et $f \in \mor$.
  Soit $Y = \text{Mat}_{\mfc{V}} f(x)$, soit $X = \text{Mat}_{\mfc{B}} x$ alors on a 
  \begin{equation*}
    Y = \text{Mat}_{\mfc{B},\mfc{V}} f \times X
  \end{equation*}
\end{prop}
\section{Endomorphismes}
\subsection{Matrices de changement de base}
\begin{definition}
  Soit $E$ un \ev de dimension $n$, soit $\mfc{B}$ et $\mfc{B}'$ deux bases de $E$ on cherche a exprimer les coordonnées de $x$ d'une base 
  à une autre. 
  \begin{equation*}
    \text{Mat}_{\mfc{B}} x = \text{Mat}_{\mfc{B},\mfc{B}'} I_d \times \text{Mat}_{\mfc{B}'} x
  \end{equation*}
  en pratique ce qui nous interesse est la matrice $\text{Mat}_{\mfc{B}',\mfc{B}}$ pour changer des coordonnées connues à la nouvelle base, on a 
  \begin{equation*}
    \text{Mat}_{\mfc{B}',\mfc{B}} I_d = \left(\text{Mat}_{\mfc{B},\mfc{B}'}\right)^{-1}
  \end{equation*}
  on sait que $\text{Mat}_{\mfc{B},\mfc{B}'} \in \mfc{GL}_n(\bK)$ car toutes les familles sont libres. 
  Une telle matrice est appellée \textit{matrice de changement de base}
\end{definition}
\begin{ex}
  Soit $B = (i,j)$ la base canonique de $\bR^2$, soit $x = \begin{pmatrix}
    5 \\ 
    1
  \end{pmatrix}$
  Soit $u = 2i + j$ et $v = -i + j$, on note $B' = (u,v)$
  \begin{align*}
    P &= \text{Mat}_{B,B'} I_2 \\ 
      &= \begin{bmatrix}
        2 & -1 \\ 
        1 & 1
      \end{bmatrix}
  \end{align*}
  On a 
  \begin{equation*}
    \det P = 2 + 1 = 3
  \end{equation*}
  Donc 
  \begin{align*}
    P^{-1} &= \frac{1}{\det P} \begin{bmatrix}
      1 & 1 \\ 
      -1 & 2
    \end{bmatrix} \\ 
    &= \frac{1}{3} \begin{bmatrix}
      1 & 1 \\ 
      -1 & 2
    \end{bmatrix}
  \end{align*}
  D'où
  \begin{align*}
    X' &= \frac{1}{3} \begin{bmatrix}
      1 & 1 \\ 
      -1 & 2
    \end{bmatrix} \times \begin{pmatrix}
      6 \\ 
      1
    \end{pmatrix} \\ 
    &= \begin{bmatrix}
      2 \\ 
      -1
    \end{bmatrix}
  \end{align*}
  d'où
  \begin{equation*}
    x = 2u - v
  \end{equation*}
  \begin{rmq}
    C'est équivalent à résoudre un système d'équation (l'inversion de matrice est essentiellement ça)
  \end{rmq}
\end{ex}
\subsection{Matrices équivalentes}
\begin{definition}
  On dit que $A$ et $A'$ sont équivalentes s'il existe $P \in \mfc{GL}_n(\bK)$ et $Q \in \mfc{GL}_m(\bK)$ telles que 
  \begin{equation*}
    A' = Q^{-1} A P
  \end{equation*}
\end{definition}
\subsection{Matrices semblables}
\begin{definition}
  On dit que $A$ et $A'$ sont semblable s'il existe $P \in \mfc{GL}_n(\bK)$ tel que 
  \begin{equation*}
    A' = P^{-1} A P
  \end{equation*}
\end{definition}
\begin{rmq}
  Deux matrices semblables représente le même endomorphisme dans des bases différentes
\end{rmq}
\begin{rmq}
  \begin{equation*}
    \text{rang}(A) = \text{rang}(A') = \text{rang}(f)
  \end{equation*}
\end{rmq}
\subsection{Application}
\begin{ex}
  Soit \vfunc{f}{\bR^2}{\bR^2}{\begin{pmatrix}
    x \\ 
    y
  \end{pmatrix}}{\begin{pmatrix}
    17x - 45y \\ 
    6x - 16y
  \end{pmatrix}}
  Dans la base canonique $B = (i,j)$ de $\bR^2$ on a 
  \begin{equation*}
    \text{Mat}_{B} f = \begin{bmatrix}
      17 & -45 \\ 
      6 & -16
    \end{bmatrix}
  \end{equation*}
  \begin{rmq}
    Mais que se passe-t-il si on veut calculer la matrice de $f^5$ ou $f^{10}$ les calculs deviennent vites horrible
  \end{rmq}
  \begin{equation*}
    \text{Mat}_{B} f^5 = \begin{bmatrix}
      197 & -495 \\ 
      66 & -166
    \end{bmatrix}
  \end{equation*}
  Il faut donc trouver une base dans lesquels c'est plus simple par exemple 
  \begin{equation*}
    B' = (u,v)
  \end{equation*}
  avec $u = 3i + j$ et $v = 5i + 2j$
  \begin{equation*}
    P = \text{Mat}_{B,B'} I_2 = \begin{bmatrix}
      3 & 5 \\ 
      1 & 2
    \end{bmatrix}
  \end{equation*}
  donc 
  \begin{equation*}
    P^{-1} = \begin{bmatrix}
      2 & -5 \\ 
      -1 & 3
    \end{bmatrix}
  \end{equation*}
  On a ensuite 
  \begin{align*}
    A' &= P^{-1} A P \\ 
       &= \begin{bmatrix}
        2 & -5 \\ 
        -1 & 3
       \end{bmatrix} 
       \begin{bmatrix}
        17 & -45 \\ 
        6 & -16 
       \end{bmatrix}
       \begin{bmatrix}
        3 & 5 \\ 
        1 & 2
       \end{bmatrix} \\ 
       &= \begin{bmatrix}
        2 & -5 \\ 
        -1 & 3
       \end{bmatrix} 
       \begin{bmatrix}
        6 & -5 \\ 
        2 & -2
       \end{bmatrix} \\
       &= \begin{bmatrix}
        2 & 0 \\ 
        0 & -1
       \end{bmatrix} \in \mfc{D}_2(\bK)
  \end{align*}
  cette matrice étant diagonale on peut facilement calculer $f^{5}$ et $f^{10}$ dans $B'$.\newline 

  La recherche d'une telle base est appelée \textit{réduction des endomorphismes}
\end{ex}


\chapter{Système linéaire homogène}
\begin{definition}
  Soit $A = [a_{i,j}] \in \mathcal{M}_{n,p}(\bK)$ soit $X = \begin{pmatrix}
    x_1 \\ 
    \vdots \\ 
    x_p
  \end{pmatrix} \in \mathcal{M}_{p,1}(\bK)$ 
  on appelle système linéaire homogène l'équation 
  \begin{equation*}
    AX = 0_{\mathcal{M}_{n,p}(\bK)}
  \end{equation*}
  aussi noté 
  \begin{equation*}
    \begin{cases}
      a_{1,1} x_1 + a_{1,2} x_2 + \cdots + a_{1,p} x_p = 0 \\
      a_{2,1} x_1 + a_{2,2} x_2 + \cdots + a_{2,p} x_p = 0 \\ 
      \vdots \\ 
      a_{n,1} x_1 + a_{n,2} x_2 + \cdots + a_{n,p} x_p = 0
    \end{cases}
  \end{equation*}
\end{definition}
\begin{rmq}
  En prenant $\mathcal B_i$ la base canonique de $\bR^i$ et en prenant $f : \bR^p \to \bR^n$ tel que 
  \begin{equation*}
    A = \text{Mat}_{\mathcal{B_p},\mathcal{B_n}} f
  \end{equation*}
  alors le système est équivalent à trouver $\ker f$. En pratique on cherche $\text{rg}(S) = \text{rg}(A)$ pour en déduire 
  la dimension du noyau de $f$ par le \textit{théorème du rang} et en prennant des vecteurs libre de $A$ 
\end{rmq}
\begin{ex}
  Soit $(S)$ le système 
  \begin{equation*}
    \begin{cases}
      x+y = 0 \\ 
      2x - y + z = 0
    \end{cases}
  \end{equation*}
  On a $A = \begin{bmatrix}
    1 & 2 & 0 \\ 
    2 & -1 & 1
  \end{bmatrix}$ $\Rightarrow$ $\begin{bmatrix}
    1 & 2 & 0 \\
    0 & -1 & 1 \\
  \end{bmatrix}$ 
  par
  $C_1 \leftarrow C_1 - 2C_2$
  et donc $\text{rg}(A) = 2$ donc $\dim \ker A = 1$ 
  On a alors 
  \begin{equation*}
    (S) \Leftrightarrow \begin{cases}
      x+y = 0 \\ 
      z = 3y
    \end{cases}
  \end{equation*}
  D'où $S = \text{Vect}((-1,1,3))$ la droite vectorielle des solutions
\end{ex}
\begin{rmq}
  On peut arriver au même résultat en appliquant la \textit{méthode de fresnel}
\end{rmq}

\chapter{Matrices}
\section{Transposée}
\begin{definition}
  Soit $M = (a_{i,j}) \in \mathcal{M}_{n,p}(\bK)$ alors on définie la transposée de $M$ notée $~^{t}M$ définie par
  \begin{equation*}
    ~^{t}M[i,j] = M[j,i]
  \end{equation*}
\end{definition}
\begin{rmq}
  Visuellement, la matrice transposée de $M$ est le symétrique des coefficients de $M$ par sa diagonale
\end{rmq}
\begin{prop}[Propriétés]
  Soit $A,B \in \mathcal M_{n,p}(\bK)$ alors 
  \begin{itemize}
    \item $~^{t}(A+B) = ~^{t}A + ~^{t}B$
    \item $~^{t}(AB) = ~^{t}B \cdot ~^{t}A$
  \end{itemize}
\end{prop}
\section{Matrices carrées}
\subsection{Matrices symétriques}
\begin{definition}
  Soit $S \in \mathcal{M}_n(\bK)$ on dit que $S$ est \textit{symétrique} si 
  \begin{equation*}
    ~^{t}S = S
  \end{equation*}
  On note $\mathcal S_n(\bK)$ l'ensemble des matrices symétriques de dimension $n$ par $n$
\end{definition}
\subsection{Trace d'une matrice}
\begin{definition}
  Soit $A = (a_{i,j}) \in \mathcal{M}_n(\bK)$ alors on appelle \textit{trace} le scalaire tel que 
  \begin{equation*}
    \text{tr}(A) = \sum_{i=1}^n a_{i,i}
  \end{equation*}
\end{definition}
\begin{prop}[Propriétés]
  Soit $A,B \in \mathcal{M}_n(\bK)$ et $\alpha \in \bK$ alors 
  \begin{itemize}
    \item $\text{tr}(A+B) = \text{tr}(A) + \text{tr}(B)$ 
    \item $\text(tr)(\alpha A) = \alpha \text{tr}(A)$
    \item $\text{tr}(~^t A) = \text{tr}(A)$
  \end{itemize}
  \begin{proof}
    Soit $A,B \in \mathcal{M}_n(\bK)$, alors 
    \begin{align*}
      \text{tr}(A+B) &= \sum_{i=1}^n [A]_{i,i} + [B]_{i,i} \\ 
      &= \sum_{i=1}^n [A]_{i,i} + \sum_{i=1}^n [B]_{i,i} \\ 
      &= \text{tr}(A) + \text{tr}(B)
    \end{align*}.
    Et soit $\alpha \in \bK$ alors 
    \begin{align*}
      \text{tr}(\alpha A) &= \sum_{i=1}^n [\alpha A]_{i,i} \\ 
      &= \sum_{i=1}^n \alpha [A]_{i,i} \\ 
      &= \alpha \sum_{i=1}^n [A]_{i,i} \\ 
      &= \alpha \text{tr}(A)
    \end{align*}
    Le troisième est par conservation de la diagonale lors de la transposition
  \end{proof}
\end{prop}
\begin{rmq}
  Ces propriétés nous disent que 
  \vfunc{\text{tr}(\cdot)}{\mathcal{M}_n(\bK)}{\bK}{M}{\text{tr}(M)}
  est une forme linéaire sur $\mathcal{M}_n(\bK)$ (et que donc $\ker \text{tr}$ est un hyperplan de $\mathcal{M}_n(\bK)$)
\end{rmq}
\begin{theorem}
  Soit $A,B \in \mathcal{M}_n(\bK)$ on a 
  \begin{equation*}
    \text{tr}(AB) = \text{tr}(BA)
  \end{equation*}
  \begin{proof}
    Pour rappel on a 
    \begin{equation*}
      [AB]_{i,j} = \sum_{k=1}^n [A]_{i,k} \cdot [B]_{k,j}
    \end{equation*}
    Donc 
    \begin{align*}
      \text{tr}(AB) &= \sum_{i=1}^n [AB]_{i,i} \\ 
      &= \sum_{i=1}^n \sum_{i=1}^n [A]_{i,k} \cdot [B]_{k,i} \\ 
      &= \sum_{k=1}^n \sum_{i=1}^n [A]_{i,k} \cdot [B]_{k,i} \\ 
      &= \sum_{k=1}^n \sum_{i=1}^n [~^{t}A]_{k,i} \cdot [~^{t}B]_{i,k} \\ 
      &= \sum_{k=1}^n [~^{t}A \cdot ~^{t}B]_{k,k} \\ 
      &= \sum_{k=1}^n [~^{t}(BA)]_{k,k} \\ 
      &= \text{tr}(~^{t}(BA)) \\
      &= \text{tr}(BA)
    \end{align*}
  \end{proof}
\end{theorem}
\begin{rmq}
  En conséquence de ceci, deux matrices semblables ont la même trace.
\end{rmq}
\subsection{Déterminant}
\begin{definition}
  Soit $A \in \mathcal{M}_n(\bK)$, on note $A_{i,j} \in \mathcal{M}_{n-1,n-1}$ la matrice à laquelle on a enlevé la $i$ème ligne et la 
  $j$ème colonne. Alors on appelle déterminant l'unique application telle que 
  \begin{equation*}
    \det A = \begin{cases}
      a_{1,1} &\text{si } n=1 \\ 
      \sum_{j=1}^{n} (-1)^{1+j} a_{1,j} \det A_{1,j}
    \end{cases} \in \bK
  \end{equation*}
  On appelle alors cette application le \textit{développement de det(A) suivant la 1ère ligne} (on peut le faire pour n'importe quelle ligne ou colonne)
\end{definition}
\begin{ex}
  Soit $A = \begin{bmatrix}
    a_{1,1} & a_{1,2} \\ 
    a_{2,1} & a_{2,2} 
  \end{bmatrix}$
  alors 
  \begin{align*}
    \det A &= \begin{vmatrix}
      a_{1,1} & a_{1,2} \\ 
      a_{2,1} & a_{2,2} 
    \end{vmatrix} \\ 
    &= a_{1,1} \abs{a_{2,2}} - a_{1,2} \abs{a_{2,1}} \\ 
    &= a_{1,1}a_{2,2} - a{1,2}a_{2,1}
  \end{align*}
\end{ex}
\begin{theorem}
  En dimension 2, soit $A = \begin{bmatrix}
    a & b \\ 
    c & d
  \end{bmatrix}$ 
  alors 
  \begin{equation*}
    \det A = ad - bc 
  \end{equation*}
  \begin{proof}
    démonstration au dessus
  \end{proof}
\end{theorem}
\begin{ex}
  Soit $A = \begin{bmatrix}
    4 & 2 \\ 
    3 & 5
  \end{bmatrix}$
  alors 
  \begin{align*}
    \det A &= \begin{vmatrix}
      4 & 2 \\ 
      3 & 5
    \end{vmatrix} \\ 
    &= 4 \cdot 5 - 3 \cdot 2 \\ 
    &= 20 - 6 \\ 
    &= 14
  \end{align*}
\end{ex}
\subsubsection{Interprétation géométrique}
\begin{definition}
  Pour $A \in \mathcal{M}_{n}(\bR)$ alors $\det A$ est le volume de l'hyper-parralépipède de dimension $n$ s'appuyant sur les vecteurs colonnes de la matrice $A$.
  \begin{rmq}
    Cela ce voit particulièrement bien en dimension 2, où $\det A$ est l'aire d'un parralélogramme.
  \end{rmq}
\end{definition}
\subsubsection{Dimension 3}
\begin{definition}
  Soit $A = (a_{i,j})$ pour $A$ une matrice carrée de dimension $3$ on a 
  \begin{equation*}
    \det A = a_{1,1} \cdot a_{2,2} \cdot a_{3,3} + a_{2,1} \cdot a_{3,2} \cdot a_{1,3} + 
    a_{3,1} \cdot a_{1,2} \cdot a_{2,3} -
    a_{3,1} \cdot a_{2,2} \cdot a_{1,3} - 
    a_{1,1} \cdot a_{3,2} \cdot a_{2,3} -
    a_{2,1} \cdot a_{1,2} \cdot a_{3,3}
  \end{equation*}
  \begin{proof}
    Soit $A = (a_{i,j})$ alors on a 
    \begin{align*}
      \det A &= a_{1,1} \begin{vmatrix}
        a_{2,2} & a_{2,3} \\ 
        a_{3,2} & a_{3,3}
      \end{vmatrix} -
      a_{1,2} \begin{vmatrix}
        a_{2,1} & a_{2,3} \\ 
        a_{3,1} & a_{3,3} 
      \end{vmatrix} + 
      a_{1,3} \begin{vmatrix}
        a_{2,1} & a_{2,2} \\ 
        a_{3,1} & a_{3,2}
      \end{vmatrix} \\ 
      &= a_{1,1} (a_{2,2}a_{3,3} - a_{3,2}a_{2,3}) - a_{1,2} (a_{2,1}a_{3,3} - a_{3,1}a_{2,3}) + a_{1,3} (a_{2,1}a_{3,2} - a_{3,1}a_{2,2}) \\ 
      &= a_{1,1}a_{2,2}a_{3,3} + a_{1,2}a_{3,1}a_{2,3} + a_{1,3}a_{2,1}a_{3,2} - a_{1,1}a_{3,2}a_{2,3} - a_{1,2}a_{2,1}a_{3,3} - a_{1,3}a_{3,1}a_{2,2}
    \end{align*}
  \end{proof}
\end{definition}
\begin{rmq}
  Avec des lettres on a $\det A = aei + dhc + gbf - gec - ahf - dbi$
\end{rmq}
\begin{theorem}[Règle de Sarrus]
  Une façon plus simple pour se souvenir de cette formule est de prendre la matrice $5$ par $5$ 
  \begin{equation*}
    \begin{bmatrix}
      a & b & c \\ 
      d & e & f \\ 
      g & h & i \\ 
      a & b & c \\ 
      d & e & f
    \end{bmatrix}
  \end{equation*}
  et de faire la somme des produit des diagonales descendantes moins la somme des produits des diagonales montantes et on a en effet 
  \begin{equation*}
    aei + dhc + gbf - gec - ahf - dbi
  \end{equation*}
\end{theorem}
\begin{ex}
  Soit $A = \begin{bmatrix}
    1 & 1 & 2 \\ 
    1 & -2 & 3 \\ 
    2 & 0 & 4
  \end{bmatrix}$
  où
  \begin{align*}
    \det A &= -8 + 0 + 6 - (-8) - 0 - 4 \\ 
    &= 2
  \end{align*}
\end{ex}
\subsection{Matrices triangulaire}
\begin{definition}
  Soit $T \in \mathcal{M}_n(\bK)$ une matrice on dit que $T$ est \textit{triangulaire supérieure} si 
  \begin{equation*}
    T = \begin{bmatrix}
      a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\ 
      0 & a_{2,2} & \cdots & a_{2,n} \\ 
      0 & \vdots & \vdots & \vdots \\ 
      0 & \cdots & \cdots & a_{n,n}
    \end{bmatrix}
  \end{equation*}
  Ou autrement définie, $T = (a_{i,j})$ est triangulaire supérieure si $\forall i \in [\![1,n]\!], \forall j < i, a_{i,j} = 0$
  On note l'ensemble des matrices triangulaire supérieures de dimensions $n$ par $n$,  $\mathcal{T}^{+}_n$
  \end{definition}
  \begin{rmq}
    De manière analogue on définie les matrices triangulaire inférieures, $\mathcal T^{-}_n$
  \end{rmq}
  \begin{theorem}
    Soit $T \in \mathcal{T}^{+}_n$ alors 
    \begin{equation*}
      \det T = \prod_{i=1}^n a_{i,i}
    \end{equation*}
    \begin{proof}
      Par récurrence sur $n$ et par définition.
    \end{proof}
  \end{theorem}
\subsection{Propriété du déterminant}
\begin{theorem}
  Le déterminant est invariant par combinaison linéaire de colonnes et de lignes de la matrice (Gauss)
\end{theorem}
\begin{prop}
  Soit $A \in \mathcal{M}_n(\bK)$, on note $C_j$ la colonne $j$ de $A$, on dit que $C_j \equiv C_i$ si, $\exists \alpha \in \bK^{*}, C_j = \alpha C_i$
  \begin{equation*}
    \det A = 0 \Leftrightarrow \exists i,j \in [\![1,n]\!]^2, i \not= j \wedge C_j \equiv C_i \\ 
  \end{equation*}
\end{prop}
\subsection{Matrices inversible}
\begin{definition}
  Soit $A \in \mathcal{M}_n(\bK)$ on dit que $A$ est inversible si et seulement si 
  \begin{equation*}
    \exists B \in \mathcal{M}_n(\bK), AB = BA = I_n
  \end{equation*}
  où $I_n$ est la matrice identité de $\mathcal{M}_n(\bK)$.\newline

  L'ensemble des matrices inversibles de $\mathcal{M}_n(\bK)$ est noté $\mathcal{GL}_n(\bK)$ où \textit{groupe linéaire}, et 
  \begin{equation*}
    \mathcal G = (\mathcal{GL}_n(\bK),\cdot)
  \end{equation*}
  où $(\cdot)$ est la multiplication matricielle, est un groupe, (tous les éléments sont inversible et le reste est hérité de la structure d'espace vectoriel de $\mathcal{M}_n(\bK)$)
\end{definition}
\begin{prop}[Critère d'inversibilité]
  $A \in \mathcal{M}_n(\bK)$ est inversible si et seulement si $\det A \not= 0$
  \begin{proof}
    Soit $A \in \mathcal{M}_n(\bK)$
    \begin{itemize}
      \item Supposons que $\det A \not= 0$ alors selon la partie précédente la famille de ses vecteurs colonnes est libre, on a donc $\text{rg}(A) = n$ donc $A$ est inversible
      \item Supposons que $A$ est inversible, supposons que $\det A = 0$ alors il existe $(i,j) \in [\![1,n]\!]^2$ tels que $C_i \equiv C_j$ donc la famille 
      des vecteurs colonnes n'est pas libre donc $\dim \ker A \leq 1$ donc $\text{rg} A < n$ donc $A$ n'est pas inversible, ce qui est absurde, donc $\det A \not= 0$   
    \end{itemize}
  \end{proof}
\end{prop}
\subsubsection{Co-matrice}
\begin{definition}
  Soit $A \in \mathcal{M}_n(\bK)$ alors on définie la co-matrice de $A$, la matrice $\text{com}(A) = (c_{i,j})$ telle que 
  \begin{equation*}
    \forall (i,j) \in [\![1,n]\!]^2, c_{i,j} = (-1)^{i+j} \det A_{i,j}
  \end{equation*}
\end{definition}
\begin{prop}
  Soit $A \in \mathcal{M}_n(\bK)$, on a alors 
  \begin{equation*}
    A \cdot \text{com}(~^tA) = \det A\cdot I_n
  \end{equation*}
  où $I_n$ est l'identité de $\mathcal{M}_n(\bK)$
  \begin{proof}
    Soit $A \in \mathcal{M}_n(\bK)$, montrons que
    \begin{equation*}
      A \cdot \text{com}(~^tA) = \det A\cdot I_n
    \end{equation*}
    Il faut donc montrer, que 
    \begin{equation*}
      \forall (i,j) \in [\![1,n]\!]^2, [A\cdot \text{com}(~^tA)]_{i,j} = [\det A\cdot I_n]_{i,j}
    \end{equation*}
    D'abord calculons à gauche
    \begin{equation*}
      \forall (i,j) \in [\![1,n]\!]^2, [\det A\cdot I_n]_{i,j} = \begin{cases}
        0 & \text{si } i \not= j \\ 
        \det A & \text{si } i=j
      \end{cases}
    \end{equation*}
    Calculons donc dans le cas $i=j$ à droite 
    \begin{equation*}
      [A\cdot \text{com}(~^tA)]_{i,i} = \sum_{k=1}^n [A]_{i,k} \cdot [\text{com}(~^tA)]_{k,i}
    \end{equation*}
    Or on a 
    \begin{equation*}
      [\text{com}(~^tA)]_{k,i} = (-1)^{k+i} \det ~^tA_{k,i}
    \end{equation*}
    d'où
    \begin{align*}
      [A\cdot \text{com}(~^tA)]_{i,i} &= \sum_{k=1}^n [A]_{i,k} \cdot (-1)^{k+i} \det ~^tA_{k,i} \\ 
      &= \sum_{k=1}^n (-1)^{i+k} a_{i,k} \det ~^tA_{k,i} 
    \end{align*}
    or $~^tA_{k,i} = A_{i,k}$ donc
    \begin{equation*}
      [A\cdot \text{com}(~^tA)]_{i,i} = \sum_{k=1}^n (-1)^{i+k} a_{i,k} \det A_{i,k}
    \end{equation*}
    ce qui est le développement de la i-ième ligne de $\det A$ donc 
    \begin{equation*}
      [A\cdot \text{com}(~^tA)]_{i,i} = \det A
    \end{equation*}
    On a prouvé que
    \begin{equation*}
      \forall i \in [\![1,n]\!], [A\cdot \text{com}(~^tA)]_{i,i} = \det A
    \end{equation*}
    il reste à prouver que 
    \begin{equation*}
      \forall (i,j) \in [\![1,n]\!]^2, i\not=j \Rightarrow [A\cdot \text{com}(~^tA)]_{i,j} = 0
    \end{equation*}
    Soit $(i,j) \in [\![1,n]\!]^2, i\not=j$,
    \begin{align*}
      [A\cdot \text{com}(~^tA)]_{i,j} &= \sum_{k=1}^n [A]_{i,k} \cdot \text{com}(~^tA)_{k,j} \\ 
      &= \sum_{k=1}^n a_{i,k} \cdot (-1)^{k+j} \cdot \det (~^tA)_{k,j} \\ 
      &= \sum_{k=1}^n a_{i,k} \cdot (-1)^{k+j} \cdot \det A_{j,k} \\ 
      &= \sum_{k=1}^n a_{i,k} \text{com}(A)_{j,k}
    \end{align*}
    Or regardons le déterminant de la matrice $A$ où l'on a remplacé la i-ième ligne par la j-ième par les formules de développement à 
    la j-ième ligne on a
    \begin{equation*}
      \det A' = \sum_{k=1}^n (-1)^{k+j} a_{j,k} \det A_{j,k} 
    \end{equation*}
    Or on a remplacé la i-ième ligne par la j-ième on a donc $\forall k \leq n, a_{i,k} = a_{j,k}$ donc 
    \begin{equation*}
      \det A' = \sum_{k=1}^n (-1)^{k+j} a_{i,k} \det A_{j,k}
    \end{equation*}
    d'où 
    \begin{equation*}
      [A\cdot \text{com}(~^tA)]_{i,j} = \det A'
    \end{equation*}
    Or $A'$ a deux lignes identiques, donc le déterminant est nul donc $\det A' = 0$ donc 
    \begin{equation*}
      \forall (i,j) \in [\![1,n]\!]^2, i \not= j \Rightarrow [A\cdot \text{com}(~^tA)]_{i,j} = 0
    \end{equation*}
    Donc, enfin par égalité des coefficients on a

    \begin{equation*}
      A\cdot \text{com}(~^tA) = \det A \cdot I_n
    \end{equation*}
  \end{proof} 
\end{prop}
\begin{theorem}
  Si $A \in \mathcal{GL}_n(\bK)$ alors 
  \begin{equation*}
    A^{-1} = \frac{1}{\det A} \text{com}(~^tA)
  \end{equation*}
  \begin{proof}
    conséquence direte de la proposition précédente.
  \end{proof}
\end{theorem}
\begin{rmq}[Inversibilité en dimension 2]
  Soit $A \in \mathcal{GL}_2(\bK) = \begin{bmatrix}
    a & b \\ 
    c & d
  \end{bmatrix}$
  on a $~^tA = \begin{bmatrix}
    a & c \\ 
    b & d
  \end{bmatrix}$
  et 
  \begin{equation*}
    \text{com}(~^tA) = \begin{bmatrix}
      d & -c \\ 
      -b & a
    \end{bmatrix}
  \end{equation*}
  d'où 
  \begin{equation*}
    A^{-1} = \frac{1}{ad-bc} \begin{bmatrix}
      d & -c \\ 
      -b & a
    \end{bmatrix}
  \end{equation*}
\end{rmq}
\begin{ex}
  Soit $A = \begin{bmatrix}
    1 & -1 & 4 \\ 
    2 & 1 & -1 \\ 
    3 & 2 & 1
  \end{bmatrix}$, on a donc 
  \begin{equation*}
  ~^tA = \begin{bmatrix}
    1 & 2 & 3 \\ 
    -1 & 1 & 2 \\ 
    4 & -1 & 1
  \end{bmatrix}
  \end{equation*}
  puis 
  \begin{equation*}
    \det A = -2 + 16 + 3 -12 -4 + 2 = 3
  \end{equation*}
  On a alors
  \begin{align*}
    \text{com}(~^tA) &= 
    \begin{bmatrix}
      \begin{vmatrix}
        1 & 2 \\ 
        -1 & -2
      \end{vmatrix}&
      - \begin{vmatrix}
        -1 & 2 \\ 
        4 & -2
      \end{vmatrix}&
      + \begin{vmatrix}
        -1 & 1 \\ 
        4 & -1
      \end{vmatrix}\\ 
      - \begin{vmatrix}
        2 & 3 \\ 
        -1 & -2
      \end{vmatrix}&
      \begin{vmatrix}
        1 & 3 \\ 
        4 & -2
      \end{vmatrix}&
      - \begin{vmatrix}
        1 & 2 \\ 
        4 & -1
      \end{vmatrix}\\ 
      \begin{vmatrix}
        2 & 3 \\ 
        1 & 2
      \end{vmatrix}&
      - \begin{vmatrix}
        1 & 3 \\ 
        -1 & 2
      \end{vmatrix}&
      \begin{vmatrix}
        1 & 2 \\ 
        -1 & 1
      \end{vmatrix}
    \end{bmatrix} \\ 
    &= \begin{bmatrix}
      0 & 6 & -3 \\ 
      1 & -14 & 9 \\ 
      1 & -5 & 3
    \end{bmatrix}
  \end{align*}
  d'où 
  \begin{equation*}
    A^{-1} = \frac{1}{3}
    \begin{bmatrix}
      0 & 6 & -3 \\ 
      1 & -14 & 9 \\ 
      1 & -5 & 3
    \end{bmatrix}
  \end{equation*}
\end{ex}

\chapter{Notations, rappels, et Hors Programme (HP)}
\section{Ensembles}
\begin{rmq}
  Soit $D$ un ensemble
  \begin{itemize}
    \item $\bar{D}$ est l'adhérence de $D$ c'est à dire le plus 
    petit ensemble fermé contenant $D$, par exemple $\bar{R} = \bR \cup \{-\infty,+\infty\}$
    \item Soit $\bK$ un corps, alors $\bK[X]$ est l'ensemble des polynomes 
    à coefficient dans $\bK$ a une indéterminée (en gros, variable)
    \item $\mfc{M}_{n,p}(\bK)$ est l'ensemble des matrices à $n$ lignes et $p$ colonnes à coefficient dans $\bK$
    \item $\mfc{M}_{n}(\bK) = \mfc{M}_{n,n}(\bK)$
    \item $\mfc{GL}_n(\bK)$ est l'ensemble des matrices inversibles de $\mfc{M}_n(\bK)$, c'est un groupe multiplicatif
  \end{itemize}
\end{rmq}
\section{Fonctions}
\subsection{Ensembles de fonctions}
\begin{rmq}
  Soit $E,F$ deux ensembles, et soit $I$ un interval de $\bR$
  \begin{itemize}
    \item $E^{F}$ est l'ensemble des applications (fonctions) de $F$ dans $E$
    \item En particulier $\rN$ est l'ensemble des suites réelles
    \item $\mC^0(I)$ est l'ensemble des fonctions continues sur $I$
    \item Dans le cas général $\mC^n(I)$ est l'ensemble des fonctions dérivable $n$ fois sur $I$ et 
    dont la n-ème dérivée est continue sur $I$
    \item On note $\mC^{\infty}(I)$ l'ensemble $\bigcup_{n=0}^{\infty} \mC^n(I)$. En pratique ces fonctions 
    sont dérivable une infinité de fois (par exemple les polynomes, exponentielle etc.)
    \item $\mM^0(I)$ est l'ensemble des fonctions continues par morceaux sur $I$
  \end{itemize}
\end{rmq}
\subsection{Opérations entre fonctions et fonctions et fonctions et scalaires}
\begin{rmq}
  Soit $f,g$ deux fonctions, Soit $\lambda \in \bR$
  \begin{itemize}
    \item $\lambda f$ est la fonction $\left(x \mapsto \lambda \cdot f(x)\right)$
    \item $f+g$ est la fonction $\left(x \mapsto f(x)+g(x)\right)$
    \item $fg$ est la fonction $\left(x \mapsto f(x)g(x)\right)$
    \item $f \circ g$ est la fonction $\left(x \mapsto f(g(x))\right)$
  \end{itemize}
\end{rmq}
\subsection{Comparaison entre fonctions et fonctions et fonctions et scalaires}
\begin{rmq}
  Soit $f,g$ deux fonctions et $\lambda \in \bR$
  \begin{itemize}
  \item $f \geq \lambda$ (resp $>, \leq, <$) représente $\forall x \in I, f(x) \geq \lambda$ (resp $>,\leq,<$)
  \item $f \geq g$ (resp $>,\leq,<$) représente $\forall x \in I, f(x) \geq g(x)$ (resp $>,\leq,<$)
  \item $f = o_a(g) \Leftrightarrow \lim_a \frac{f}{g} = 0$
  \item $f = \mathcal{O}_a(g) \Leftrightarrow \lim_a \frac{f}{g} \in \bR$
  \item $f \equiv_a g \Leftrightarrow \lim_a \frac{f}{g} = 1$
  \end{itemize}
\end{rmq}
\subsection{Limites, continuité et dérivabilité}
\begin{rmq}
  Soit $f$ une fonction définie sur $I$ et $a \in I$
  \begin{itemize}
    \item Définition de la limite de $f$ au point $a$
    \begin{equation*}
      \left(\lim_{x \to a} f(x) = l\right) \Leftrightarrow \left(\forall \varepsilon > 0, \exists 
      \nu > 0, \abs{x-a} < \nu \Rightarrow \abs{f(x) - l} < \varepsilon\right)
    \end{equation*}
    \item $\lim_a f = \lim_{x \to a} f(x)$ 
    \item $f$ est continue en $a$ si $\lim_a f = f(a)$ 
    \item $f$ est continue sur $I$ si $\forall x \in I, f$ est continue en $x$
    \item $f$ est dérivable en $a$ si le quotient $\frac{f(x) - f(a)}{x - a}$ admet une limite finie quand $x \to a$
    \item $f$ est dérivable sur $I$ si $\forall x \in I, f$ est dérivable en $x$
  \end{itemize}
\end{rmq}
\section{Bornes sup et bornes inf}
\begin{definition}
  On appelle la \textit{borne supérieure} de $D \subset \bR$ le nombre $M \in \bR$ caractérisé par 
  \begin{align*}
    &\forall x \in D, x \leq M && \text{M est un majorant de } D \\ 
    &\forall \lambda \text{ tq } (\forall x \in D, x \leq \lambda), M \leq \lambda && \text{M est le plus petit des majorants}
  \end{align*}
  On note alors 
  \begin{equation*}
    M = \sup A
  \end{equation*}
  Par analogie la \textit{borne inférieure} est le plus grand des minorants, noté 
  \begin{equation*}
    m = \inf A
  \end{equation*}
  Ces définitions sont soumises à condition d'existences (il faut que $A$ admette une borne sup/inf)
\end{definition}
\begin{rmq}
  Si $M = \sup A \in A$ (resp $m = \inf A \in A$) alors on dit que $M$ est le maximum de $A$ (resp $m$ 
  est le minimum de $A$) et on le note $M = \max A$ (resp $m = \min A)$ 
\end{rmq}
\begin{rmq}
  Le maximum (resp minimum) s'il existe est unique
  \begin{proof}
    Soit $M,M'$ deux maximum de $A$. On a alors par définition 
    \begin{align*}
      &\forall x \in A, x \leq M \\ 
      &\forall x \in A, x \leq M'
    \end{align*}
    or $M\in A$ et $M' \in A$ donc en particulier on a 
    \begin{align*}
      &M' \leq M \\ 
      &M \leq M'
    \end{align*}
    Par antisymétrie de $(\leq)$ on a $M=M'$
  \end{proof}
  La preuve est analogue pour l'unicité du minimum de $A$
\end{rmq}
\begin{theorem}[Propriété fondamentale de $\bR$]
  Soit $E \subset \bR$ tel que $E \not= \emptyset$ alors si $E$ est majoré il admet une borne supérieure.\newline
  Soit $E \subset \bR$ tel que $E \not= \emptyset$ alors si $E$ est minoré il admet une borne inférieure.
\end{theorem}
\begin{theorem}[Caractérisation séquentielle de la borne sup/inf]
  Soit $E \subset \bR$ non vide, et $M$ un majorant de $E$ (resp $m$ un minorant de $E$) alors 
  \begin{equation*}
    M = \sup E \Leftrightarrow \exists (a_n) \in E^{\bN}, \ln a_n = M
  \end{equation*}
  et 
  \begin{equation*}
    m = \inf E \Leftrightarrow \exists (a_n) \in E^{\bN}, \ln a_n = m
  \end{equation*}
\end{theorem}
\section{Convergence}
\begin{rmq}
  $ $
  \begin{itemize}
    \item Pour la convergence on abbrège \textit{CV} 
    \item Pour l'absolue convergence on dit \textit{ACV} 
    \item Pour la convergence uniforme on dit \textit{CVU} 
    \item Pour la convergence normale on dit \textit{CVN}
  \end{itemize}
\end{rmq}
\section{Suites, et séries}
\subsection{Suites de scalaires}
\begin{rmq}
  Soit $(a_n)_{n\in\bN}$ une suite de complexe (ou réels)
  \begin{itemize}
    \item L'ensemble de toutes les suites est noté $\rN$. $E = (\rN,+,\cdot)$ où $(\cdot)$ est la loi de composition externe suivante 
    \vfunc{\cdot}{\bK\times \rN}{\rN}{(\lambda,(a_n))}{(\lambda a_n)}
    est un espace vectoriel
    \item Une suite $(u_n)$ est dite convergente si $u_n \to \lambda \in \bR$ 
    \item Une suite $(u_n)$ est dite absolument convergente si $\abs{u_n} \to \lambda \in \bR$
  \end{itemize}
\end{rmq}
\subsection{Suites et série d'application}
\begin{rmq}
  Soit $I$ un interval de $\bR$, $f$ une fonction de $\bR^{I}$ et $(f_n) \in \left(\bR^{I}\right)^{\bN}$ une suite d'application.
  \begin{itemize}
    \item \textbf{HP} On dit que $(f_n)$ converge simplement sur $I$ vers $f$ si 
    \begin{equation*}
      \forall x \in I, \ln f_n(x) = f(x)
    \end{equation*}
    ce qui est équivalent à 
    \begin{equation*}
      \forall x \in I, \forall \varepsilon > 0, \exists N_x \in \bN, \forall n \geq N_x, \abs{f_n(x) - f(x)} < \varepsilon
    \end{equation*}
    \item \textbf{HP} On dit que $(f_n)$ converge uniformément sur $I$ vers $f$ si 
    \begin{equation*}
      \forall \varepsilon > 0, \exists N \in \bN, \forall n \geq N, \forall x \in I, \abs{f_n(x) - f(x)} < \varepsilon
    \end{equation*}
    La différence avec la définition de la convergence simple est dans l'emplacement du $\forall x \in I$ le $N$ ne dépend plus de $x$
    c'est le même pour tous!
    \item \textbf{HP} On dit que $\sum f_n$ converge absolument en tout point de $I$ si 
    \begin{equation*}
      \forall x \in I, \sum \abs{f_n(x)} \text{ CV}
    \end{equation*} 
    \item \textbf{HP} On dit que $\sum f_n$ converge normalement sur $A$ si 
    \begin{equation*}
      \sum \norm{f_n}^A_{\infty} \text{ CV}
    \end{equation*}
    La notation $\norm{f}_{\infty}$ étant la \textbf{norme infinie} de $f$ définie par 
    \begin{equation*}
      \norm{f}^A_{\infty} = \sup_{x \in A} \abs{f(x)}
    \end{equation*}
  \end{itemize}
\end{rmq}
\subsection{Séries}
\begin{rmq}
  Soit $(u_n) \in \rN$
  \begin{itemize}
    \item $\sum u_n$ est la suite $(S_n)_{n\in\bN}$ avec pour $n \in \bN$, $S_n = \sum_{k=0}^n u_k$ et 
    est \textit{la série de terme générale $u_n$}.
    \item Une SATP est une série dont tous les termes sont positifs
    \item On se ramène à l'étude d'une SATP en étudiant le module d'une série
  \end{itemize}
\end{rmq}
\subsection{Séries Entières}
\begin{rmq}
  Soit $(a_n) \in \rN$ 
  \begin{itemize}
    \item On note $\sum a_n x^n$ la suite de fonctions $\left(\sum_{k=0}^n a_k x^k\right)_{n\in \bN}$, appelée
    \textit{série entière} de terme général $a_n$
    \item $\sum a_n x^n$ est une suite de \textbf{fonctions}, alors que pour $\lambda \in D$ la suite $\left(\left(\sum a_n x^n\right)(\lambda)\right)_{n \in \bN}$
    est une suite de \textbf{réel} des fonctions évalués en $\lambda$ elle est égale à la suite de réelle $\left(\sum a_n \lambda^n\right)_{n \in \bN}$
    \end{itemize}
\end{rmq}
\section{Rappels d'Algèbre}
\subsection{Algèbre Générale}
\subsubsection{Loi de Composition Internes}
\begin{rmq}
  Soit $E$ un ensemble
  \begin{itemize}
    \item On dit que $(\cdot)$ est une \textit{loi de composition interne} sur $E$ si 
    \vfunc{(\cdot)}{E^2}{E}{(x,y)}{x\cdot y}
    \item On dit que $\varepsilon$ est un élement neutre pour $(\cdot)$ une lci si 
    \begin{equation*}
      \forall x \in E, \varepsilon \cdot x = x \cdot \varepsilon = x
    \end{equation*}
    Il est par définition unique.
    \item On dit que $(\cdot)$ est associative si 
    \begin{equation*}
      \forall (x,y,z) \in E^3, (x \cdot y) \cdot z = x \cdot (y \cdot z)
    \end{equation*}
    \item On dit que $(\cdot)$ est commutative si 
    \begin{equation*}
      \forall (x,y) \in E^2, x \cdot y = y \cdot x
    \end{equation*}
    \item On dit que $x \in E$ est \textit{inversible} pour la loi $(\cdot)$ si 
    \begin{equation*}
      \exists y \in E, x\cdot y = \varepsilon
    \end{equation*}
    On note alors $y = x^{-1}$
  \end{itemize}
\end{rmq}
\subsubsection{Magma et Monoïde}
\begin{rmq}
  Soit $E$ un ensemble et $(\cdot)$ une lci sur $E$ 
  \begin{itemize}
    \item On appelle la donnée du couple $(E,\cdot)$ un \textit{magma}
    \item On appelle la donnée du couple $(E,\cdot)$ où $(\cdot)$ possède un élément neutre un \textit{magma unifère}
    \item Si $(\cdot)$ est associative alors la donnée du couple $(E,\cdot)$ est appelée un \textit{magma associatif} ou plus rarement \textit{demi-groupe}
    \item On appelle la donnée du couple $(E,\cdot)$ où $(\cdot)$ est associative et possède un élement neutre un \textit{monoïde}
  \end{itemize}
\end{rmq}
\subsubsection{Groupe}
\begin{definition}
  Soit $\mathcal G = (E,\cdot)$ un monoïde d'élement neutre $\varepsilon$, on appelle $\mathcal G$ un \textit{groupe} si et seulement si 
  \begin{equation*}
    \forall x \in E, \exists y \in E, x \cdot y = y \cdot x = \varepsilon
  \end{equation*}
  autrement dit $\mathcal G$ est un monoïde où tous les élements sont inversibles (ou symétrisable)
\end{definition}
\begin{definition}
  Soit $F \subset E$ on dit que $F$ est un \textit{sous groupe} de $(E,\cdot)$ si
  \begin{itemize}
    \item $(E,\cdot)$ est un groupe
    \item \begin{equation*}
      \forall y \in F, y^{-1} \in F
    \end{equation*}
  \end{itemize}
\end{definition}
\begin{rmq}
  Si $\mathcal G = (E,\cdot)$ est un groupe et que $(\cdot)$ est commutative alors on appelle $\mathcal G$ un \textit{groupe abélien} ou \textit{groupe commutatif}
\end{rmq}
\begin{ex}
  On a comme groupes notables,
  \begin{itemize}
    \item $(\bC,+)$ et ses sous groupes $(\bR,+),(\bQ,+),(\bZ,+)$ etc. 
    \item $(\bN,+)$ est un \textit{monoïde} mais pas un groupe car aucun élement (autre que $0$) n'est inversible pour $(+)$
    \item $(\bC^*,\cdot)$ est un groupe ainsi que ses sous groupes.
  \end{itemize}
\end{ex}
\begin{theorem}[Théorème de caractérisation des sous groupes de $(\bR,+)$ HP]
  Soit $H \in \bR$ tel que $(H,+)$ soit un sous groupe de $(\bR,+)$ alors
  \begin{itemize}
    \item Soit $H$ est dense dans $\bR$
    \item Soit $\exists \alpha \in \bR^{+}, H = \alpha \bZ$ où $\alpha \bZ = \left\{\alpha z, z\in \bZ\right\}$
  \end{itemize}
  \begin{proof}
    Encore une fois le théorème est \textbf{Hors Programme}, la preuve encore plus, mais la voilà pour les curieux,\newline\par
    Soit $H \subset \bR$ tel que $(H,+)$ est un groupe, on suppose que 
    \begin{equation}
      H = \{0\}
    \end{equation}
    alors $H = 0\bZ$ fin de la démonstration. Donc à partir de maintenant on suppose que 
    \begin{equation}
      H \not= \{0\}\
    \end{equation}
    Autrement dit 
    \begin{equation}
      \exists h \not= 0 \in H
    \end{equation}
    Et par propriété des groupes on a aussi
    \begin{equation*}
      -h \in H
    \end{equation*}
    On pose 
    \begin{equation*}
      \mathcal{\varepsilon} = \left\{x > 0, x \in H\right\}
    \end{equation*}
    Or par $(3)$ $\exists h \not= 0 \in H$ donc 
    \begin{align*}
      &\text{si } h > 0, h \in \mathcal{\varepsilon} && \text{et} \\ 
      &\text{si } h < 0, -h > 0 \in H, -h \in \mathcal{\varepsilon} 
    \end{align*}
    Donc 
    \begin{equation}
      \mathcal{\varepsilon} \not= \emptyset
    \end{equation}
    Or $\mathcal{\varepsilon} \subset \bR$ non vide et minoré par $0$ donc par propriété fondamentale de $\bR$,
    $\mathcal{\varepsilon}$ admet une borne inf, on note
    \begin{equation*}
      \alpha = \inf \mathcal{\varepsilon}
    \end{equation*}
    On a que $\alpha > 0$ ou $\alpha = 0$.
    \begin{itemize}
      \item Supposons que $\alpha > 0$, on suppose que $\alpha \not\in H$ alors 
      \begin{equation*}
        \exists x \in \mathcal{\varepsilon} \in ]\alpha,2\alpha[
      \end{equation*}
      De même on a $y \in \mathcal{\varepsilon}$ tel que 
      \begin{equation*}
        \alpha < y < x
      \end{equation*}
      donc 
      \begin{equation*}
        0 < x-y < \alpha
      \end{equation*}
      Et alors 
      \begin{equation*}
        (x-y) \in H \in \mathcal{\varepsilon} < \alpha
      \end{equation*}
      or $\alpha = \inf \mathcal{\varepsilon}$
      ce qui est une absurdité donc 
      \begin{equation}
        \alpha \in H
      \end{equation}
      Or par propriété des groupes additifs
      \begin{equation*}
        \forall n \in \bZ, n\cdot \alpha \in H
      \end{equation*}
      Donc 
      \begin{equation*}
        \alpha Z \subset H
      \end{equation*}
      Réciproquement, prenons $h \in H$ on note alors 
      \begin{equation*}
        \bR = \bigcup_{n \in \bZ} [n\alpha,(n+1)\alpha[
      \end{equation*}
      Il existe donc $\delta \in [0,\alpha[$ et $n \in \bZ$ tel que $h = n\alpha + \delta$
      or 
      \begin{equation*}
        n\alpha \in H
      \end{equation*}
      donc par propriété de $H$ on a 
      \begin{equation*}
        \delta = h - n\alpha \in H \cap R^{+}
      \end{equation*}
      Supposons alors que $\delta > 0$ on a alors $\delta \in \mathcal{\varepsilon}$ et $\delta < \alpha$ ce qui 
      est une absurdité car $\alpha = \inf \mathcal{\varepsilon}$ donc par l'absurde on a 
      \begin{equation*}
        \delta = 0
      \end{equation*}
      ce qui revient à 
      \begin{equation*}
        h = n\alpha
      \end{equation*}
      Donc
      \begin{equation*}
        H \subset \alpha\bZ 
      \end{equation*}
      ce qui par double inclusion 
      \begin{equation*}
        H = \alpha\bZ
      \end{equation*}
      \item Supposons maintenant que $\alpha = 0$. Soit $u,v \in \bR^2$ tels que $v > u$ on note 
      \begin{equation*}
        I = ]u,v[
      \end{equation*}
      Soit $\varepsilon = v-u > 0$ par définition de la borne inférieure on a 
      \begin{equation*}
        \exists h \in ]\alpha,\varepsilon[ \cap H
      \end{equation*}
      ce qui dans notre cas où $\alpha = 0$ revient à 
      \begin{equation*}
        \exists h \in ]0,\varepsilon[ \cap H
      \end{equation*}
      Soit $m \in \bZ$ tel que
      \begin{equation*}
        (m-1)h \leq u \leq mh
      \end{equation*}
      par propriété des groupes additifs $mh \in H$ et 
      \begin{equation*}
        u < mh = (m-1)h + h \leq u+h < u + \varepsilon = v
      \end{equation*}
      Donc $mh \in I$ et $mh \in H$ et $I \cap H \not= \emptyset$ donc pour tout interval non trivial $I$ de $\bR$ il existe $h \in H \cap I$ donc 
      \begin{equation*}
        H\text{ est dense dans } \bR
      \end{equation*}
    \end{itemize}
  \end{proof}
\end{theorem}
\begin{rmq}
  Ce théorème est souvent utilisé lorsqu'on touche aux sous groupes de $\bR$ et les irrationels comme $\mathcal G = \bZ + 2\pi\bZ$ par exemple, pour l'algèbre modulaire
\end{rmq}
\subsubsection{Anneaux}
\begin{definition}
  On appelle la donnée de $\mathcal A = (A,+,\cdot)$ anneaux si et seulement si 
  \begin{itemize}
    \item $(A,+)$ est un groupe abélien (commutatif)
    \item $(\cdot)$ admet un élement neutre (élement neutre multiplicatif)
    \item $(\cdot)$ est distributive sur $(+)$ c'est à dire
    \begin{align*}
      &\forall (x,y,z) \in A^3 a \cdot (b+c) = (a \cdot b) + (a \cdot c) && \text{et} \\ 
      &\forall (x,y,z) \in A^3 (a+b) \cdot c = (a \cdot c) + (b \cdot c)
    \end{align*}
  \end{itemize}
\end{definition}
\begin{rmq}
  Si $(\cdot)$ est commutative alors l'anneau est dit \textit{commutatif}.
\end{rmq}
\begin{definition}
  On note $A^{\times}$ l'ensemble des éléments de $A$ inversible pour $\cdot$, on a 
  \begin{equation*}
    \mathcal G = (A^{\times},\cdot)
  \end{equation*}
  qui forme un groupe (commutatif ou non selon la commutativité de $\cdot$)
\end{definition}
\begin{definition}
  Soit $B \subset A$, on dit que $B$ est un sous anneau de $A$ si et seulement si 
  \begin{itemize}
    \item $(A,+,\cdot)$ est un anneaux
    \item $B$ est stable par $(+)$ et $(\cdot)$
  \end{itemize}
\end{definition}
\begin{ex}
  On a comme anneaux notables,
  \begin{itemize}
    \item $(\bC,+,\cdot)$ et ses sous anneaux 
    \item $\mathcal{M}_{n}(\bK)$ (qui sont non commutatifs)
  \end{itemize}
\end{ex}
\begin{definition}
  On dit que $\mathcal A = (A,+,\cdot)$ est intègre si et seulement si 
  \begin{equation*}
    \forall (x,y) \in A^2, x \cdot y = 0_A \Leftrightarrow (x = 0_A) \vee (y = 0_A)
  \end{equation*}
\end{definition}
\begin{ex}
  Les anneaux $(\bR,+,\cdot)$ et $(\bR[X],+,\cdot)$ sont intègre mais dans le cas général les anneaux $(\mathcal M_n(\bK),+,\cdot)$ 
  ne le sont pas
\end{ex}
\begin{definition}
  On appelle $\mathcal C $ le centre d'un anneau $\mathcal A = (A,+,\cdot)$ l'ensemble 
  \begin{equation*}
    \mathcal C = \left\{a \in A, (\forall b \in A, a \cdot b = b \cdot a)\right\}
  \end{equation*}
  C'est à dire l'ensemble des éléments de $A$ qui commutent avec tous les autres
\end{definition}
\begin{ex}
  Par exemple le centre de $(\mathcal M_n(\bK),+,\cdot)$ est $\mathcal C = \left\{\lambda I_n, \lambda \in \bK\right\}$
  \begin{proof}
    Laissée à l'éxercice du lecteur (indice: par analyse synthèse)
  \end{proof}
\end{ex}
\subsubsection{Idéaux}
\begin{definition}
  Soit $I \subset A$ avec $\mathcal A = (A,+,\cdot)$ un anneau commutatif intègre, on dit que $I$ est un \textit{idéal} de $\mathcal A$ si 
  \begin{itemize}
    \item $(I,+)$ est un sous groupe de $(A,+)$
    \item \begin{equation*}
      \forall x \in I, \forall a \in A, x\cdot a = a\cdot x \in I
    \end{equation*}
  \end{itemize}
\end{definition}
\begin{rmq}
  $\{0_A\}$ et $A$ sont appelés \textit{idéaux triviaux} de $\mathcal A$
\end{rmq}
\begin{ex}
  Pour $\bZ$ les idéaux sont de la forme $n\bZ$ avec $n \in \bN$
\end{ex}
\begin{definition}
  On appelle idéal premier $P$ de $\mathcal A$ un idéal $P$ tel que 
  \begin{itemize}
    \item $P \not= A$
    \item \begin{equation*}
      \forall (x,y) \in A^2, (xy) \in P \Rightarrow (x \in P) \vee (y \in P)
    \end{equation*}
  \end{itemize}
\end{definition}
\begin{definition}
  On appelle idéal maximal $M$ de $\mathcal A$ un idéal $M$ lorsque 
  \begin{equation*}
    \forall I \text{ idéal }, M \subset I \Rightarrow (I = M) \vee (I = A)
  \end{equation*}
\end{definition}
\begin{ex}
  Dans $\bZ$ les idéaux maximaux sont exactement de la forme $p\bZ$ avec $p$ un nombre premier.
\end{ex}
\subsubsection{Corps}
\begin{definition}
  On dit que $\mathcal A = (A,+,\cdot)$ est un corps si et seulement si 
  \begin{itemize}
    \item $\mathcal A$ est un anneaux intègre 
    \item $A^{\times} = A - \{0_A\}$ (tous les élements sauf l'élement neutre additifs sont inversibles)
  \end{itemize}
\end{definition}
\begin{rmq}
  Les corps sont les ensembles que l'on a l'habitude de manipuler tels que $\bR$ où on peut faire "toutes les opération" (multiplier, additionner, diviser, soustraire)
\end{rmq}
\begin{ex}
  Les corps les plus connus (écris souvents $\bK$) sont $\bR$ ou $\bC$, mais $\bQ$ est aussi un corps et plus généralement les anneaux 
  quotients $\bZ/p\bZ$ où $p$ est un entier premier.
\end{ex}
\subsection{Linéaire}
\subsubsection{Loi de Composition Externe}
\begin{definition}
  On appelle \textit{loi de composition externe} (lce) de $\bK$ sur $E$ une application de la forme 
  \vfunc{(\cdot)}{\bK \times E}{E}{(\lambda,x)}{\lambda \cdot x}
\end{definition}
\begin{rmq}
  Ces lois sont généralement d'un ensemble qu'on appelle ensemble des \textit{scalaire} (en général : un corps commutatif)
  sur un \textit{espace vectoriel}
\end{rmq}
\subsubsection{Espaces vectoriels}
\begin{definition}
  On appelle \textit{espace vectoriel} sur $\bK$ ou $\bK$-ev la donnée du triplet $(E,+,\cdot)$ tels que 
  \begin{itemize}
    \item $(E,+)$ est un groupe abélien
    \item $(\cdot)$ est une lce de $\bK$ sur $E$ 
    \item $(\cdot)$ est distributive à droite sur $+$ et à gauche sur $+_{\bK}$, c'est à dire 
    \begin{align*}
      \forall (u,v) \in E^2, \forall (\lambda,\mu) \in \bK^2, &\lambda(u+v) = (\lambda \cdot u) + (\lambda \cdot v) \\ 
      &(\lambda + \mu) \cdot u = (\lambda \cdot u) + (\mu \cdot v) \\ 
      &(\lambda \mu) \cdot v = \lambda (\mu \cdot v) \\ 
      &1_{\bK} v = v \\
      &\lambda u = 0_E \Leftrightarrow (\lambda = 0_{\bK}) \vee (u = 0_E) \\ 
      &- (\lambda u) = -\lambda (u) = \lambda (-u)
    \end{align*}
  \end{itemize}
\end{definition}
\begin{rmq}
  On a comme espaces vectoriels 
  \begin{itemize}
    \item $\bR$
    \item $\bC$
    \item $\bR^{\bR}$ (les fonctions réelles)
    \item $\mfc{C}^0$ (les fonctions continues)
    \item etc.
  \end{itemize}
\end{rmq}
\begin{definition}
  On appelle un sous espace vectoriel tout ensemble $F \subset E$ tels que 
  \begin{itemize}
    \item $(E,+,\cdot)$ est un $\bK$-ev
    \item $F$ est stable par combinaison linéaire c'est à dire
    \begin{equation*}
      \forall (x,y) \in F, \forall \lambda \in \bK, \lambda x + y \in F
    \end{equation*}
  \end{itemize}
\end{definition}
\begin{rmq}
  En pratique c'est cette caractérisation ci que l'on utilise pour savoir si $E$ est un espace vectoriel
\end{rmq}
\begin{prop}
  Soit $E,F$ deux \ev alors $E \times F$ est un \ev 
  \begin{proof}
    Laissé à éxercice du lecteur (mais découle de la définition)
  \end{proof}
\end{prop}
\begin{rmq}
  Donc $R^2$, $R^3$ etc. $R^n$ sont des espaces vectoriels par produit
\end{rmq}
\subsubsection{Familles libres/génératrices}
\begin{definition}
  Soit $E$ un $\bK$-ev, soit $x = (x_i)_{i \in I}$ une famille de vecteurs de $E$ indexé sur $I$
  on dit que $x$ est libre si et seulement si 
  \begin{equation*}
    \forall (\lambda_i)_{i \in I} \in \bK^I, \sum_{i\in I} \lambda_i x_i = 0_E \Rightarrow \forall i \in I, \lambda_i = 0_E
  \end{equation*}
\end{definition}
\begin{definition}
  Soit $E$ un $\bK$-ev, soit $x = (x_i)_{i \in I}$ une famille de vecteur de $E$ indexée sur $I$, 
  on dit que $x$ est génératrice si et seulement si 
  \begin{equation*}
    \forall x \in E, \exists (\lambda_i)_{i \in I} \in \bK^I, x = \sum_{i\in I} \lambda_i x_i
  \end{equation*}
  Autrement dit $E = \text{Vect}(x_i)$
\end{definition}
\begin{definition}
  Soit $E$ un $\bK$-ev soit $x = (x_i)_{i \in I}$ une famille de vecteur de $E$ indexée sur $I$, on dit que $x$ est 
  une base de $E$ si et seulement si 
  \begin{itemize}
    \item $x$ est libre 
    \item $x$ est génératrice
  \end{itemize}
\end{definition}
\begin{rmq}
  Dans le cas général $I$ est une partie de $\bN$.
\end{rmq}
\subsubsection{Dimension finie}
\begin{definition}
  On dit que $E$ est un $\bK$-ev de dimension finie si et seulement si il existe $\mathcal B$ une base de $E$ telle que $\abs{B} \in \bN$ 
  on dit alors que 
  \begin{equation*}
    \dim E = \abs{B}
  \end{equation*}
\end{definition}
\begin{prop}[Critère nécessaire de liberté]
  \begin{equation*}
    (x_i)_{i \in I} \text{ libre dans } E \Rightarrow \abs{(x_i)} \leq \dim E
  \end{equation*} 
\end{prop}
\begin{prop}[Critère nécessaire de génération]
  \begin{equation*}
    (x_i)_{i \in I} \text { génératrice de } E \Rightarrow \abs{(x_i)} \geq \dim E
  \end{equation*}
\end{prop}
\begin{theorem}
  On a donc 
  \begin{equation*}
    \forall (\mathcal B,\mathcal B') \text{ base de } E, \abs{\mathcal B} = \abs{\mathcal B'} = \dim E
  \end{equation*}
\end{theorem}
\subsubsection{Morphismes}
\begin{definition}
  Soit $E,F$ deux \ev. Un \textit{morphisme} de $E$ dans $F$ est une application linéaire de $E$ dans $F$, on note 
  l'ensemble des morphismes de $E$ dans $F$ $\mor$
\end{definition}
\begin{definition}
  \begin{itemize}
    \item Un morphisme de $E$ dans $E$ est appelé \textit{endomorphisme}, on note l'ensemble des endomorphisme $\endo$
    \item Un morphisme bijectif de $E$ dans $F$ est appelé \textit{isomorphisme}
    \item Un morphisme bijectif de $E$ dans $E$ est appelé \textit{automorphisme}, on note leur ensemble $\mfc{GL}(E)$ aussi 
    appelé \textit{groupe linéaire} de $E$
    \item Un morphisme de $E$ dans $\bK$ est appelé \textit{forme linéaire} on note leur ensemble $\mfc{L}(E,K)$ ou encore $E^*$ appelé 
    \textit{espace dual} de $E$
  \end{itemize}
\end{definition}
\begin{theorem}[Théorème d'isomorphie]
  Soit $E,F$ deux espaces vectoriels de dimensions finies
  \begin{equation*}
    \dim E = \dim F \Leftrightarrow \exists f \in \mfc{L}(E,F) \text{ bijective}
  \end{equation*}
\end{theorem}
\begin{rmq}
  En particulier tout \ev de dimension $n$ est isomorphe à $R^n$ appelé \textit{espace canonique de dimension n}
\end{rmq}
\subsubsection{Espace pré-hilbertien, euclidien}
\begin{definition}
  Soit $E$ un $\bR$-ev, on dit que $f = (\cdot | \cdot)$ est un produit scalaire sur $E$ si c'est une forme bilinéaire, définie, positive, symétrique
  \begin{itemize}
    \item forme bilinéaire : $f : E^2 \to \bR^+$ 
    \item définie : $\forall x \in E, f(x,x) = 0 \Rightarrow x=0$
    \item positive : $\forall x \in E, f(x,x) \geq 0$ 
    \item symétrique : $\forall (x,y) \in E^2, f(x,y) = f(y,x)$
  \end{itemize}
\end{definition}
\begin{definition}
  Soit $E$ un $\bK$-ev on appelle \textit{espace pré-hilbertien} $E$ muni d'un produit scalaire $(\cdot | \cdot)$
\end{definition}
\begin{definition}
  Un \textit{espace euclidien} est un espace pré-hilbertien de dimension finie.
\end{definition}
\subsubsection{Espace vectoriel normés}
\begin{definition}
  Soit $E$ un $\bK$-ev on dit que $f = \norm{\cdot}$ est une norme sur $E$ si 
  \begin{itemize}
    \item forme linéaire (positive) : $f : E \to \bR^+$
    \item positivité : $\forall x \in E, f(x) \geq 0$ 
    \item définie : $\forall x \in E, f(x)=0 \Rightarrow x=0_E$
    \item homogène : $\forall x \in E, \forall \lambda \in \bK, f(\lambda x) = \abs{\lambda} f(x)$
    \item inégalité triangulaire, $\forall (x,y) \in E^2, f(x+y) \leq f(x) + f(y)$
  \end{itemize}
\end{definition}
\begin{ex}
  Par exemple la \textit{norme infinie} est une norme sur $\bR^{\bR}$ définie par 
  \begin{equation*}
    \forall f \in \bR^{\bR}, \norm{f}_{\infty} = \sup{x \in \bR} \abs{f(x)}
  \end{equation*}
\end{ex}
\begin{definition}
  On appelle $(E,f)$ un \textit{espace vectoriel normé} si $E$ est un espace vectoriel et $f$ une norme sur $E$
\end{definition}
\paragraph{Dimension finie}
\begin{definition}
  Si $E$ est un \ev de dimension finie alors toutes les normes sont équivalentes, et toutes les endomorphismes de $E$ sont continus
\end{definition}
\section{Autre}
\begin{rmq}
  Soit $f$ une fonction définie sur $I$ un interval de $\bR$ tel que $I = [a,b]$
  \begin{itemize}
    \item $\int_I f = \int_a^b f = \int_a^b f(x) \text{d}x$
  \end{itemize}
\end{rmq}
\chapter{Hors Programme - Supplément}
\section{Relation binaire}
\begin{definition}
  Soit $E$ un ensemble, soit $\Gamma \subset E^2$ on lui associe la relation binaire suivante $\mathcal{R}$ définie par 
  \begin{equation*}
    \forall (x,y) \in E^2, x\mathcal{R}y \Leftrightarrow (x,y) \in \Gamma
  \end{equation*}
  On dit que $x$ \textit{est en relation} avec $y$ pour $\mathcal{R}$
\end{definition}
\subsection{Propriétés particulières}
\begin{definition}
  Soit $E$ un ensemble, soit $\Gamma \subset E^2$ et $\mathcal{R}$ la relation binaire associée
  \begin{itemize}
    \item si $\forall x \in E, x\mathcal{R}x$ on dit que $\mathcal{R}$ est \textit{reflexive} c'est équivalent à 
    \begin{equation*}
      \forall x \in E, (x,x) \in \Gamma
    \end{equation*}
    \item si $\forall (x,y) \in E^2, x\mR y \Rightarrow y\mR x$ on dit que $\mR$ est \textit{symétrique} c'est équivalent à 
    \begin{equation*}
      \forall (x,y) \in E^2, (x,y) \in \Gamma \Rightarrow (y,x) \in \Gamma
    \end{equation*}
    \item si $\forall (x,y,z) \in E^3, (x \mR y) \wedge (y \mR z) \Rightarrow x\mR z$ on dit que $\mR$ est \textit{transitive} c'est équivalent à 
    \begin{equation*}
      \forall (x,y,z) \in E^3 ((x,y) \in \Gamma) \wedge ((y,z) \in \Gamma) \Rightarrow (x,z) \in \Gamma
    \end{equation*}
    \item si $\forall (x,y) \in E^2 (x\mR y) \wedge (y\mR x) \Rightarrow x=y$ alors on dit que $\mR$ est \textit{antisymétrique}
  \end{itemize}
\end{definition}
\section{Relations particulière}
\subsection{Relation d'ordre}
\begin{definition}
  Soit $E$ un ensemble, soit $\Gamma \subset E^2$ et $\mR$ la relation binaire associé on dit que $\mR$ est une relation de pré-ordre sur $E$ si
  \begin{itemize}
    \item $\mR$ est réflexive
    \item $\mR$ est transitive
  \end{itemize}
\end{definition}
\begin{definition}
  Soit $E$ un ensemble, soit $\Gamma \subset E^2$ et $\mR$ la relation associée, on dit que $\mR$ est une relation d'ordre sur $E$ si 
  \begin{itemize}
    \item $\mR$ est un pré-ordre sur $E$
    \item $\mR$ est antisymétrique
  \end{itemize}
\end{definition}
\begin{ex}
  Par exemple $(\leq)$ ou $(\geq)$ sont des ordres sur $\bN,\bZ,\bQ$ et $\bR$, on appelle la donnée d'un ensemble et d'une relation d'ordre sur cet ensemble 
  un \textit{ensemble ordonné}
\end{ex}
\begin{definition}
  Soit $\mR$ une relation d'ordre on dit que $\mR$ est totale si 
  \begin{equation*}
    \forall (x,y) \in E^2, (x\mR y) \vee (y\mR x) \vee (x=y)
  \end{equation*}
\end{definition}
\begin{ex}
  Par exemple la relation $|$ "divise" sur $\bN$ est une relation d'ordre non-totale, par exemple $2$ et $3$ ne sont pas en relation entre eux et $2 \not= 3$
\end{ex}
\section{Relation d'équivalence}
\begin{definition}
  Soit $\mR$ un pré-ordre sur $E$ alors $\mR$ est une relation d'équivalence si et seulement si $\mR$ est symétrique, on note alors $\mR$ en général par le symbole "tilde" $\sim$
\end{definition}
\subsection{Classes d'équivalence}
\begin{definition}
  Soit $\mR$ une relation d'équivalence sur $E$. Soit $x \in E$ on note
  \begin{equation*}
    \bar{x} = \left\{y \in E, x\mR y\right\} = \left\{x \in E, y\mR x \right\}
  \end{equation*}
  la \textit{classe d'équivalence} de $x$.
\end{definition}
\begin{prop}
  Soit $\mR$ une relation d'équivalence sur $E$ alors 
  \begin{equation*}
    \forall (x,y) \in E^2, x\mR y \Leftrightarrow \bar{x} =\bar{y}
  \end{equation*}
  \begin{proof}
    Soit $\mR$ une relation d'équivalence sur $E$ et soit $(x,y) \in E^2$ 
    \begin{itemize}
      \item On suppose que $x \mR y$ alors soit $z \in \bar{x}$ donc $z\mR x$ par transitivité on a $z \mR y$ donc $z \in \bar{y}$ 
      soit $z \in \bar{y}$ alors $y \mR z$ donc par transitivité $x \mR z$ donc $z \in \bar{x}$ par double inclusion on a $\bar{x} = \bar{y}$
      \item On suppose que $\bar{x} = \bar{y}$ alors soit $z \in \bar{x}$ on a $x \mR z$ et comme $\bar{x} = \bar{y}$ on a $z \in \bar{y}$ donc $z \mR y$ par transitivité on a $x \mR y$
    \end{itemize}
  \end{proof}
\end{prop}
\section{Ensemble quotient}
\begin{definition}
  Soit $\mR$ une relation d'équivalence sur $E$, on note l'ensemble des classes d'équivalences 
  \begin{equation*}
    E/\mR = \left\{\bar{x},x \in E\right\}
  \end{equation*}
  On appelle $E/\mR$ ensemble quotient de $E$ par $\mR$
\end{definition}
\begin{rmq}
  Pour $x \in E$, on appelle $y \in \bar{x}$ \textit{représentant de la classe} $\bar{x}$
\end{rmq}
\begin{prop}
  Les classes d'équivalences sont disjointes c'est à dire, 
  \begin{equation*}
    \forall (\bar{x},\bar{y}) \in (E/\mR)^2 (\bar{x}\not= \bar{y}) \Rightarrow x \cap y = \emptyset
  \end{equation*}
  \begin{proof}
    Soit $(\bar{x},\bar{y}) \in (E/\mR)^2$ tel que $(\bar{x}\not= \bar{y})$, soit $z \in \bar{x} \cap \bar{y}$ donc par définition
    $x \mR z$ et $z \mR y$ donc par transitivité $x \mR y$ donc par le théorème précédent $\bar{x} =\bar{y}$ ce qui est absurde, donc $\bar{x} \cap \bar{y} = \emptyset$
  \end{proof}
\end{prop}
\begin{prop}
  Les classes d'équivalences forment une partition de $E$
  \begin{proof}
    Il a été précédamment prouvé que $\bar{x}$ et $\bar{y}$ sont disjoints deux à deux, or 
    \begin{equation*}
      \forall x \in E, x \in \bar{x} 
    \end{equation*}
    donc 
    \begin{equation*}
      \bigcup_{\bar{x}\in E} = E
    \end{equation*}
    puis comme l'union est disjointes les classes d'équivalences forment bien une partition de $E$
  \end{proof}
\end{prop}
\section{Morphismes}
\subsection{Morphisme de Groupes}
\begin{definition}
  Soit $\mathcal G = (G,\cdot)$ et $\mathcal G' = (G',\cdot_1)$ deux groupes soit $f : G \to G'$ on dit que $g$ est un morphisme de groupe si et seulement si 
  \begin{itemize}
    \item $\forall (x,y) \in G, f(x \cdot y) = f(x) \cdot_1 f(y)$ 
    \item $f(\varepsilon_{G}) = \varepsilon_{G'}$
  \end{itemize}
\end{definition}
\subsubsection{Images et Noyaux}
\begin{definition}
  Soit $f$ un morphisme de groupe entre $G$ et $G'$ alors on défini l'image de $f$ par 
  \begin{equation*}
    \Im f = \left\{f(h), h \in G\right\}
  \end{equation*}
\end{definition}
\begin{definition}
  Soit $f$ un morphisme de groupe entre $G$ et $G'$ alors on défini le noyau de $f$ par 
  \begin{equation*}
    \ker f = \left\{h \in G, f(h) = \varepsilon_{G'}\right\}
  \end{equation*}
\end{definition}
\subsubsection{Morphisme de groupes finis}
\begin{theorem}[Formule des cardinaux]
  Soit $G$ un groupe fini, soit $G'$ un groupe, soit $f : G \to G'$ un morphisme de groupe alors 
  \begin{equation*}
    \abs{G} = \abs{\ker f} \cdot \abs{\Im f}
  \end{equation*}
  \begin{proof}
    Soit $G$ un groupe fini, $G'$ un groupe et $f$ un morphisme de groupe alors 
    \begin{equation*}
      \exists p \in \bN, \left\{y_1,\cdots,y_p\right\} \subset G' = \Im f
    \end{equation*}
    car $G$ est fini
    Prenons maintenant l'application $f' : G \to \Im f$ par définition elle est surjective (toute application est surjective sur son image)
    donc il vient que 
    \begin{equation*}
      \abs{G} \geq \abs{\Im f} = p
    \end{equation*}
    On note 
    \begin{equation*}
      I = [1,p] \cap \bN
    \end{equation*}
    Maintenant on note la famille $(x_i)$ définie par 
    \begin{equation*}
      \forall i \in I, f(x_i) = y_i
    \end{equation*}
    On observe par définition d'une application que 
    \begin{equation*}
      G = \bigcup_{i=1}^p f^{-1}\{y_i\}
    \end{equation*}
    On note alors 
    \begin{equation*}
      \forall i \in I, G_i = f^{-1}\{y_i\}
    \end{equation*}
    On prouve ensuite que l'union est disjointe, soit $i,j \in I$ tels que $i\not=j$, on suppose qu'il existe $x \in G_i \cap G_j$ 
    alors ça veut dire que 
    \begin{equation*}
      f(x) = y_i
    \end{equation*}
    et 
    \begin{equation*}
      f(x) = y_j
    \end{equation*}
    Or $y_i \not= y_j$ donc c'est une absurdité donc 
    \begin{equation*}
      G_i \cap G_j = \emptyset
    \end{equation*}
    Donc 
    \begin{equation*}
      G = \biguplus_{i=1}^p G_i
    \end{equation*}
    Il vient donc 
    \begin{equation*}
      \abs{G} = \sum_{i=1}^p \abs{G_i}
    \end{equation*}
    Il suffit donc de calculer le cardinal de $G_i$ pour $i \in I$, pour cela soit $x \in G$ on défini 
    \vfunc{\varphi}{\ker f}{x\ker f}{h}{xh}
    \begin{itemize}
      \item On montre que $\varphi$ est injective, en effet soit $h,h' \in G$ tels que 
      \begin{align*}
        \varphi(h) = \varphi(h') &\Leftrightarrow \\ 
        &\Leftrightarrow xh = xh' \\
        &\Leftrightarrow x^{-1}xh = x^{-1}xh' && \text{car } x \in G \\ 
        &\Leftrightarrow h=h'
      \end{align*}
      Donc $\varphi$ est injective
      \item On montre maintenant que $\varphi$ est surjective, soit $y \in x\ker f$ alors 
      \begin{align*}
        \varphi(h) = y &\Leftrightarrow xh = y \\ 
        &\Leftrightarrow x^{-1}xh = x^{-1}y \\ 
        &\Leftrightarrow h = x^{-1}y
      \end{align*}
      Donc $\forall y \in x\ker f, \exists h \in \ker f, \varphi(h) = y$ donc $\varphi$ est surjective
    \end{itemize}
    Donc $\varphi$ est bijective, donc (lemme 1)
    \begin{equation*}
      \abs{\ker f} = \abs{x\ker f}
    \end{equation*}
    
    Montrons ensuite que pour un morphisme $f$, $x_0 \in G$ on a 
    \begin{equation*}
      f(x_0)^{-1} = f(x_0^{-1})
    \end{equation*}
    \begin{align*}
      f(x_0)f(x_0^{-1}) &= f(x_0\cdot x_0^{-1}) && \text{morphisme de groupe} \\ 
      &= f(\varepsilon) \\ 
      &= \varepsilon_{G'}
    \end{align*}
    Donc (lemme 2)
    \begin{equation*}
      f(x_0)^{-1} = f(x_0^{-1})
    \end{equation*}
    Soit $i \in I$
    \begin{align*}
      G_i &= f^{-1}\{y_i\} \\ 
      &= \{x \in G, f(x) = y_i\} \\ 
      &= \{x \in G, f(x) = f(x_i)\} \\ 
      &= \{x \in G, f(x_i)^{-1}f(x) = \varepsilon_{G'}\} \\ 
      &= \{x \in G, f(x_i^{-1})f(x) = \varepsilon_{G'}\} && \text{lemme 2}\\ 
      &= \{x \in G, x_i^{-1}x \in \ker f\} \\ 
      &= \{x \in G, x \in x_i\ker f\}
    \end{align*}
    Donc 
    \begin{align*}
      \abs{G_i} &= \abs{x_i\ker f} \\ 
      &= \abs{\ker f} && \text{lemme 1}
    \end{align*}
    Donc finalement 
    \begin{align*}
      \abs{G} &= \sum_{i=1}^p \abs{G_i} \\ 
      &= \sum_{i=1}^p \abs{\ker f} \\ 
      &= p\abs{\ker f} \\ 
      &= \abs{\Im f}\abs{\ker f}
    \end{align*}
    Donc enfin 
    \begin{equation*}
      \abs{G} = \abs{\Im f} \cdot \abs{\ker f}
    \end{equation*}
  \end{proof}

\end{theorem}
\subsection{Morphismes d'Anneaux}
\begin{definition}
  Soit $\mathcal A = (A,+_1,\cdot_1)$ et $\mathcal B = (B,+_2,\cdot_2)$ deux anneaux, et $f : A \to B$ on dit que $f$ est un morphisme 
  d'anneaux si et seulement si 
  \begin{itemize}
    \item $\forall (x,y) \in A^2, f(x +_1 y) = f(x) +_2 f(y)$
    \item $\forall (x,y) \in A^2, f(x \cdot_1 y) = f(x) \cdot_2 f(y)$
    \item $f(1_A) = 1_B$
  \end{itemize}
\end{definition}
\subsubsection{Images et noyaux}
\begin{definition}
  Soit $f$ un morphisme d'anneau entre $A$ et $B$ alors on défini l'image de $f$ par 
  \begin{equation*}
    \Im f = \left\{f(a), a \in A\right\}
  \end{equation*}
  C'est un sous-anneau de $B$
\end{definition}
\begin{definition}
  Soit $f$ un morphisme d'anneau entre $A$ et $B$ alors on défini le noyau de $f$ par 
  \begin{equation*}
    \ker f = \left\{a \in A, f(a) = 0_B\right\}
  \end{equation*}
  C'est un sous groupe additif de $(A,+)$
\end{definition}
\subsubsection{Bijectivité}
\begin{theorem}
  Soit $f$ un morphisme d'anneau entre $A$ et $B$ alors 
  \begin{itemize}
    \item $f$ est \textit{injective} de $A$ dans $B$ si $\ker f = \{0_A\}$
    \item $f$ est \textit{surjective} de $A$ dans $B$ si $\Im f = B$
    \item $f$ est \textit{bijective} de $A$ dans $B$ si $f$ est injective et surjective
  \end{itemize}
\end{theorem}
\begin{definition}
  On dit que $f$ est une \textit{isomorphie d'anneaux} si et seulement si $f$ est un morphisme bijectif d'anneaux, dans ce cas là on dit que 
  $A$ est isomorphe à $B$
\end{definition}
\begin{rmq}
  La relation $\sim$ définie par
  \begin{equation*}
    A \sim B \Rightarrow \exists f : A \to B \textit{ bijective} 
  \end{equation*}
  est une relation d'équivalence
  \begin{proof}
    Soit $\sim$ définie comme ci dessus
    \begin{itemize}
      \item La fonction $Id_{A}$ l'identité de $A$ est un morphisme d'anneaux bijectif de $A$ dans $A$ donc $A \sim A$ donc $\sim$ est reflexive
      \item Soit $A,B$ deux anneaux et $f$ une isomorphie d'anneaux entre $A$ et $B$ alors $f^{-1}$ est une isomorphie d'anneaux (par définition) entre $B$ et $A$ donc $B \sim A$
      donc $\sim$ est symétrique 
      \item Soit $A,B,C$ trois anneaux, soit $f : A \to B$ et $g : B \to C$ deux isomorphie d'anneaux alors $f \circ g : A \to C$ est aussi une isomorphie d'anneaux donc $A \sim C$ 
      donc $\sim$ est transitive
    \end{itemize}
    Donc $\sim$ est une relation symétrique, transitive et réfléxive, donc $\sim$ est bien une relation d'équivalence
  \end{proof}
\end{rmq}
\subsubsection{Anneaux finis}
\begin{definition}
  Soit $A$ un anneau, on dit que $A$ est fini si son cardinal est un entier naturel, on note son cardinal $\abs{A}$
\end{definition}
\begin{prop}[Condition nécessaire]
  \begin{equation*}
    (A,+,\cdot) \textit{ fini } \Rightarrow (A,+) \textit{ groupe fini} 
  \end{equation*}
\end{prop}
\begin{theorem}[Formule des cardinaux]
  Soit $A$ un anneau fini et $B$ un anneau et $f : A \to B$ un morphisme d'anneau alors on a 
  \begin{equation*}
    \abs{A} = \abs{\Im f} \cdot \abs{\ker f}
  \end{equation*}
  \begin{proof}
    Le groupe $(A,+)$ induit de $A$ est fini car $A$ est fini, c'est donc une conséquence de la formule des cardinaux pour les 
    groupes finis
  \end{proof}
\end{theorem}
\begin{prop}
  Soit $A,B$ deux anneaux finis, et $f : A \to B$ deux anneaux finis alors 
  \begin{equation*}
    f \text{ est une isomorphie } \Rightarrow \abs{A} = \abs{B}
  \end{equation*}
  \begin{proof}
    D'après la formule des cardinaux on a 
    \begin{equation*}
      \abs{A} = \abs{\Im f} \cdot \abs{\ker f}
    \end{equation*}
    Or $f$ est une isomorphie donc $f$ est injective donc $\abs{\ker f} = 1$ donc 
    \begin{equation*}
      \abs{A} = \abs{\Im f}
    \end{equation*}
    Or $f$ est une isomorphie donc $f$ est surjective donc $\Im f = B$ donc 
    \begin{equation*}
      \abs{A} = \abs{B}
    \end{equation*}
  \end{proof}
\end{prop}
\begin{theorem}
  Soit $A$ et $B$ deux anneaux finis (i.e anneaux avec un nombre d'élement fini). On note $\abs{\cdot}$ le cardinal d'un ensemble, 
  soit $f$ un morphisme d'anneaux entre $A$ et $B$ alors 
  \begin{equation*}
    f \text{ bijective} \Leftrightarrow (\abs{A} = \abs{B}) \wedge (\ker f = \{0_A\}) \Leftrightarrow (\abs{A} = \abs{B}) \wedge (\Im f = B)
  \end{equation*}
  ce qui est équivalent à, si $\abs{A} = \abs{B}$ alors 
  \begin{equation*}
    f \text{ bijective} \Leftrightarrow f \text{ injective} \Leftrightarrow f \text{ surjective}
  \end{equation*}
  \begin{proof}
    Démontrons par démonstration circulaire c'est à dire $(A \Rightarrow B) \wedge (B \Rightarrow C) \wedge (C \Rightarrow A)$
    \begin{itemize}
      \item Montrons $(A \Rightarrow B)$, on suppose que $f$ est une isomorphie d'anneaux, alors par la condition nécessaire d'isomorphie on a 
      \begin{equation*}
        \abs{A} = \abs{B}
      \end{equation*}
      ensuite, $f$ est une isomorphie donc $f$ est injective
      \item Montrons $(C \Rightarrow A)$ on suppose que $f$ est surjective et $\abs{A} = \abs{B}$, $f$ est surjective donc 
      \begin{equation*}
        \Im f = B
      \end{equation*}
      or par formule des cardinaux on sait que 
      \begin{equation*}
        \abs{A} = \abs{\ker f} \cdot \abs{\Im f}
      \end{equation*}
      or $\abs{\Im f} = \abs{B} = \abs{A}$ donc 
      \begin{equation*}
        \abs{\ker f} = \frac{\abs{A}}{\abs{A}}
      \end{equation*}
      d'où 
      \begin{equation*}
        \abs{\ker f} = 1
      \end{equation*}
      Or on a forcément $\varepsilon \in \ker f$ par définition donc finalement 
      \begin{equation*}
        \ker f = \{\varepsilon\}
      \end{equation*}
      Donc $f$ est injective, donc $f$ est bijective 
      \item Montrons finalement $(B \Rightarrow C)$, on suppose que $f$ est injective et que $\abs{A} = \abs{B}$
      par formule des cardinaux on a 
      \begin{equation*}
        \abs{A} = \abs{\ker f} \cdot \abs{\Im f}
      \end{equation*}
      Or $f$ est injective donc $\abs{\ker f} = 1$ donc 
      \begin{equation*}
        \abs{A} = \abs{\Im f}
      \end{equation*}
      donc 
      \begin{equation*}
        \abs{B} = \abs{\Im f}
      \end{equation*}
      Or $\Im f \subset B$ et $\abs{B} = \abs{\Im f}$ donc $\Im f = B$ donc $f$ est surjective
    \end{itemize}
    Par démonstration circulaire, la propriété est démontrée
  \end{proof}
\end{theorem}
\subsection{Anneaux quotient}
\begin{definition}
  Soit $\mathcal A = (A,+,\cdot)$ un anneau et $\mR$ une relation compatible avec $(+)$ et $(\cdot)$ alors 
  on appelle anneau quotient la donnée du triplet $(A/\mR, +, \cdot)$ avec 
  \begin{align*}
    &\forall (x,y) \in A^2, \bar{x} + \bar{y} = \overline{x+y} \\ 
    &\forall (x,y) \in A^2, \bar{x} \cdot \bar{y} = \overline{x \cdot y}
  \end{align*}
\end{definition}
\section{Algèbre modulaire}
\subsection{Les anneaux $\bZ/n\bZ$}
Pour cette section $n \geq 2$ en effet $\bZ/1\bZ$ est juste le corps trivial réduit à $0$
\begin{definition}
  On note $\bZ/n\bZ$ l'anneau des entier modulo $n$ 
\end{definition}
\begin{prop}
  Soit $n \in \bN^*$ l'ensemble quotient $\bZ/n\bZ$ est de cardinal $n$ et $\bZ/n\bZ = \left\{\bar{0},\cdots,\overline{n-1}\right\}$
  \begin{proof}
    Conséquence directe de la congruence modulo $n$
  \end{proof}
\end{prop}
\subsubsection{Inverses modulaires}
\begin{definition}
  Les inverses modulos $n$ sont exactements les éléments inversibles de $\bZ/n\bZ$
\end{definition}
\begin{prop}
  L'inverse de $a$ modulo $n$ existe si et seulement si $a$ et $n$ sont premiers entre eux.
  \begin{proof}
    L'inverse de $a$ modulo $n$ est un entier $u$ vérifiant 
    \begin{equation*}
      au \equiv 1 \mod n
    \end{equation*}
    ce qui est équivalent à s'il existe un entier $v$ tel que 
    \begin{equation*}
      au + nv = 1
    \end{equation*}
    Or d'après le théorème de Bézout on a que $a \wedge b = 1 \Leftrightarrow \exists (u,v) \in \bN^*, au + bv = 1$,
    donc d'après le théorème de bézout il faut que $a \wedge n = 1$ donc que $a$ et $n$ soient premier entre eux
  \end{proof}
\end{prop}
\subsubsection{Les corps $\bZ/p\bZ$}
\begin{theorem}
  L'ensemble quotient $\bZ/p\bZ$ est un corps si et seulement si $p$ est premier
  \begin{proof}
    D'après la définition on sait que $\bZ/p\bZ = \left\{\bar{0},\cdots,\bar{p-1}\right\}$
    \begin{itemize}
      \item On suppose que $p$ est premier, alors $\forall \bar{x} \in \bZ/p\bZ$ $p \wedge \bar{x} = 1$ si $\bar{x} \not= \bar{0}$, or 
      $\bar{0}$ est l'élement neutre additif de $\bZ/p\bZ$ alors par la proposition précédente
      \begin{equation*}
        \forall \bar{x} \in \bZ/p\bZ, \bar{x} \not= \bar{0} \Rightarrow \bar{x}^{-1} \in \bZ/p\bZ 
      \end{equation*}
      donc tous les éléments sauf l'élement neutre additif sont inversible donc $\bZ/p\bZ$ est un corps
      \item On suppose maintenant que $\bZ/p\bZ$ alors
      \begin{equation*}
        \forall \bar{x} \in \bZ/p\bZ, \bar{x} \not= \bar{0} \Rightarrow \bar{x}^{-1} \in \bZ/p\bZ
      \end{equation*}
      Donc tous les éléments ont un inverse modulo $p$ par la proposition précédente on a donc 
      \begin{equation*}
        \forall \bar{x} \in \bZ/p\bZ, \bar{x} \not= \bar{0} \Rightarrow p \wedge \bar{x} = 1 
      \end{equation*}
      Donc 
      \begin{equation*}
        \forall i \in [1,p-1] \cap \bN, i \wedge p = 1
      \end{equation*}
      Donc $p$ est premier avec tous les entiers précédents, donc $p$ est premier
    \end{itemize}
    Donc $\bZ/p\bZ$ est un corps si et seulement si $p$ est premier
  \end{proof}
\end{theorem}
\subsection{Théorème des restes chinois}
\begin{theorem}[Théorème des restes chinois]
  Soit $n$ et $m$ tels que $n \wedge m = 1$ alors $\bZ/nm\bZ$ est isomorphe à $\bZ/n\bZ \times \bZ/m\bZ$
  \begin{proof}
    Soit $n$ et $m$ tels que $n \wedge m = 1$.
    \begin{itemize}
      \item Premièrement $\abs{\bZ/nm\bZ} = nm$ et $\abs{\bZ/n\bZ \times \bZ/m\bZ} = nm$ donc les cardinaux sont égaux
      \item Ensuite soit \vfunc{f}{\bZ/nm\bZ}{\bZ/n\bZ\times \bZ/m\bZ}{\bar{x}_{nm}}{(\bar{x}_n,\bar{x}_m)}
      Soit $(x,y)$ tels que $\bar{x}_{nm} = \bar{y}_nm$ on a que $nm | x - y$ donc comme $n \wedge m = 1$ par théorème de Gauss, 
      on a 
      \begin{equation*}
        (n | x - y) \wedge (m | x - y)
      \end{equation*}
      donc 
      \begin{equation*}
        \bar{x}_{n} = \bar{y}_{n} \wedge \bar{x}_m = \bar{y}_m
      \end{equation*}
      donc finalement 
      \begin{equation*}
        f(\bar{x}_{nm}) = f(\bar{y}_{nm})
      \end{equation*}
      Donc $f$ est bien définie. Soit $(x,y) \in \bZ^2$ 
      \begin{align*}
        f(\bar{x}_{nm} + \bar{y}_{nm}) &= f(\overline{x+y}_{nm}) && \text{addition dans } \bZ/nm\bZ \\ 
        &= (\overline{x+y}_n,\overline{x+y}_m) && \text{définition de } f \\ 
        &= (\bar{x}_n + \bar{y}_n,\bar{x}_m + \bar{x}_m) && \text{addition dans } \bZ/n\bZ \text{ et } \bZ/m\bZ \\ 
        &= (\bar{x}_n,\bar{x}_m) + (\bar{y}_n,\bar{y}_m) \\ 
        &= f(\bar{x}_{nm}) + f(\bar{y}_{nm}) 
      \end{align*}
      On prouve de la même manière la conservation de l'élément neutre multiplicatif.
      Donc $f$ est linéaire donc $f$ est un morphisme d'anneaux.
      \item Puis, soit $x \in \bZ$ tel que 
      \begin{align*}
        f(\bar{x}_{nm}) &= (\bar{0}_n,\bar{0}_m) \\
        (\bar{x}_n = \bar{0}_n) &\wedge (\bar{x}_m = \bar{0}_m) \\ 
        (n | x) &\wedge (m | x) \\ 
        (nm &| x) && \text{ car } n \wedge m = 1 \\ 
        \bar{x}_{nm} &= \bar{0}_{nm}
      \end{align*}
      Donc $f(x) = 0 \Rightarrow x=0$ donc $f$ est injective
      \item Enfin, $f$ est un morphisme injectif entre $\bZ/nm\bZ$ et $\bZ/n\bZ \times \bZ/m\bZ$ et les deux ensembles sont de cardinal égaux 
      donc $f$ est une isomorphie d'anneaux entre $\bZ/nm\bZ$ et $\bZ/n\bZ \times \bZ/m\bZ$
    \end{itemize}
    Donc $\bZ/nm\bZ$ est isomorphe à $\bZ/n\bZ \times \bZ/m\bZ$ 
  \end{proof}
\end{theorem}
\subsection{Décomposition des $\bZ/n\bZ$}
\begin{definition}
  Le théorème fondamental de l'arithmétique dit 
  \begin{align*}
    \forall n \in \bN, !\exists (p_i) \in \mathbb{P}^{I} \wedge (\alpha_i) \in \bN^{I}, n = \sum_{i\in I} p_i^{\alpha_i}
  \end{align*}
\end{definition}
\begin{theorem}[Théorème fondamental de l'arithmétique, version algébrique]
  Soit $n \in \bN_{\geq 2}$, soit $(p_i)$ et $(\alpha_i)$ l'unique Décomposition de $n$ alors 
  \begin{equation*}
    \bZ/n\bZ = \prod_{i\in I} (\bZ/p_i\bZ)^{\alpha_i}
  \end{equation*}
  \begin{proof}
    Les premiers étants premier deux à deux, conséquences du théorème des restes chinois.
  \end{proof}
\end{theorem}
\end{document}